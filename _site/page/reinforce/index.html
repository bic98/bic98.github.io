<!DOCTYPE html>
<html >
	<head>
		

<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<meta name="google-site-verification" content="googlef9130e9561130734.html" />
<title>Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology</title>

	<meta name="description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">


	<meta name="keywords" content="Í∞ïÌôîÌïôÏäµ, Î®∏Ïã†Îü¨Îãù, Ïù∏Í≥µÏßÄÎä•, Ïù∏Ï∞¨Î∞±, InchanBaek, Î¶¨ÏõåÎìú, ÏóêÏù¥Ï†ÑÌä∏, Ïï°ÏÖò, MDP, ÎßàÎ•¥ÏΩîÌîÑ Í≤∞Ï†ï Í≥ºÏ†ï, Q-Îü¨Îãù, reinforcement learning, machine learning, AI, reward, agent, action, Markov decision process, Q-learning, deep reinforcement learning">

<meta name="author" content="Inchan Baek">
<!-- Open Graph / Facebook -->
<meta property="og:type" content="website">
<meta property="og:url" content="http://localhost:4000/page/reinforce/">
<meta property="og:title" content="Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology">
<meta property="og:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">
<!-- Twitter -->
<meta property="twitter:card" content="summary">
<meta property="twitter:url" content="http://localhost:4000/page/reinforce/">
<meta property="twitter:title" content="Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology">
<meta property="twitter:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">
<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="canonical" href="http://localhost:4000/page/reinforce/">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning from Scratch</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Reinforcement Learning from Scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques." />
<meta property="og:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques." />
<link rel="canonical" href="http://localhost:4000/page/reinforce/" />
<meta property="og:url" content="http://localhost:4000/page/reinforce/" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reinforcement Learning from Scratch" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.","headline":"Reinforcement Learning from Scratch","url":"http://localhost:4000/page/reinforce/"}</script>
<!-- End Jekyll SEO tag -->

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/moonspam/NanumSquare@1.0/nanumsquare.css">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<link rel="stylesheet" href="/doks-theme/assets/css/custom.css">
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      securityLevel: 'loose'
    });
    
    // Find all pre blocks with mermaid class and render them
    document.querySelectorAll('pre code.language-mermaid').forEach(function(element) {
      // Get the mermaid code
      const mermaidCode = element.textContent;
      
      // Create a new div for the mermaid diagram
      const newDiv = document.createElement('div');
      newDiv.className = 'mermaid';
      newDiv.textContent = mermaidCode;
      
      // Replace the pre element with the new div
      const preElement = element.parentElement;
      preElement.parentElement.replaceChild(newDiv, preElement);
    });
    
    // Trigger mermaid render
    mermaid.init();
  });
</script>

	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
		


	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">InchanBaek Note</a>
					
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Reinforcement Learning from Scratch</h1>
								
								
									<p class="hero-subheader__desc">A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.</p>
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="content">
							<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h2 id="bandit-problem">Bandit Problem</h2>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>

<ul>
  <li>
    <p>Supervised Learning : When the input and output data are given, it is a method of modeling the relationship between input data and output data.</p>
  </li>
  <li>
    <p>Unsupervised Learning : When the input data is given, it is a method of finding the characteristics of the input data.</p>
  </li>
  <li>
    <p><strong>Reinforcement Learning</strong> : <strong>Agent</strong> is an entity that interacts with the <strong>environment</strong> and receives information about the environment to choose <strong>actions</strong> that <strong>maximize rewards</strong>.</p>
  </li>
</ul>

<h3 id="what-is-bandit-problem">What is Bandit problem?</h3>

<p>Bandit == Slot machine</p>

<p>Each slot machine has a different probability.</p>

<p>At first, we don‚Äôt know which slot machine is the best.</p>

<p>We need to find the good machine by actually playing.</p>

<p>The goal is to get as much reward as possible within a limited number of plays.</p>

<div align="center">
  <div class="mermaid">
    graph LR
    A[Agent] --&gt;|action| B[Environment]
    B --&gt;|reward| A
  </div>
</div>

<p><strong>The agent as a player selects actions in a given environment, and the environment provides rewards to the agent.</strong></p>

<p><strong>Goal</strong>: <strong>Select actions that maximize rewards</strong> -&gt; <strong>Get as many coins as possible</strong> -&gt; <strong>Find the best slot machine</strong></p>

<h3 id="value-and-action-value">Value and Action Value</h3>

<ul>
  <li><strong>Value</strong>: Expected reward that can be obtained in a specific state</li>
</ul>

\[E[R_t]\]

<ul>
  <li><strong>Action Value</strong>: Expected reward obtained as a result of an action</li>
</ul>

\[Q(A) = E[R_t | A]\]

<p>(E = Expectation, Q = Quality, A = Action, R = Reward)</p>

<p>Let‚Äôs calculate the expected rewards for slot machines a and b.</p>

<p>Below is a table for slot machine a.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Slot machine a</th>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">¬†</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Coins obtainable</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">10</td>
    </tr>
    <tr>
      <td style="text-align: center">Reward probability</td>
      <td style="text-align: center">0.70</td>
      <td style="text-align: center">0.15</td>
      <td style="text-align: center">0.12</td>
      <td style="text-align: center">0.03</td>
    </tr>
  </tbody>
</table>

<p>Here‚Äôs a table for slot machine b.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Slot machine b</th>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">¬†</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Coins obtainable</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">10</td>
    </tr>
    <tr>
      <td style="text-align: center">Reward probability</td>
      <td style="text-align: center">0.50</td>
      <td style="text-align: center">0.40</td>
      <td style="text-align: center">0.09</td>
      <td style="text-align: center">0.01</td>
    </tr>
  </tbody>
</table>

<p>The expected values for the two machines are:</p>

<ul>
  <li>Slot machine a: (0.7 * 0 + 0.15 * 1 + 0.12 * 5 + 0.03 * 10) = 1.05</li>
  <li>Slot machine b: (0.5 * 0 + 0.4 * 1 + 0.09 * 5 + 0.01 * 10) = 0.95</li>
</ul>

<p><strong>Slot machine a is better than slot machine b.</strong></p>

<h3 id="value-estimation">Value Estimation</h3>

<p>Let‚Äôs say the rewards obtained during n plays are R1, R2, ‚Ä¶, Rn.
Then the action value estimate Qn can be calculated as follows:</p>

\[Q_n = \frac{R_1 + R_2 + ... + R_n}{n}\]

<p>However, if we estimate the value this way after n plays, the computational and memory load becomes large.
We can calculate the nth value estimate using the (n-1)th value estimate.</p>

\[Q_{n-1} = \frac{R_1 + R_2 + ... + R_{n-1}}{n-1}\]

<p>If we multiply both sides of this equation by (n-1):</p>

\[(n - 1)Q_{n-1} = R_1 + R_2 + ... + R_{n-1}\]

<p>Now we can calculate the nth value estimate:</p>

\[Q_n = \frac{1}{n} (R_1 + R_2 + ... + R_{n-1} + R_n)\]

\[=\frac{1}{n} (n - 1)Q_{n-1} + \frac{1}{n} R_n\]

\[= Q_{n - 1} + \frac{1}{n} (R_n - Q_{n - 1})\]

<h3 id="players-policy">Player‚Äôs Policy</h3>

<p>If we completely trust uncertain estimates, we might miss the best action. Therefore, the agent needs to reduce uncertainty and increase the reliability of estimation.</p>

<ul>
  <li><strong>Policy</strong>: The strategy that determines the actions an agent selects when interacting with the environment</li>
</ul>

<p>There are two policies that can be used to reduce uncertainty:</p>

<ol>
  <li><strong>Exploration</strong>: Selecting uncertain actions to gain information about the environment</li>
  <li><strong>Exploitation</strong>: Selecting the best action based on information available so far</li>
</ol>

<p><strong>Ultimately, reinforcement learning algorithms are about finding the right ‚Äòbalance between exploitation and exploration‚Äô!!!!</strong></p>

<h3 id="epsilon-greedy-policy">Epsilon-Greedy Policy</h3>
<p>This is one of the algorithms used to balance exploration and exploitation.
For example, if \(\epsilon\) = 0.1, it selects a random action with 10% probability and selects the best action with 90% probability.</p>

<h3 id="solving-the-bandit-problem">Solving the Bandit Problem</h3>

<ul>
  <li><strong>Action Value Estimation</strong>: Estimate the action value and select the best action.</li>
  <li><strong>Policy</strong>: Use the <strong>epsilon-greedy policy</strong> to balance <strong>exploration</strong> and <strong>exploitation</strong>.</li>
</ul>

<p>Let‚Äôs implement the above content in code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arms</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.38991635</span><span class="p">,</span> <span class="mf">0.5937864</span><span class="p">,</span>  <span class="mf">0.55356798</span><span class="p">,</span> <span class="mf">0.46228943</span><span class="p">,</span> <span class="mf">0.48251845</span><span class="p">,</span> <span class="mf">0.47595196</span><span class="p">,</span> <span class="mf">0.53560295</span><span class="p">,</span> <span class="mf">0.43374032</span><span class="p">,</span> <span class="mf">0.55913105</span><span class="p">,</span> <span class="mf">0.57484477</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
        <span class="n">rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">rate</span> <span class="o">&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span> <span class="p">:</span> 
            <span class="k">return</span> <span class="mi">0</span>


<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epslion</span><span class="p">,</span> <span class="n">action_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epslion</span> <span class="o">=</span> <span class="n">epslion</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Ns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Ns</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">])</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">Ns</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">epslion</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">steps</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="nc">Bandit</span><span class="p">()</span>

<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">()</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">bandit</span><span class="p">.</span><span class="nf">play</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
    <span class="n">rates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">act</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Actions</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Action</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<div align="center">
  <img src="/images/bandit.png" alt="bandit" width="100%" />
</div>

<p>After about 10,000 plays, it still doesn‚Äôt know that selecting the slot machine at index 1 as an action is optimal.
Let‚Äôs try with more steps.</p>

<div align="center">
  <img src="/images/bandit2.png" alt="bandit" width="100%" />
</div>

<p>After about 30,000 plays, it learns that selecting the slot machine at index 1 as an action is optimal.
It took an additional 20,000 plays to recognize a probability difference of about 2%.</p>

<h3 id="non-stationary-problem">Non-stationary Problem</h3>

<p>The bandit problem we‚Äôve covered so far belongs to the category of <strong>stationary problems</strong>. A stationary problem is one where the probability distribution of rewards <strong>does not change</strong>. In the code above, you can see that the probabilities are fixed in the variable called rates.</p>

<p>However, in reality, the probability distribution of rewards often changes. This is called a <strong>non-stationary problem</strong>. How should we handle this?</p>

<p>First, in stationary problems, we updated the action value estimate with the following equation:</p>

\[Q_n = Q_{n - 1} + \frac{1}{n} (R_n - Q_{n - 1})\]

<p>But in <strong>non-stationary problems</strong>, we update the action value estimate with the following equation:</p>

\[Q_n = Q_{n - 1} + \alpha (R_n - Q_{n - 1})\]

<p>This method <strong>reduces the weight of rewards obtained long ago</strong> and <strong>increases the weight of recently obtained rewards</strong>. Here, \(\alpha\) is called the <strong>learning rate</strong>.</p>

<div style="overflow-x: auto;">

$$
= Q_{n - 1} + \alpha (R_n - Q_{n - 1})
$$
$$
= (1 - \alpha) Q_{n - 1} + \alpha R_n
$$
$$
= \alpha R_n + (1 - \alpha) {(\alpha R_{n - 1} + (1 - \alpha) Q_{n - 2})}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 Q_{n - 2}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} + (1 - \alpha)^3 Q_{n - 3}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} +
$$
$$
(1 - \alpha)^3 \alpha R_{n - 3} + ... + (1 - \alpha)^{n - 1} \alpha R_1 + (1 - \alpha)^n Q_0
$$

</div>

<p>\(Q_0\) is the initial value. Depending on the value we set, bias can occur in the learning results. However, when using sample averages, the bias disappears.</p>

<p>This method is called <strong>exponential moving average</strong> or <strong>exponentially weighted moving average</strong>.</p>

<ul>
  <li><strong>Exponential Weighted Moving Average</strong>: A method that gives <strong>more weight to recently obtained rewards</strong> and <strong>less weight to rewards obtained long ago</strong></li>
</ul>

<p>Let‚Äôs implement this in Python code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arms</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.38991635</span><span class="p">,</span> <span class="mf">0.7837864</span><span class="p">,</span>  <span class="mf">0.55356798</span><span class="p">,</span> <span class="mf">0.46228943</span><span class="p">,</span> <span class="mf">0.48251845</span><span class="p">,</span> <span class="mf">0.47595196</span><span class="p">,</span> <span class="mf">0.53560295</span><span class="p">,</span> <span class="mf">0.43374032</span><span class="p">,</span> <span class="mf">0.55913105</span><span class="p">,</span> <span class="mf">0.57484477</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
        <span class="n">rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">+=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">rate</span> <span class="o">&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span> <span class="p">:</span> 
            <span class="k">return</span> <span class="mi">0</span>

<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epslion</span><span class="p">,</span> <span class="n">action_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epslion</span> <span class="o">=</span> <span class="n">epslion</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">epslion</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> 

<span class="n">steps</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="nc">Bandit</span><span class="p">()</span>

<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">()</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">bandit</span><span class="p">.</span><span class="nf">play</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
    <span class="n">rates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">act</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Actions</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Action</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/bandit3.png" alt="bandit" width="100%" />
</div>

<p>When we set the fixed value \(\alpha\) = 0.8, we can see that the results converge faster than when using sample averages.</p>

<h3 id="summary">Summary</h3>

<ul>
  <li><strong>Bandit Problem</strong>: A fundamental problem in reinforcement learning where the goal is to find a method that maximizes rewards among multiple slot machines</li>
  <li><strong>Action Value</strong>: The expected reward obtained as a result of an action</li>
  <li><strong>Policy</strong>: The strategy that determines the actions an agent selects when interacting with the environment</li>
  <li><strong>Epsilon-Greedy Policy</strong>: One of the algorithms used to balance <strong>exploration and exploitation</strong></li>
  <li><strong>Non-stationary Problem</strong>: A problem where the probability distribution of rewards changes</li>
  <li><strong>Exponential Weighted Moving Average</strong>: A method that gives <strong>more weight to recently obtained rewards</strong> and <strong>less weight to rewards obtained long ago</strong></li>
</ul>

<h2 id="markov-decision-process">Markov Decision Process</h2>

<p>Let‚Äôs examine problems where the state of the environment changes according to an agent‚Äôs actions.</p>

<h3 id="what-is-a-markov-decision-process">What is a Markov Decision Process?</h3>

<ul>
  <li>
    <p><strong>Markov Decision Process (MDP)</strong>: A method of modeling an environment where the agent interacts with the environment, and the environment‚Äôs state satisfies the Markov property</p>
  </li>
  <li>
    <p><strong>Markov Property</strong>: The property where the <strong>future state depends only on the current state</strong></p>
  </li>
</ul>

<p>MDPs require the concept of time. At a specific time, the agent takes an action, and as a result, transitions to a new state. The time unit in this case is called a time step.</p>

<div align="center">
  <div class="mermaid">
    graph LR
    A[Agent] --&gt;|action| B[Environment]
    B --&gt;|reward, state| A
  </div>
</div>

<ul>
  <li><strong>State Transition</strong>: How does the state transition?</li>
  <li><strong>Reward</strong>: How is the reward given?</li>
  <li><strong>Policy</strong>: How does the agent determine its actions?</li>
</ul>

<p>The above three elements must be expressed in formulas.</p>

<p>If the state transition is <strong>deterministic</strong>, the next state s‚Äô depends only on the current state s and action a.</p>

<p><strong>State transition function</strong> =&gt; 
\(s' = f(s, a)\)</p>

<p>If the state transition is <strong>probabilistic</strong>, the next state s‚Äô depends only on the current state s and action a.</p>

<p><strong>State transition probability</strong> =&gt;
\(P(s' | s, a)\)</p>

<h3 id="reward-function">Reward Function</h3>

<p>The <strong>reward function</strong> returns the reward for state s and action a. It returns the reward received when the agent takes action a in state s and moves to the next state s‚Äô.</p>

<p><strong>Reward function</strong> =&gt;
\(r(s, a, s')\)</p>

<h3 id="agents-policy">Agent‚Äôs Policy</h3>

<p>The agent‚Äôs <strong>policy</strong> refers to how the agent determines its actions. The agent determines its actions based solely on the <strong>‚Äòcurrent state‚Äô</strong>.
This is because <strong>‚Äòall the information needed about the environment is contained in the current state‚Äô</strong>.</p>

<p>A policy that the agent decides probabilistically can be expressed as follows:</p>

<p><strong>Policy</strong> =&gt;
\(\pi(a | s) = P(a | s)\)</p>

<h3 id="goal-of-mdp">Goal of MDP</h3>

<p>The goal of MDP is to find a policy that maximizes rewards. The agent behaves according to the policy 
\(\pi(a | s)\)
The next state is determined according to that action and the state transition probability \(P(s' | s, a)\). And the agent receives rewards according to the reward function \(r(s, a, s')\).</p>

<h3 id="return">Return</h3>

<p>The state at time t is \(S_t\), according to the policy \(\pi\), the action is \(A_t\), the reward is \(R_t\), and this leads to a flow that transitions to the new state \(S_{t+1}\). The return at this time can be defined as follows:</p>

\[G_t = R_t + rR_{t+1} + r^2R_{t+2} + ... = \sum_{k=0}^{\infty} r^k R_{t+k}\]

<p>As time passes, the reward decreases exponentially due to \(\gamma\).</p>

<h3 id="state-value-function">State Value Function</h3>

<p>The agent‚Äôs goal is to maximize returns. Even if an agent starts in the same state, the returns can vary for each episode. To respond to such stochastic behavior, we use the expectation, i.e., the expected return, as an indicator.</p>

<p>The state value function is a function that represents the expected value of rewards that can be received in the future, starting from a specific state in reinforcement learning. It is generally represented as \(V(s)\), where \(s\) represents the state. The state value function is calculated according to policy \(\pi\), and is defined by the following formula:</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s, \pi \right]
$$
</div>

\[= \mathbb{E}_\pi \left[G_t \mid S_t = s \right]\]

<p>Where:</p>
<ul>
  <li>\(\mathbb{E}_\pi\): Expected value according to policy \(\pi\)</li>
  <li>\(\gamma\): Discount rate (0 ‚â§ \(\gamma\) &lt; 1)</li>
  <li>\(R_{t+1}\): Reward at time \(t+1\)</li>
  <li>\(S_0 = s\): Initial state</li>
</ul>

<p>In other words, the state value function is used to predict the total rewards that will be received in the long term, starting from a specific state, when following a given policy. This plays an important role in evaluating the quality of a policy or finding the optimal policy.</p>

<h3 id="optimal-policy-and-optimal-value-function">Optimal Policy and Optimal Value Function</h3>

<p>In reinforcement learning, the optimal policy \(\pi^*\) is a policy that maximizes the expected reward in all states. If the agent follows the optimal policy, it can obtain the maximum possible reward.</p>

<p>The optimal value function \(V^*(s)\) is the sum of expected rewards that can be obtained when starting from state \(s\) and following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
V^*(s) = \max_{\pi} V^{\pi}(s) = \max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s \right]
$$
</div>

<p>Similarly, the optimal action-value function \(Q^*(s,a)\) is the sum of expected rewards that can be obtained when taking action \(a\) in state \(s\) and thereafter following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
Q^*(s,a) = \max_{\pi} Q^{\pi}(s,a) = \max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right]
$$
</div>

<p>The optimal policy and optimal value function can be defined through the Bellman Optimality Equation:</p>

<div style="overflow-x: auto;">
$$
V^*(s) = \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]
$$
</div>

<div style="overflow-x: auto;">
$$
Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')
$$
</div>

<p>The goal of reinforcement learning is to find such an optimal policy or optimal value function.</p>

<h2 id="bellman-equation">Bellman equation</h2>

<p>First, Summary of the Above.</p>

<ul>
  <li><strong>‚ùì What is an MDP?</strong></li>
</ul>

<p>An MDP is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under the control of an agent.</p>

<p>It consists of:</p>

<ul>
  <li>A set of <strong>states (S)</strong></li>
  <li>A set of <strong>actions (A)</strong></li>
  <li>A <strong>transition probability function (P)</strong></li>
  <li>A <strong>reward function (R)</strong></li>
  <li>A <strong>discount factor (Œ≥)</strong></li>
</ul>

<p>So, MDP is the <strong>foundation of reinforcement learning</strong>, where an agent learns to choose actions that maximize cumulative reward over time.</p>

<ul>
  <li><strong>‚ùìWhy is important Bellman equation in MDP?</strong></li>
</ul>

<p>The <strong>Bellman equation</strong> is important in Markov Decision Processes (MDPs) because it provides a <strong>recursive decomposition of the value function</strong>, which represents the expected return starting from a given state. It serves as the <strong>foundation for many reinforcement learning algorithms</strong>, enabling <strong>efficient computation of optimal policies</strong> by breaking down complex problems into smaller subproblems.</p>

<p>üîë <strong>Bellman Equation ‚Äì Easy Explanation (with Keywords)</strong></p>
<ul>
  <li>
    <p><strong>The Bellman equation expresses</strong>
‚Äú<strong>What kind of future reward can I expect if I act well in this state?</strong>‚Äù</p>
  </li>
  <li>
    <p><strong>It uses recursion to break down a complex problem into smaller subproblems.</strong></p>
  </li>
  <li>
    <p><strong>This allows us to efficiently and systematically optimize the overall policy.</strong></p>
  </li>
  <li>
    <p><strong>Many reinforcement learning algorithms like Q-learning and Value Iteration</strong>
are based on the Bellman equation.</p>
  </li>
</ul>

<h3 id="derivation-of-bellman-equation">Derivation of Bellman Equation.</h3>

<p>First, let‚Äôs define ‚ÄòReturn at time t‚Äô as the sum of rewards from time ‚Äòt‚Äô</p>

<div style="overflow-x: auto;">
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$
</div>

<p>Second, What is ‚ÄòReturn at time t + 1‚Äô?</p>
<div style="overflow-x: auto;">
$$
G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
</div>

<p>So, we can rearrange the equation as above two equatios.</p>

<div style="overflow-x: auto;">
$$
G_t = R_t + \gamma G_{t+1}
$$
</div>

<p>We know the relation between \(G_t\) and \(G_{t+1}\).</p>

<p>Based on the state-vlaue function \(V_\pi(s)\) we obtained earlier, we can derived the following conclusion.</p>

<div style="overflow-x: auto;">
$$
V_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
    = \mathbb{E}_\pi[R_t + \gamma G_{t+1} | S_t = s]
    = \mathbb{E}_\pi[R_t | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
$$
</div>

<p>(Since Linearity of Expectation üëâ \(\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]\))</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        s((s)) --&gt; A((A))
        s((s)) --&gt; B((B))
        s((s)) --&gt; C((C))
        A --&gt; A1((A1))
        A --&gt; A2((A2))
        B --&gt; B1((B1))
        B --&gt; B2((B2))
        C --&gt; C1((C1))
        C --&gt; C2((C2))
        style s fill:#ffffff,stroke:#000000,stroke-width:2px
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
        style A1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style A2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C2 fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>we define \(\pi(a | s)\) as the probability of taking action \(a\) in state \(s\).</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        s((s)) --&gt; A((A))
        s((s)) --&gt; B((B))
        s((s)) --&gt; C((C))
        style s fill:#ffffff,stroke:#000000,stroke-width:2px
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>so 
\(\pi(a_1 | s) = A\), 
\(\pi(a_2 | s) = B\), 
\(\pi(a_3 | s) = C\)</p>

<p>and we choose the action along with the policy \(\pi\). we move \(s\) to \(s'\) with the probability 
\(P(s' | s, a)\). (P is the transition probability function)</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        A((A)) --&gt; A1((A1))
        A --&gt; A2((A2))
        B((B)) --&gt; B1((B1))
        B --&gt; B2((B2))
        C((C)) --&gt; C1((C1))
        C --&gt; C2((C2))
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
        style A1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style A2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C2 fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>According to above graph,</p>

\[A_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[A_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

\[B_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[B_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

\[C_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[C_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

<p>Let‚Äôs generalize the above equation.</p>

<div style="overflow-x: auto;">

$$
\mathbb{E}_\pi[R_t | S_t = s] = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s')
$$

</div>

<div style="overflow-x: auto;">

$$
V_\pi(s) = \mathbb{E}_\pi[R_t | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
$$


$$
= \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s') + \gamma \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) V_\pi(s')
$$

$$
= \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right]
$$

</div>

<p>This is the <strong>Bellman euqation</strong> for the state value function.</p>

<h3 id="state-value-function-and-action-value-functionq-function">State Value Function and Action Value Function(Q-function)</h3>

<p>The state value function \(V_\pi(s)\) is the expected return starting from state \(s\) and following policy \(\pi\). It can be expressed as:</p>

<div style="overflow-x: auto;">
$$
V_\pi(s) = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right]
= \mathbb{E}_\pi \left[ G_t | S_t = s \right]
$$
</div>

<p>The Q-function represents the expected return when taking action a in state s at time t, and thereafter following policy œÄ.</p>

<div style="overflow-x: auto;">
$$
q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]
$$
</div>

<p>In short,</p>

<ul>
  <li>
    <p>policy 
\(\pi\)
determines how to act in a given state \(s\)</p>
  </li>
  <li>
    <p>Value function \(V_\pi(s)\) evalutes how good it is to be in a specific state under policy \(\pi\)</p>
  </li>
  <li>
    <p>Action value function(Q-function) \(q_\pi(s, a)\) evalutes how good it is to take a specific action in a given state under policy \(\pi\)</p>
  </li>
</ul>

<div style="overflow-x: auto;">
$$
q_\pi(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right] = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' | s') q_\pi(s', a') \right]
$$
</div>

<h3 id="optimal-action-value-function">optimal Action Value Function</h3>

<p>The optimal action value function \(q^*(s, a)\) is the maximum expected return when taking action \(a\) in state \(s\) and thereafter following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
q^*(s, a) = \max_{\pi} q_\pi(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t = s, A_t = a \right] = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \max_{a'} q^*(s', a') \right]
$$
</div>

<h3 id="optimal-policy">optimal Policy</h3>

<p>We assume that the optimal action value function \(q^*(s, a)\) is known. Then the optimal policy at state \(s\) is defined as follows.</p>

<div style="overflow-x: auto;">
$$
\mu^*(s) = \arg \max_a q^*(s, a)
$$
</div>

<h2 id="dynamic-programming">Dynamic Programming</h2>

<p>Dynamic programming is a method used to solve complex problems by breaking them down into simpler subproblems. It is particularly useful in reinforcement learning for solving Markov Decision Processes (MDPs).</p>

<h3 id="3x4-grid-world">3x4 grid world</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---+---+---+---+
| S |   |   | G |
+---+---+---+---+
|   | # |   | B |
+---+---+---+---+
|   |   |   |   |
+---+---+---+---+
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">S</code>: Start position</li>
  <li><code class="language-plaintext highlighter-rouge">G</code>: Goal position</li>
  <li><code class="language-plaintext highlighter-rouge">#</code>: Obstacle or blocked cell</li>
  <li><code class="language-plaintext highlighter-rouge">B</code>: Bomb location with a reward of <code class="language-plaintext highlighter-rouge">-1.0</code></li>
  <li>Blank cells are navigable spaces.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">common.gridworld_render</span> <span class="k">as</span> <span class="n">render_helper</span>


<span class="k">class</span> <span class="nc">GridWorld</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># ÌñâÎèô Í≥µÍ∞Ñ(Í∞ÄÎä•Ìïú ÌñâÎèôÎì§)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>  <span class="c1"># ÌñâÎèôÏùò ÏùòÎØ∏
</span>            <span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">UP</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">:</span> <span class="sh">"</span><span class="s">DOWN</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="sh">"</span><span class="s">LEFT</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">3</span><span class="p">:</span> <span class="sh">"</span><span class="s">RIGHT</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>  <span class="c1"># Î≥¥ÏÉÅ Îßµ(Í∞Å Ï¢åÌëúÏùò Î≥¥ÏÉÅ Í∞í)
</span>            <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
             <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
             <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>    <span class="c1"># Î™©Ìëú ÏÉÅÌÉú(Ï¢åÌëú)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>    <span class="c1"># Î≤Ω ÏÉÅÌÉú(Ï¢åÌëú)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>   <span class="c1"># ÏãúÏûë ÏÉÅÌÉú(Ï¢åÌëú)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>   <span class="c1"># ÏóêÏù¥Ï†ÑÌä∏ Ï¥àÍ∏∞ ÏÉÅÌÉú(Ï¢åÌëú)
</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>

    <span class="k">def</span> <span class="nf">states</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
                <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="c1"># Ïù¥Îèô ÏúÑÏπò Í≥ÑÏÇ∞
</span>        <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># Ïù¥ÎèôÌïú ÏúÑÏπòÍ∞Ä Í∑∏Î¶¨Îìú ÏõîÎìúÏùò ÌÖåÎëêÎ¶¨ Î∞ñÏù¥ÎÇò Î≤ΩÏù∏Í∞Ä?
</span>        <span class="k">if</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">elif</span> <span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

        <span class="k">return</span> <span class="n">next_state</span>  <span class="c1"># Îã§Ïùå ÏÉÅÌÉú Î∞òÌôò
</span>
    <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>

    <span class="k">def</span> <span class="nf">render_v</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_q</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="class-gridworld">Class: <code class="language-plaintext highlighter-rouge">GridWorld</code></h3>
<p>This class represents a simple grid-based environment for reinforcement learning. It defines the grid‚Äôs structure, the agent‚Äôs movement, and the rewards associated with each state.</p>

<hr />

<h4 id="__init__-method"><strong><code class="language-plaintext highlighter-rouge">__init__</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># up, down, left, right
</span>    <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">up</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="sh">'</span><span class="s">down</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">2</span><span class="p">:</span> <span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="sh">'</span><span class="s">right</span><span class="sh">'</span>
    <span class="p">}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">])</span>
    <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
</code></pre></div></div>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">action_space</code></strong>: Defines the possible actions the agent can take:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">0</code>: Move up</li>
      <li><code class="language-plaintext highlighter-rouge">1</code>: Move down</li>
      <li><code class="language-plaintext highlighter-rouge">2</code>: Move left</li>
      <li><code class="language-plaintext highlighter-rouge">3</code>: Move right</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">action_meaning</code></strong>: Maps action indices to human-readable directions.</p>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">reward_map</code></strong>: A 2D NumPy array representing the grid. Each cell contains:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">0</code>: Neutral reward.</li>
      <li><code class="language-plaintext highlighter-rouge">1.0</code>: Positive reward (goal state).</li>
      <li><code class="language-plaintext highlighter-rouge">-1.0</code>: Negative reward (bomb state).</li>
      <li><code class="language-plaintext highlighter-rouge">None</code>: Represents an obstacle (wall).</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">start_state</code></strong>: The agent‚Äôs starting position <code class="language-plaintext highlighter-rouge">(2, 0)</code> (row 2, column 0).</p>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">wall_state</code></strong>: The position of the wall <code class="language-plaintext highlighter-rouge">(1, 1)</code> (row 1, column 1), which the agent cannot pass through.</p>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">goal_state</code></strong>: The position of the goal <code class="language-plaintext highlighter-rouge">(0, 3)</code> (row 0, column 3).</p>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">agent_state</code></strong>: Tracks the agent‚Äôs current position, initialized to the start state.</li>
</ol>

<hr />

<h4 id="properties"><strong>Properties</strong></h4>
<p>These properties provide useful information about the grid.</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">height</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the number of rows in the grid.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">width</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the number of columns in the grid.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">shape</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the grid‚Äôs dimensions as a tuple <code class="language-plaintext highlighter-rouge">(rows, columns)</code>.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">actions</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the list of possible actions.</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="state-method"><strong><code class="language-plaintext highlighter-rouge">state</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
            <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>A generator that iterates over all possible states (grid cells) in the environment.</li>
  <li>Each state is represented as a tuple <code class="language-plaintext highlighter-rouge">(row, column)</code>.</li>
</ul>

<hr />

<h4 id="next_state-method"><strong><code class="language-plaintext highlighter-rouge">next_state</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">:</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">if</span> <span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">return</span> <span class="n">next_state</span>
</code></pre></div></div>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">action_move_map</code></strong>: Maps actions to their corresponding movements:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">(-1, 0)</code>: Move up (decrease row index).</li>
      <li><code class="language-plaintext highlighter-rouge">(1, 0)</code>: Move down (increase row index).</li>
      <li><code class="language-plaintext highlighter-rouge">(0, -1)</code>: Move left (decrease column index).</li>
      <li><code class="language-plaintext highlighter-rouge">(0, 1)</code>: Move right (increase column index).</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">next_state</code></strong>: Calculates the agent‚Äôs next position based on the current state and action.</p>
  </li>
  <li><strong>Boundary Check</strong>:
    <ul>
      <li>If the next state is outside the grid‚Äôs boundaries, the agent stays in the current state.</li>
    </ul>
  </li>
  <li><strong>Wall Check</strong>:
    <ul>
      <li>If the next state is a wall, the agent stays in the current state.</li>
    </ul>
  </li>
  <li><strong>Returns</strong>: The valid next state after applying the action.</li>
</ol>

<hr />

<h4 id="reward-method"><strong><code class="language-plaintext highlighter-rouge">reward</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
</code></pre></div></div>

<ol>
  <li><strong>Inputs</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">state</code>: The current state.</li>
      <li><code class="language-plaintext highlighter-rouge">action</code>: The action taken.</li>
      <li><code class="language-plaintext highlighter-rouge">next_state</code>: The resulting state after the action.</li>
    </ul>
  </li>
  <li><strong>Returns</strong>: The reward associated with the <code class="language-plaintext highlighter-rouge">next_state</code>, as defined in the <code class="language-plaintext highlighter-rouge">reward_map</code>.</li>
</ol>

<hr />

<h3 id="summary-1">Summary</h3>
<p>The <code class="language-plaintext highlighter-rouge">GridWorld</code> class provides a simple environment for reinforcement learning:</p>
<ul>
  <li>It defines the grid layout, including walls, rewards, and penalties.</li>
  <li>It allows the agent to move within the grid while handling boundaries and obstacles.</li>
  <li>It provides rewards based on the agent‚Äôs position.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">()</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/gridworld1.png" alt="gridworld" width="100%" />
</div>

<h3 id="implementation-of-iterative-policy-evaluation">Implementation of Iterative Policy Evaluation</h3>

<p>First, let‚Äôs implement a function that performs a single step of the update.</p>

<ul>
  <li>pi(difaultdict) : <code class="language-plaintext highlighter-rouge">policy</code></li>
  <li>V (defaultdict) : <code class="language-plaintext highlighter-rouge">value function</code></li>
  <li>env(GridWorld) : <code class="language-plaintext highlighter-rouge">environment</code></li>
  <li>gamma (float) : <code class="language-plaintext highlighter-rouge">discount factor</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">})</span>
<span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">env</span><span class="p">.</span><span class="n">goal_state</span><span class="p">:</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="k">continue</span>

        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">new_V</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_prob</span> <span class="ow">in</span> <span class="n">action_probs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">new_V</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_V</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
</code></pre></div></div>

<p>If we try one step of the update, we can see the result below.</p>

<div align="center">
  <img src="/images/gridworld2.png" alt="gridworld" width="100%" />   
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">new_V</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_prob</span> <span class="ow">in</span> <span class="n">action_probs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">new_V</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_V</span>
</code></pre></div></div>

<p>this code is defined as follows.</p>

\[s' = f(s, a)\]

<p>and,</p>

\[V_{k + 1}(s) = \sum_{a} \pi(a | s) \left[r(s, a, s') + \gamma V_k(s') \right]\]

<p>Therefore, we continue repeating this process until the threshold is reached.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">policy_eval</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">old_V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">V</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">t</span><span class="p">:</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="n">t</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/gridworld3.png" alt="gridworld" width="100%" />
</div>

<h3 id="policy-iteration-method">Policy Iteration Method</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Optimal Policy</code>: \(\pi^*(s)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Value Function</code>: \(V^*(s)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Action Value Function</code>: \(Q^*(s, a)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Action</code>:</li>
</ul>
<div style="overflow-x: auto;">

$$ \mu^*(s) = \arg \max_a Q^*(s, a) $$

$$
= \arg \max_a \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]
$$
</div>

<p>What do we call the process of finding the optimal policy through repeated evaluation and greedification?</p>

<p>Policy Iteration is an algorithm for findings the optmal policy in a MDPs by alternating between two phase.</p>

<ul>
  <li>
    <p>1 <strong>Policy Evaluation</strong>: calculate the value function for the current policy by iteratively applying the Bellman expectation equation until convergence.</p>
  </li>
  <li>
    <p>2 <strong>Policy Improvement</strong>: update the policy to be greedy with respect to the current value function. This means for each state, selecting the action that maximizes expected value.</p>
  </li>
</ul>

<p>By repeating these two steps until the policy no longer changes, we can find the optimal policy. This approach is guaranteed to converge to the optimal policy in finite MDPs.</p>

<p>In gridworld, since states transition uniquely, we can define greedification as follows.</p>

<div style="overflow-x: auto;">
$$
\mu^*(s) = \arg \max_a \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]
= \arg \max_a \left[ R(s, a, s') + \gamma V^*(s') \right]
$$
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
        <span class="n">max_action</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">action_values</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="p">{</span><span class="n">a</span><span class="p">:</span> <span class="mf">0.0</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">()}</span>
        <span class="n">action_probs</span><span class="p">[</span><span class="n">max_action</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_probs</span>
    <span class="k">return</span> <span class="n">pi</span>

<span class="k">def</span> <span class="nf">policy_iter</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">is_render</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">})</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">policy_eval</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="n">new_pi</span> <span class="o">=</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_render</span><span class="p">:</span>
            <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_pi</span> <span class="o">==</span> <span class="n">pi</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="n">new_pi</span>
    <span class="k">return</span> <span class="n">pi</span><span class="p">,</span> <span class="n">V</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">policy_iter</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>this result is two step of the policy iteration.</p>

<div align="center">
  <img src="/images/gridworld4.png" alt="gridworld" width="100%" />
</div>

<p>Four steps of the policy iteration are as follows.</p>

<div align="center">
  <img src="/images/gridworld5.png" alt="gridworld" width="100%" />
</div>

<p>By implementing it this way, the value function of all states is updated multiple times. It‚Äôs too slow. Is there a way to update only one state‚Äôs value function and proceed?</p>

<h3 id="value-iteration-method">Value Iteration Method</h3>

<h4 id="why-value-iteration-works">Why Value Iteration Works</h4>

<p>Policy Iteration has two separate steps - policy evaluation (which runs until convergence) and policy improvement. This is computationally expensive because we‚Äôre repeatedly evaluating the entire state space multiple times before making a single policy improvement.</p>

<p>Value Iteration addresses this inefficiency by recognizing that:</p>

<ul>
  <li><strong>Similar Calculations</strong> - Both policy evaluation and improvement use the Bellman equation structure</li>
  <li><strong>Partial Convergence</strong> - We can improve the policy before the value function fully converges</li>
  <li><strong>Combined Steps</strong> - We can directly incorporate the max operation into the value update</li>
</ul>

<h4 id="how-value-iteration-works">How Value Iteration Works</h4>

<p>Value Iteration combines policy evaluation and improvement into a single update:</p>

\[V(s) \leftarrow \max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]\]

<p>This update directly finds the value of the best action for each state, effectively:</p>

<ul>
  <li>Assuming a greedy policy at each step</li>
  <li>Skipping the explicit policy representation</li>
  <li>Performing only one sweep through the state space per iteration</li>
</ul>

<h4 id="value-iteration-algorithm">Value Iteration Algorithm</h4>

<ol>
  <li>Initialize \(V(s) = 0\) for all states</li>
  <li>Repeat until convergence:
    <ul>
      <li>For each state \(s\):</li>
    </ul>
    <div style="overflow-x: auto;">
      $$V(s) \leftarrow \max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]$$
 </div>
  </li>
  <li>Extract the final policy:</li>
</ol>
<div style="overflow-x: auto;"> 
   $$\pi(s) = \arg\max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]$$
</div>

<h4 id="advantages-over-policy-iteration">Advantages Over Policy Iteration</h4>

<ul>
  <li><strong>Computational Efficiency</strong> - No need to perform full policy evaluation at each step</li>
  <li><strong>Fewer Iterations</strong> - Usually converges in fewer sweeps through the state space</li>
  <li><strong>Simplicity</strong> - Only need to maintain a value function, not an explicit policy</li>
  <li><strong>Direct Optimization</strong> - Works towards optimal values from the start</li>
</ul>

<p>For deterministic environments like our grid world example, the update becomes even simpler:</p>

<div style="overflow-x: auto;">
$$V(s) \leftarrow \max_a \left[R(s,a) + \gamma \cdot V(\text{next_state}(s,a)) \right]$$
</div>

<p>This makes Value Iteration particularly efficient for deterministic problems.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">value_iter_onestep</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">env</span><span class="p">.</span><span class="n">goal_state</span><span class="p">:</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">continue</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="k">def</span> <span class="nf">value_iter</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_render</span><span class="p">:</span>
            <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
        <span class="n">old_V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">value_iter_onestep</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">V</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">t</span><span class="p">:</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="n">t</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">value_iter</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
</code></pre></div></div>

<ol>
  <li>one step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld6.png" alt="gridworld" width="100%" />
</div>

<ol>
  <li>two step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld7.png" alt="gridworld" width="100%" />
</div>

<ol>
  <li>three step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld8.png" alt="gridworld" width="100%" />
</div>
<ol>
  <li>four step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld9.png" alt="gridworld" width="100%" />
</div>

<p>So, the result of optimal policy is as follows.</p>

<div align="center">
  <img src="/images/gridworld10.png" alt="gridworld" width="100%" />
</div>

<h2 id="monte-carlo-method">Monte Carlo Method</h2>

<p>We know the transition probabilities \(P( s, a)\) and the reward function \(R\), which allows us to apply Dynamic Programming.</p>

<p>Also, using dynamic programming (DP) is too complex to calculate the entire problem.</p>

<p><strong>What is Monte Carlo method?</strong></p>

<p>It assumes a value function for the agent to gain experience in an environment.
The experience mentioned here refers to the data (state, action, reward) obtained through the interaction between the environment and the agent.</p>

<p>The following situation can be considered: Think about all possible outcomes when rolling a dice twice.</p>

<div align="center">
  <div class="mermaid">
graph TD
    Start((Start))
    Start --&gt; D1((Die 1: 1))
    Start --&gt; D2((Die 1: 2))
    Start --&gt; D3((Die 1: 3))
    Start --&gt; D4((Die 1: 4))
    Start --&gt; D5((Die 1: 5))
    Start --&gt; D6((Die 1: 6))

    D1 --&gt; D1_1((Die 2: 1))
    D1 --&gt; D1_2((Die 2: 2))
    D1 --&gt; D1_3((Die 2: 3))
    D1 --&gt; D1_4((Die 2: 4))
    D1 --&gt; D1_5((Die 2: 5))
    D1 --&gt; D1_6((Die 2: 6))

    D2 --&gt; D2_1((Die 2: 1))
    D2 --&gt; D2_2((Die 2: 2))
    D2 --&gt; D2_3((Die 2: 3))
    D2 --&gt; D2_4((Die 2: 4))
    D2 --&gt; D2_5((Die 2: 5))
    D2 --&gt; D2_6((Die 2: 6))

    D3 --&gt; D3_1((Die 2: 1))
    D3 --&gt; D3_2((Die 2: 2))
    D3 --&gt; D3_3((Die 2: 3))
    D3 --&gt; D3_4((Die 2: 4))
    D3 --&gt; D3_5((Die 2: 5))
    D3 --&gt; D3_6((Die 2: 6))

    D4 --&gt; D4_1((Die 2: 1))
    D4 --&gt; D4_2((Die 2: 2))
    D4 --&gt; D4_3((Die 2: 3))
    D4 --&gt; D4_4((Die 2: 4))
    D4 --&gt; D4_5((Die 2: 5))
    D4 --&gt; D4_6((Die 2: 6))

    D5 --&gt; D5_1((Die 2: 1))
    D5 --&gt; D5_2((Die 2: 2))
    D5 --&gt; D5_3((Die 2: 3))
    D5 --&gt; D5_4((Die 2: 4))
    D5 --&gt; D5_5((Die 2: 5))
    D5 --&gt; D5_6((Die 2: 6))

    D6 --&gt; D6_1((Die 2: 1))
    D6 --&gt; D6_2((Die 2: 2))
    D6 --&gt; D6_3((Die 2: 3))
    D6 --&gt; D6_4((Die 2: 4))
    D6 --&gt; D6_5((Die 2: 5))
    D6 --&gt; D6_6((Die 2: 6))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class Start,D1,D2,D3,D4,D5,D6,D1_1,D1_2,D1_3,D1_4,D1_5,D1_6,D2_1,D2_2,D2_3,D2_4,D2_5,D2_6,D3_1,D3_2,D3_3,D3_4,D3_5,D3_6,D4_1,D4_2,D4_3,D4_4,D4_5,D4_6,D5_1,D5_2,D5_3,D5_4,D5_5,D5_6,D6_1,D6_2,D6_3,D6_4,D6_5,D6_6 circle;
  </div>
</div>

<p>If some outcomes represent a probability distribution, we use the sample distribution.
A sample distribution is a method of observing the results of actual sampling.</p>

<p>Let‚Äôs use the incremental method learned earlier to sample and calculate the expected value of the sum when two dice are rolled.</p>

<p><code class="language-plaintext highlighter-rouge">incremental method</code> : \(V_n = V_{n - 1} + \frac{1}{n} (s_n - V_{n - 1})\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trial</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">def</span> <span class="nf">Sample</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="n">trial</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">V</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nc">Sample</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">V</span> <span class="o">+=</span> <span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trial </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">: Sample mean = </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># result
# Trial 100: Sample mean = 7.119999999999997
# Trial 200: Sample mean = 6.8199999999999985
# Trial 300: Sample mean = 6.783333333333331
# Trial 400: Sample mean = 6.8575
# Trial 500: Sample mean = 6.844000000000001
# Trial 600: Sample mean = 6.861666666666671
# Trial 700: Sample mean = 6.8885714285714315
# Trial 800: Sample mean = 6.8999999999999995
# Trial 900: Sample mean = 6.948888888888891
# Trial 1000: Sample mean = 6.938000000000002
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Value - Function</code> : \(V_n = \mathbb{E_{\pi}}[G \mid s]\)</p>

<p>This method applies the Monte Carlo approach to estimate values.</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}
$$
</div>

<p>where \(G^{(i)}\) is the return of the \(i\)-th episode.</p>

<p>Let me explain the first trial episode.</p>

<div align="center">
  <div class="mermaid">
graph TD
    S((S))
    S --&gt;|reward_1| A((A))
    A --&gt;|reward_0| B((B))
    B --&gt;|reward_2| END((END))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class S,A,B,C,END circle;
  </div>
</div>

<div style="overflow-x: auto;">
$$
G^{(1)} = 1 + 0 + 2 = 3
$$
</div>

<p>The second trial episode is as follows.</p>

<div align="center">
  <div class="mermaid">
graph TD
    S((S))
    S --&gt;|reward_1| A((A))
    A --&gt;|reward_0| B((B))
    B --&gt;|reward_1| C((C))
    C --&gt;|reward_1| END((END))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class S,A,B,C,END circle;

  </div>
</div>

<div style="overflow-x: auto;">
$$
G^{(2)} = 1 + 0 + 1 + 1 = 3
$$
</div>

<p>As a result, the expected value is as follows.</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}}{2} = \frac{3 + 3}{2} = 3
$$
</div>

<p>Let‚Äôs calculate the value function for all states using the Monte Carlo method. If there are three states (A, B, C), sample data is obtained by performing actual actions.</p>

<div align="center">
  <div class="mermaid">
flowchart TD

    %% A ÌååÏù¥ÌîÑÎùºÏù∏ 1
    A1((A)) --&gt; A2([...]) --&gt; A3([...]) --&gt; Aout((‚óã))

    %% A ÌååÏù¥ÌîÑÎùºÏù∏ 2
    A1b((A)) --&gt; A2b([...]) --&gt; A3b([...]) --&gt; Aoutb((‚óã))


    %% B ÌååÏù¥ÌîÑÎùºÏù∏ 1
    B1((B)) --&gt; B2([...]) --&gt; B3([...]) --&gt; Bout((‚óã))

    %% B ÌååÏù¥ÌîÑÎùºÏù∏ 2
    B1b((B)) --&gt; B2b([...]) --&gt; B3b([...]) --&gt; Boutb((‚óã))

    %% C ÌååÏù¥ÌîÑÎùºÏù∏ 1
    C1((C)) --&gt; C2([...]) --&gt; C3([...]) --&gt; Cout((‚óã))

    %% C ÌååÏù¥ÌîÑÎùºÏù∏ 2
    C1b((C)) --&gt; C2b([...]) --&gt; C3b([...]) --&gt; Coutb((‚óã))

    %% Ïä§ÌÉÄÏùº ÏßÄÏ†ï
    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px

    class A1,A2,A3,Aout,A1b,A2b,A3b,Aoutb aStyle
    class B1,B2,B3,Bout,B1b,B2b,B3b,Boutb bStyle
    class C1,C2,C3,Cout,C1b,C2b,C3b,Coutb cStyle
</div>
</div>

<p>Let‚Äôs consider starting from state A, taking actions according to policy \(\pi\), and reaching the final destination.</p>

<div align="center">
  <div class="mermaid">
graph TD
    A((A))
    A --&gt;|R0| B((B))
    B --&gt;|R1| C((C))
    C --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class A aStyle
    class B bStyle
    class C cStyle
    class END circle;
</div>
</div>

<p>The total rewards accumulated from state A to the end are as follows.</p>

<div style="overflow-x: auto;">
$$
G_A = R_0 + \gamma R_1 + \gamma^2 R_2
$$
</div>

<p>Let‚Äôs consider starting from state B.</p>

<div align="center">
  <div class="mermaid">
graph TD
    B((B)) --&gt;|R1| C((C))
    C --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class B bStyle
    class C cStyle
    class END circle;
</div>
</div>

<div style="overflow-x: auto;">
$$
G_B = R_1 + \gamma R_2
$$
</div>

<p>Let‚Äôs consider starting from state C.</p>

<div align="center">
  <div class="mermaid">
graph TD
    C((C)) --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class C cStyle
    class END circle;
</div>
</div>

<div style="overflow-x: auto;">
$$
G_C = R_2
$$
</div>

<p>So, the following sequence of calculations can eliminate redundant computations.</p>

<div style="overflow-x: auto;">
$$
G_C = R_2
$$
</div>

<div style="overflow-x: auto;">
$$
G_B = R_1 + \gamma G_C
$$
</div>

<div style="overflow-x: auto;">
$$
G_A = R_0 + \gamma G_B
$$
</div>

<h3 id="implement">implement</h3>

<p>Alright, according to the reading, we can implement this for the agent to interact with the environment.</p>

<div align="center">
  <img src="/images/RL.png" alt="RL" width="100%" />
</div>

<p>The start point is (0, 0), the end point is (5, 5), and the black cell represents a wall that the agent cannot pass.</p>

<div align="center">
  <img src="/images/monemap.png" alt="MM" width="80%" />
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">goal</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cnts</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">):</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span>
            <span class="n">self</span><span class="p">.</span><span class="n">cnts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">cnts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>


<span class="n">agent</span> <span class="o">=</span> <span class="nc">RandomAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1e3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Training Progress</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<p>The value function (expected value) of each cell obtained through the Monte Carlo method is as follows.</p>

<div align="center">
  <img src="/images/MM1.png" alt="MM" width="80%" />
</div>

<h3 id="policy-control-using-the-monte-carlo-method">Policy Control Using the Monte Carlo Method</h3>

<p>The optimal policy alternates between evaluation and improvement.</p>
<ul>
  <li><strong>Policy Evaluation</strong>: Calculate the value function for the current policy using the Monte Carlo method.</li>
  <li><strong>Policy Improvement</strong>: Update the policy to be greedy with respect to the current value function.</li>
</ul>

<p>State Value Function Evaluation</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">General Method</code> : \(V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}\)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Incremental Method</code> : \(V_{\pi}(s) = V_{\pi}(s) + \frac{1}{n} (G - V_{\pi}(s))\)</p>
  </li>
</ul>

<p>Q-Function Evaluation</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">General Method</code> : \(Q_{\pi}(s, a) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}\)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Incremental Method</code> : \(Q_{\pi}(s, a) = Q_{\pi}(s, a) + \frac{1}{n} (G - Q_{\pi}(s, a))\)</p>
  </li>
</ul>

<h3 id="important-concept">important concept</h3>

<ol>
  <li>
    <p>Use an epsilon-greedy policy to give the agent opportunities to explore.</p>
  </li>
  <li>
    <p>Train the model by applying an exponential moving average with a fixed value <code class="language-plaintext highlighter-rouge">a</code>, giving greater weight to more recent data.</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">common.gridworld_render</span> <span class="k">as</span> <span class="n">render_helper</span>


<span class="k">class</span> <span class="nc">GridWorld</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># ÌñâÎèô Í≥µÍ∞Ñ(Í∞ÄÎä•Ìïú ÌñâÎèôÎì§)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>  <span class="c1"># ÌñâÎèôÏùò ÏùòÎØ∏
</span>            <span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">UP</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">:</span> <span class="sh">"</span><span class="s">DOWN</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="sh">"</span><span class="s">LEFT</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">3</span><span class="p">:</span> <span class="sh">"</span><span class="s">RIGHT</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">reward_map</span> <span class="k">if</span> <span class="n">reward_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="p">])</span>

        <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="n">goal</span>
        <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span>
            <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">==</span> <span class="bp">None</span><span class="p">)))</span>  <span class="c1"># Î≤Ω ÏÉÅÌÉú(Ï¢åÌëú)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="n">start</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>   <span class="c1"># ÏóêÏù¥Ï†ÑÌä∏ Ï¥àÍ∏∞ ÏÉÅÌÉú(Ï¢åÌëú)
</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>

    <span class="k">def</span> <span class="nf">states</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
                <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="c1"># Ïù¥Îèô ÏúÑÏπò Í≥ÑÏÇ∞
</span>        <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># Ïù¥ÎèôÌïú ÏúÑÏπòÍ∞Ä Í∑∏Î¶¨Îìú ÏõîÎìúÏùò ÌÖåÎëêÎ¶¨ Î∞ñÏù¥ÎÇò Î≤ΩÏù∏Í∞Ä?
</span>        <span class="k">if</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">elif</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

        <span class="k">return</span> <span class="n">next_state</span>  <span class="c1"># Îã§Ïùå ÏÉÅÌÉú Î∞òÌôò
</span>
    <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>

    <span class="k">def</span> <span class="nf">render_v</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_q</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>
</code></pre></div></div>

<p>Define the grid environment and the way the agent moves (policy).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">sys</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="k">def</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">action_size</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">qs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Q</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)]</span> <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">action_size</span><span class="p">)]</span>
    <span class="n">max_action</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">qs</span><span class="p">))</span>

    <span class="n">base_prob</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="o">/</span> <span class="n">action_size</span>
    <span class="n">action_probs</span> <span class="o">=</span> <span class="p">{</span><span class="n">action</span><span class="p">:</span> <span class="n">base_prob</span> <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">action_size</span><span class="p">)}</span>  <span class="c1">#{0: Œµ/4, 1: Œµ/4, 2: Œµ/4, 3: Œµ/4}
</span>    <span class="n">action_probs</span><span class="p">[</span><span class="n">max_action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">action_probs</span>


<span class="k">class</span> <span class="nc">McAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># (Ï≤´ Î≤àÏß∏ Í∞úÏÑ†) Œµ-ÌÉêÏöï Ï†ïÏ±ÖÏùò Œµ
</span>        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>    <span class="c1"># (Îëê Î≤àÏß∏ Í∞úÏÑ†) Q Ìï®Ïàò Í∞±Ïã† ÏãúÏùò Í≥†Ï†ïÍ∞í Œ±
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># self.cnts = defaultdict(lambda: 0)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">):</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span> <span class="o">+</span> <span class="n">reward</span>
            <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="c1"># self.cnts[key] += 1
</span>            <span class="c1"># self.Q[key] += (G - self.Q[key]) / self.cnts[key]
</span>            <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>
            <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span>


<span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">goal</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">McAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Training Progress</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">Q</span><span class="p">)</span>
</code></pre></div></div>

<p>The agent alternates between exploration and exploitation over 10,000 episodes. Additionally, it reduces the weight of past experiences and assigns higher weight to the rewards obtained through current experiences.</p>

<p>The Q(S, a) for each state is as follows.</p>

<div align="center">
  <img src="/images/Figure_1.png" alt="bandit" width="100%" />
</div>

<p>And, The Optimal Policy for each state is as follows.</p>

<div style="display: flex; justify-content: center; gap: 10px;">
  <img src="/images/Figure_2.png" alt="bandit1" style="width: 48%;" />
  <img src="/images/Figure_3.png" alt="bandit2" style="width: 48%;" />
</div>

<h3 id="montecarlo-method-quiz">Montecarlo method QUIZ!</h3>

<details>
<summary>1. ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V(s)\) Ï∂îÏ†ï</summary>

Î™¨ÌÖåÏπ¥Î•ºÎ°ú Î∞©Î≤ïÏóêÏÑú ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V(s)\)Î•º Ï∂îÏ†ïÌï† Îïå, Ï†ÑÏ≤¥ ÏóêÌîºÏÜåÎìúÏùò ÌèâÍ∑†ÏùÑ Ïù¥Ïö©ÌïòÎäî ÏùºÎ∞òÏ†ÅÏù∏ Î∞©Î≤ïÍ≥º Ï†êÏßÑÏ†Å(incremental) Î∞©Î≤ïÏùò Ï∞®Ïù¥Ï†êÏùÑ ÏÑ§Î™ÖÌïòÎùº. Ïù¥ Îëê Î∞©Î≤ïÏùò Ïû•Îã®Ï†êÏùÑ ÎπÑÍµêÌïòÍ≥†, Ïã§Ï†ú Íµ¨ÌòÑ Ïãú Ïñ¥Îñ§ Í≤ΩÏö∞Ïóê Ï†êÏßÑÏ†Å Î∞©Î≤ïÏù¥ Îçî Ïú†Î¶¨ÌïúÏßÄ ÏÑúÏà†ÌïòÎùº.
 

<details>
<summary> 1.Ï†ïÎãµ </summary>
ÏùºÎ∞òÏ†ÅÏù∏ Î∞©ÏãùÏùÄ Í∞Å ÏÉÅÌÉúÏóêÏÑú ÏñªÏñ¥ÏßÑ Î™®Îì† return Í∞íÏùÑ Ï†ÄÏû•Ìïú Îí§, Ïù¥Îì§Ïùò ÌèâÍ∑†ÏùÑ ÌÜµÌï¥ ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V(s)\)Î•º Í≥ÑÏÇ∞ÌïòÎØÄÎ°ú Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ Ïª§ÏßÄÍ≥†, ÏóêÌîºÏÜåÎìú ÏàòÍ∞Ä ÎßéÏïÑÏßàÏàòÎ°ù Í≥ÑÏÇ∞Îüâ ÎòêÌïú ÎàÑÏ†ÅÏ†ÅÏúºÎ°ú Ï¶ùÍ∞ÄÌïòÍ≤å ÎêúÎã§. Î∞òÎ©¥ Ï¶ùÎ∂Ñ Î∞©Ïãù(incremental update)ÏùÄ ÏßÅÏ†ÑÍπåÏßÄÏùò ÌèâÍ∑†Í∞í \(V_{n-1}(s)\)ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÉàÎ°ú Îì§Ïñ¥Ïò® return \(G_n\)ÏùÑ Îã®Ïùº ÏàòÏãùÏúºÎ°ú Í∞±Ïã†ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê, Í≥ºÍ±∞ Îç∞Ïù¥ÌÑ∞Î•º Î™®Îëê Ï†ÄÏû•Ìï† ÌïÑÏöî ÏóÜÏù¥ Ïò®ÎùºÏù∏ ÌôòÍ≤ΩÏóêÏÑúÎèÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏûëÎèôÌïòÎ©∞, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Ïù¥ Ï†ÅÍ≥† Ïó∞ÏÇ∞ÎèÑ Í∞ÄÎ≥çÎã§Îäî Ïû•Ï†êÏù¥ ÏûàÎã§. Îî∞ÎùºÏÑú Îç∞Ïù¥ÌÑ∞Í∞Ä Ïã§ÏãúÍ∞ÑÏúºÎ°ú Îì§Ïñ¥Ïò§Í±∞ÎÇò ÏóêÌîºÏÜåÎìú ÏàòÍ∞Ä ÏÇ¨Ï†ÑÏóê Ï†ïÌï¥ÏßÄÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ÏóêÎäî Ï¶ùÎ∂Ñ Î∞©ÏãùÏù¥ ÌäπÌûà Ïú†Î¶¨ÌïòÎã§.
</details>
</details>

<details>
<summary>2. ÏóêÌîºÏÜåÎìúÏôÄ Î¶¨ÌÑ¥ \(G_0\)</summary>

\(S_0 \rightarrow a_0, r_0 \rightarrow S_1 \rightarrow a_1, r_1 \rightarrow S_2 \rightarrow a_2, r_2 \rightarrow end\)

Ìï†Ïù∏Ïú® \(\gamma\)Í∞Ä Ï£ºÏñ¥Ï°åÏùÑ Îïå, Ïù¥ ÏóêÌîºÏÜåÎìúÎ•º Î∞îÌÉïÏúºÎ°ú \(S_0\) ÏÉÅÌÉúÏùò Î¶¨ÌÑ¥ \(G_0\)ÏùÑ Ï†ïÏùòÌïòÎùº. Í∑∏Î¶¨Í≥† Ïù¥Î•º Í∏∞Î∞òÏúºÎ°ú Î™¨ÌÖåÏπ¥Î•ºÎ°ú Î∞©Î≤ïÏúºÎ°ú \(V(S_0)\)ÏùÑ Ïñ¥ÎñªÍ≤å Ï∂îÏ†ïÌïòÎäîÏßÄ ÏàòÏãùÍ≥º Ìï®Íªò ÏÑúÏà†ÌïòÎùº.

<br />

<details>
<summary> 2.Ï†ïÎãµ </summary>
ÏóêÏù¥Ï†ÑÌä∏Í∞Ä \(s_0\) ÏÉÅÌÉúÏóêÏÑú ÏãúÏûëÌï¥ \(r_0, r_1, r_2\)Ïùò Î≥¥ÏÉÅÏùÑ ÏàúÏ∞®Ï†ÅÏúºÎ°ú Î∞õÎäî Í≤ΩÏö∞, 
\(G_0\)Îäî Ìï†Ïù∏Ïú® \(\gamma\)Î•º Ï†ÅÏö©Ìïú ÎàÑÏ†Å Î≥¥ÏÉÅÏúºÎ°ú Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÎêúÎã§:

\[
G_0 = r_0 + \gamma r_1 + \gamma^2 r_2
\]

Ïù¥Îäî Ï≤´ Î≤àÏß∏ ÏÉÅÌÉúÏóêÏÑúÏùò Ï¥ù returnÏùÑ ÏùòÎØ∏ÌïòÎ©∞, Ïù¥Î•º Ïó¨Îü¨ ÏóêÌîºÏÜåÎìúÏóêÏÑú Î∞òÎ≥µ Ï∏°Ï†ïÌïòÏó¨ 
ÏñªÏùÄ ÌèâÍ∑†Í∞íÏùÑ ÌÜµÌï¥ ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V(s_0)\)Î•º Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï∂îÏ†ïÌï† Ïàò ÏûàÎã§:

\[
V(s_0) = \mathbb{E}_\pi [G_0 \mid s_0]
\]

Monte Carlo Î∞©ÏãùÏùÄ Ïù¥ÏôÄ Í∞ôÏù¥ Í∞Å ÏóêÌîºÏÜåÎìúÏùò returnÏùÑ Ïù¥Ïö©ÌïòÏó¨ ÏßÅÏ†ëÏ†ÅÏù∏ Ï∂îÏ†ïÏùÑ ÏàòÌñâÌïòÎ©∞, 
Ï†ïÏ±Ö \(\pi\)Ïóê Îî∞Îùº ÏñªÏñ¥ÏßÑ Ïã§Ï†ú Í≤ΩÎ°úÏùò Í≤ΩÌóòÏùÑ Î∞îÌÉïÏúºÎ°ú Í∏∞ÎåÄÍ∞íÏùÑ Í∑ºÏÇ¨ÌïúÎã§.
</details>
</details>

<details>
<summary>3. Ï§ëÏöîÎèÑ ÏÉòÌîåÎßÅ</summary>

Ïò§ÌîÑ-Ï†ïÏ±Ö Î™¨ÌÖåÏπ¥Î•ºÎ°ú ÏòàÏ∏°ÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Ï§ëÏöîÎèÑ ÏÉòÌîåÎßÅ(importance sampling)Ïùò Í∞úÎÖêÏùÑ ÏÑ§Î™ÖÌïòÎùº. Í∑∏Î¶¨Í≥† Ordinary Importance SamplingÍ≥º Weighted Importance SamplingÏùò Ï∞®Ïù¥Ï†êÏùÑ ÏÑ§Î™ÖÌïòÍ≥†, Í∞Å Î∞©ÏãùÏùò ÏàòÎ†¥ ÌäπÏÑ±Í≥º Î∂ÑÏÇ∞ Ï∞®Ïù¥Î•º ÏÑúÏà†ÌïòÎùº.


<details>
<summary>3.Ï†ïÎãµ</summary>
Ï§ëÏöîÎèÑ ÏÉòÌîåÎßÅÏùÄ off-policy Monte Carlo ÌïôÏäµÏóêÏÑú ÏÇ¨Ïö©ÎêúÎã§. Ïù¥Îäî Îç∞Ïù¥ÌÑ∞Î•º ÎßåÎì† ÌñâÎèô Ï†ïÏ±ÖÍ≥º Ïö∞Î¶¨Í∞Ä ÌïôÏäµÌïòÎ†§Îäî Î™©Ìëú Ï†ïÏ±ÖÏù¥ Îã§Î•º Îïå, Í∑∏ Ï∞®Ïù¥Î•º Î≥¥Ï†ïÌïòÍ∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©ÎêúÎã§. ÏòàÎ•º Îì§Ïñ¥, ÌñâÎèô Ï†ïÏ±ÖÏù¥ ÎûúÎç§ÏúºÎ°ú ÏòÅÌôîÎ•º Ï∂îÏ≤úÌïòÍ≥†, Î™©Ìëú Ï†ïÏ±ÖÏù¥ ÏÇ¨Ïö©ÏûêÍ∞Ä Ï¢ãÏïÑÌï† Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏùÄ ÏòÅÌôîÎ•º Ï∂îÏ≤úÌïúÎã§Í≥† ÌïòÏûê. Ïù¥ Í≤ΩÏö∞, Í∞Å ÏóêÌîºÏÜåÎìúÏóêÏÑú Í≥ÑÏÇ∞Îêú Í≤∞Í≥ºÏóê ÌñâÎèô ÌôïÎ•†Í≥º Î™©Ìëú ÌôïÎ•†Ïùò ÎπÑÏú®ÏùÑ Í≥±Ìï¥ Í∏∞ÎåÄÍ∞íÏùÑ Îã§Ïãú Í≥ÑÏÇ∞ÌïúÎã§. ÏùºÎ∞òÏ†Å Ï§ëÏöîÎèÑ ÏÉòÌîåÎßÅÏùÄ Î™®Îì† ÎπÑÏú®ÏùÑ Í≥±Ìï¥ ÌèâÍ∑†ÏùÑ ÎÇ¥Î©∞, ÏàòÎ†¥ÏùÄ Î≥¥Ïû•ÎêòÏßÄÎßå Î≥ÄÎèôÏÑ±Ïù¥ ÌÅ¨Îã§. Î∞òÎ©¥, Í∞ÄÏ§ë Ï§ëÏöîÎèÑ ÏÉòÌîåÎßÅÏùÄ Ï†ÑÏ≤¥ ÌôïÎ•†Ïùò Ìï©ÏúºÎ°ú Ï†ïÍ∑úÌôîÌï¥ Î≥ÄÎèôÏÑ±ÏùÄ ÎÇÆÏßÄÎßå ÏïΩÍ∞ÑÏùò Ìé∏Ìñ•Ïù¥ ÏÉùÍ∏∏ Ïàò ÏûàÎã§.

</details>
</details>

<details>
<summary>4. Epsilon-greedy Ï†ïÏ±Ö</summary>

Epsilon-greedy Ï†ïÏ±ÖÏùÑ ÏÇ¨Ïö©Ìïú Î™¨ÌÖåÏπ¥Î•ºÎ°ú Ï†úÏñ¥Î≤ïÏóêÏÑú Ï†ïÏ±Ö Í∞úÏÑ†(policy improvement)Ïù¥ Ïñ¥Îñ§ Î∞©ÏãùÏúºÎ°ú Ïù¥Î£®Ïñ¥ÏßÄÎäîÏßÄ ÏÑ§Î™ÖÌïòÎùº. ÌäπÌûà \(\epsilon\)Ïù¥ ÏûëÍ±∞ÎÇò ÌÅ¥ Îïå Ï†ïÏ±ÖÏùò ÏàòÎ†¥ ÏÜçÎèÑÎÇò ÏïàÏ†ïÏÑ±Ïóê Ïñ¥Îñ§ ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎäîÏßÄÎèÑ ÎÖºÌïòÎùº.

<details>
<summary>Ï†ïÎãµ</summary>
Epsilon-greedy Ï†ïÏ±ÖÏóêÏÑúÎäî ÎåÄÎ∂ÄÎ∂ÑÏùò Í≤ΩÏö∞(1 - Œµ ÌôïÎ•†) ÌòÑÏû¨ÍπåÏßÄ Í∞ÄÏû• Í∞ÄÏπòÍ∞Ä ÎÜíÎã§Í≥† ÌåêÎã®ÎêòÎäî ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÍ≥†, ÎÇòÎ®∏ÏßÄ Œµ ÌôïÎ•†Î°úÎäî ÏûÑÏùòÏùò ÌñâÎèôÏùÑ Í≥†Î•∏Îã§. Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÌÉêÌóòÍ≥º Ïù¥Ïö©Ïùò Í∑†ÌòïÏùÑ ÎßûÏ∂ú Ïàò ÏûàÎã§. Œµ Í∞íÏù¥ ÏûëÏúºÎ©¥ ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Í±∞Ïùò Ìï≠ÏÉÅ ÏµúÏÑ†Ïùò ÌñâÎèôÎßå ÏÑ†ÌÉùÌïòÎØÄÎ°ú Îπ†Î•¥Í≤å ÏàòÎ†¥Ìï† Ïàò ÏûàÏßÄÎßå, ÏµúÏ†ÅÏù¥ ÏïÑÎãê ÏàòÎèÑ ÏûàÎäî ÌñâÎèôÏóê ÎåÄÌïú ÌÉêÌóòÏù¥ Î∂ÄÏ°±Ìï¥Ïßà Ïàò ÏûàÎã§. Î∞òÎåÄÎ°ú Œµ Í∞íÏù¥ ÌÅ¨Î©¥ Îã§ÏñëÌïú ÌñâÎèôÏùÑ ÎßéÏù¥ ÏãúÎèÑÌï¥ ÏïàÏ†ïÏ†ÅÏúºÎ°ú ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Ï∞æÏùÑ Ïàò ÏûàÏßÄÎßå, ÏàòÎ†¥ ÏÜçÎèÑÍ∞Ä ÎäêÎ†§Ïßà Ïàò ÏûàÎã§.
</details>
</details>

<details>
<summary>5. Exploring Starts Í∞ÄÏ†ï</summary>

Monte Carlo ÏòàÏ∏°(Monte Carlo Prediction)ÏóêÏÑú Exploring Starts Í∞ÄÏ†ïÏù¥ ÌïÑÏöîÌïú Ïù¥Ïú†Îäî Î¨¥ÏóáÏù∏Í∞Ä? Ïù¥ Í∞ÄÏ†ïÏùÑ ÌòÑÏã§Ï†ÅÏúºÎ°ú Ï†ÅÏö©ÌïòÍ∏∞ Ïñ¥Î†§Ïö¥ Ïù¥Ïú†ÏôÄ, Ïù¥Î•º ÎåÄÏã†Ìï† Ïàò ÏûàÎäî Î∞©Î≤ï(Ïòà: \(\epsilon\)-greedy)ÏùÑ ÏÑ§Î™ÖÌïòÎùº.

<details>
<summary>Ï†ïÎãµ</summary>
Monte Carlo ÏòàÏ∏°ÏóêÏÑú Exploring Starts Í∞ÄÏ†ïÏù¥ ÌïÑÏöîÌïú Ïù¥Ïú†Îäî Î™®Îì† ÏÉÅÌÉú-ÌñâÎèô ÏåçÏù¥ Ï†ÅÏñ¥ÎèÑ Ìïú Î≤àÏùÄ Î∞©Î¨∏ÎêòÏñ¥Ïïº Ïò¨Î∞îÎ•∏ Í∞ÄÏπò Ï∂îÏ†ïÏù¥ Í∞ÄÎä•ÌïòÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§. ÌïòÏßÄÎßå Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî ÏûÑÏùòÏùò ÏÉÅÌÉúÏôÄ ÌñâÎèôÏóêÏÑú ÏóêÌîºÏÜåÎìúÎ•º ÏãúÏûëÌïòÎäî Í≤ÉÏù¥ Ïñ¥Î†µÎã§. Ïù¥Î•º ÎåÄÏã†Ìï† Ïàò ÏûàÎäî Î∞©Î≤ïÏúºÎ°úÎäî Œµ-greedy Ï†ïÏ±ÖÏ≤òÎüº ÏùºÎ∂ÄÎü¨ Î¨¥ÏûëÏúÑ ÌñâÎèôÏùÑ ÏÑûÏñ¥ Î™®Îì† ÏÉÅÌÉú-ÌñâÎèô ÏåçÏùÑ ÏûêÏó∞Ïä§ÎüΩÍ≤å Î∞©Î¨∏ÌïòÎèÑÎ°ù ÎßåÎìúÎäî Î∞©Î≤ïÏù¥ ÏûàÎã§.
</details>
</details>

<details>
<summary>6. Q-Í∞í ÏóÖÎç∞Ïù¥Ìä∏</summary>

$$
Q[(s, a)] += \alpha (G - Q[(s, a)])
$$

Ïù¥ ÏóÖÎç∞Ïù¥Ìä∏Í∞Ä ÏùòÎØ∏ÌïòÎäî Î∞îÎ•º ÏÑúÏà†ÌïòÎùº. Í∑∏Î¶¨Í≥† \(\alpha\)Í∞Ä Í≥†Ï†ïÍ∞íÏùº ÎïåÏôÄ \(\frac{1}{N(s, a)}\)Ïùº Îïå Í∞ÅÍ∞ÅÏùò Ïû•Îã®Ï†êÏùÑ ÏÑ§Î™ÖÌïòÎùº.

<details>
<summary>Ï†ïÎãµ</summary>
Ïù¥ ÏóÖÎç∞Ïù¥Ìä∏Îäî ÏµúÍ∑ºÏùò return G Í∞íÏùÑ ÌôúÏö©Ìï¥ Í∏∞Ï°¥Ïùò QÍ∞íÏùÑ Í∞±Ïã†ÌïòÎäî ÏßÄÏàòÏù¥ÎèôÌèâÍ∑†(Exponential Moving Average) Î∞©ÏãùÏù¥Îã§. Í≥ºÍ±∞Ïùò Q Ï∂îÏ†ïÏπòÎ•º ÏôÑÏ†ÑÌûà Î≤ÑÎ¶¨ÏßÄ ÏïäÍ≥† ÏùºÎ∂Ä Í∞ÄÏ§ëÏπòÎßå ÎÇ®Í∏¥ Ï±Ñ ÏÉàÎ°úÏö¥ Ï†ïÎ≥¥Î•º Îçî ÎßéÏù¥ Î∞òÏòÅÌïúÎã§. Œ±Í∞Ä Í≥†Ï†ïÎêú Í∞íÏù¥Î©¥, ÏµúÍ∑º Í≤ΩÌóòÏóê Îçî ÌÅ∞ ÎπÑÏ§ëÏùÑ ÎëêÎäî Ìö®Í≥ºÍ∞Ä ÏÉùÍ≤® ÌôòÍ≤ΩÏù¥ Î≥ÄÌôîÌïòÍ±∞ÎÇò Î≥¥ÏÉÅÏù¥ ÎπÑÏ†ïÏÉÅ(non-stationary)Ìïú Í≤ΩÏö∞ÏóêÎèÑ Îπ†Î•¥Í≤å Ï†ÅÏùëÌï† Ïàò ÏûàÎã§. Î∞òÎ©¥ Œ± = 1/N(s, a)Î°ú ÏÑ§Ï†ïÌïòÎ©¥, Í≤ΩÌóòÏù¥ ÏåìÏùºÏàòÎ°ù QÍ∞íÏùò Î≥ÄÌôîÌè≠Ïù¥ Ï†êÏ†ê Ï§ÑÏñ¥Îì§Ïñ¥ Í∞íÏù¥ ÏïàÏ†ïÏ†ÅÏúºÎ°ú ÏàòÎ†¥ÌïúÎã§. ÏòàÎ•º Îì§Ïñ¥, Œ±Î•º 0.1Î°ú Í≥†Ï†ïÌïòÎ©¥ ÏµúÍ∑º 10Î≤àÏùò Í≤ΩÌóòÏóê Îçî ÎØºÍ∞êÌïòÍ≤å Î∞òÏùëÌïòÍ≥†, Œ±Î•º 1/N(s, a)Î°ú ÌïòÎ©¥ Í≤ΩÌóòÏù¥ ÎßéÏïÑÏßàÏàòÎ°ù QÍ∞íÏù¥ Ï≤úÏ≤úÌûà Î≥ÄÌïúÎã§.
</details>
</details>

<details>
<summary>7. Monte Carlo Control Îã®Í≥Ñ</summary>

Monte Carlo ControlÏùÑ ÏÇ¨Ïö©ÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌôòÍ≤ΩÏóêÏÑú ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ ÌïôÏäµÌï¥Í∞ÄÎäî Í≥ºÏ†ïÏùÑ Îã®Í≥ÑÎ≥ÑÎ°ú ÏÑúÏà†ÌïòÎùº. Í∞Å Îã®Í≥ÑÏóêÏÑú ÏàòÌñâÎêòÎäî Ïó∞ÏÇ∞Ïùò Î™©Ï†ÅÍ≥º ÏùòÏùòÎ•º Î™ÖÌôïÌûà ÏÑ§Î™ÖÌïòÎùº.

<details>
<summary>Ï†ïÎãµ</summary>
Monte Carlo Control Î∞©Î≤ïÏùÄ Îß§ ÏóêÌîºÏÜåÎìúÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌòÑÏû¨ Œµ-greedy Ï†ïÏ±ÖÏóê Îî∞Îùº ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÍ≥†, ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇ† ÎïåÍπåÏßÄ ÏÉÅÌÉú-ÌñâÎèô-Î≥¥ÏÉÅ Ï†ïÎ≥¥Î•º ÏàúÏÑúÎåÄÎ°ú Ï†ÄÏû•ÌïúÎã§. ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇú Îí§ÏóêÎäî ÎßàÏßÄÎßâ ÏÉÅÌÉúÎ∂ÄÌÑ∞ Í±∞Íæ∏Î°ú ÎàÑÏ†Å Î≥¥ÏÉÅ(return G)ÏùÑ Í≥ÑÏÇ∞ÌïúÎã§. Ïù¥Î†áÍ≤å ÏñªÏùÄ return Í∞íÏùÑ Ïù¥Ïö©Ìï¥ Q(s, a) Í∞íÏùÑ Ï†êÏßÑÏ†ÅÏúºÎ°ú Í∞±Ïã†ÌïúÎã§. Ïù¥ÌõÑ Í∞Å ÏÉÅÌÉúÏóêÏÑúÏùò Ï†ïÏ±ÖÏùÄ ÌòÑÏû¨ Q Ìï®ÏàòÏóê ÎåÄÌï¥ Í∞ÄÏû• ÎÜíÏùÄ Í∞íÏùÑ Ï£ºÎäî ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÎèÑÎ°ù Í∞úÏÑ†ÌïúÎã§. Ïù¥ Í≥ºÏ†ïÏùÑ Ïó¨Îü¨ Î≤à Î∞òÎ≥µÌïòÎ©¥ Ï†ïÏ±Ö ÌèâÍ∞ÄÏôÄ Ï†ïÏ±Ö Í∞úÏÑ†Ïù¥ Î≤àÍ∞àÏïÑ ÏùºÏñ¥ÎÇòÎ©¥ÏÑú ÏµúÏ†Å Ï†ïÏ±ÖÏóê Ï†êÏ†ê Í∞ÄÍπåÏõåÏßÑÎã§.
</details>
</details>

<details>
<summary>8. Monte Carlo vs. DP/TD</summary>

Monte Carlo Î∞©Î≤ïÏù¥ Dynamic ProgrammingÏù¥ÎÇò Temporal-Difference ÌïôÏäµÍ≥º ÎπÑÍµêÌïòÏó¨ Í∞ñÎäî ÌäπÏßï(Ïû•Ï†êÍ≥º Îã®Ï†ê)ÏùÑ ÏµúÏÜå 3Í∞ÄÏßÄ Ïù¥ÏÉÅ ÎπÑÍµê ÏÑ§Î™ÖÌïòÎùº.


<details>
<summary>Ï†ïÎãµ</summary>
Dynamic Programming(DP)ÏùÄ ÌôòÍ≤ΩÏùò ÏôÑÏ†ÑÌïú Î™®Îç∏(P, R)ÏùÑ ÏïåÍ≥† ÏûàÏùÑ Îïå Bellman Î∞©Ï†ïÏãùÏùÑ Ïù¥Ïö©Ìï¥ ÏÉÅÌÉú Í∞ÄÏπòÎÇò Ï†ïÏ±ÖÏùÑ Î∞òÎ≥µÏ†ÅÏúºÎ°ú Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎã§. Ïù¥ Î∞©ÏãùÏùÄ Îπ†Î•¥Í≥† Ï†ïÌôïÌïòÏßÄÎßå, Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî Î™®Îç∏ÏùÑ ÏïåÍ∏∞ Ïñ¥Î†µÎã§Îäî ÌïúÍ≥ÑÍ∞Ä ÏûàÎã§. Monte Carlo(MC) Î∞©Î≤ïÏùÄ ÌôòÍ≤Ω Î™®Îç∏Ïù¥ ÏóÜÏñ¥ÎèÑ ÎêòÍ≥†, ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇú ÌõÑ Ïã§Ï†ú Í≤ΩÌóòÏùÑ Î∞îÌÉïÏúºÎ°ú ÌïôÏäµÌï† Ïàò ÏûàÎã§. ÌïòÏßÄÎßå ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇ† ÎïåÍπåÏßÄ Í∏∞Îã§Î†§Ïïº ÌïòÍ≥†, Í≤∞Í≥ºÏùò Î≥ÄÎèôÏÑ±Ïù¥ Ïª§ÏÑú ÏàòÎ†¥Ïù¥ ÎäêÎ¶¥ Ïàò ÏûàÎã§. Î∞òÎ©¥ Temporal-Difference(TD) ÌïôÏäµÏùÄ Í≤ΩÌóòÏùÑ Î∞îÌÉïÏúºÎ°ú Ìïú Îã®Í≥ÑÏî© Î∞îÎ°ú ÏóÖÎç∞Ïù¥Ìä∏Ìï† Ïàò ÏûàÏñ¥ Îπ†Î•¥Í≥† Î≥ÄÎèôÏÑ±Ïù¥ Ï†ÅÏúºÎ©∞, Ïò®ÎùºÏù∏ ÌïôÏäµÏóê Ï†ÅÌï©ÌïòÎã§. ÏòàÎ•º Îì§Ïñ¥, Í≤åÏûÑÏùÑ ÌîåÎ†àÏù¥Ìï† Îïå DPÎäî Í≤åÏûÑÏùò Î™®Îì† Í∑úÏπôÍ≥º Í≤∞Í≥ºÎ•º ÏïåÏïÑÏïº ÌïòÍ≥†, MCÎäî Ìïú ÌåêÏù¥ ÎÅùÎÇú Îí§ÏóêÎßå ÌïôÏäµÌïòÏßÄÎßå, TDÎäî Îß§ ÌÑ¥ÎßàÎã§ Î∞îÎ°ú ÌïôÏäµÌï† Ïàò ÏûàÎã§.
</details>
</details>

<details>
<summary>9. Í∑∏Î¶¨ÎìúÏõîÎìú ÌôòÍ≤Ω</summary>

Îã§Ïùå Í∑∏Î¶¨ÎìúÏõîÎìú ÌôòÍ≤ΩÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Î™©Ï†ÅÏßÄÍπåÏßÄ ÎèÑÎã¨ÌïòÎäî Îç∞ Í±∏Î¶¨Îäî ÌèâÍ∑† Î¶¨ÌÑ¥ÏùÑ Monte Carlo Î∞©ÏãùÏúºÎ°ú Ï∂îÏ†ïÌïòÍ≥†Ïûê ÌïúÎã§.
1. ÏÉòÌîå Í≤ΩÎ°úÎì§ÏùÑ Ïñ¥ÎñªÍ≤å ÏÉùÏÑ±Ìï† Í≤ÉÏù¥Î©∞,
2. Í∞Å ÏÉÅÌÉúÏùò Í∞ÄÏπò Ìï®ÏàòÎ•º Ïñ¥ÎñªÍ≤å ÏóÖÎç∞Ïù¥Ìä∏Ìï†ÏßÄ ÏàòÏãù Î∞è ÏïåÍ≥†Î¶¨Ï¶ò ÌùêÎ¶Ñ Ï§ëÏã¨ÏúºÎ°ú ÏÑúÏà†ÌïòÎùº.

<details>
<summary>Ï†ïÎãµ</summary>
Î®ºÏ†Ä ÏóêÏù¥Ï†ÑÌä∏Î•º ÏãúÏûë ÏÉÅÌÉúÏóê ÎëêÍ≥† Œµ-greedy Ï†ïÏ±ÖÏóê Îî∞Îùº ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÎ©¥ÏÑú Í∞Å Ïä§ÌÖùÏóêÏÑú Î∞õÏùÄ Î≥¥ÏÉÅÍ≥º Ï†ÑÏù¥Îêú ÏÉÅÌÉúÎ•º ÏàúÏÑúÎåÄÎ°ú Í∏∞Î°ùÌïúÎã§. ÎèÑÏ∞© ÏßÄÏ†êÏóê Ïù¥Î•¥Í±∞ÎÇò ÏµúÎåÄ Ïä§ÌÖù ÏàòÏóê ÎèÑÎã¨ÌïòÎ©¥ ÏóêÌîºÏÜåÎìúÎ•º Ï¢ÖÎ£åÌïòÍ≥† Ïù¥ Í∏∞Î°ùÏùÑ ÌïòÎÇòÏùò ÏÉòÌîå Í≤ΩÎ°úÎ°ú Ï∑®Í∏âÌïúÎã§.

ÏÉòÌîå Í≤ΩÎ°úÏóê ÎåÄÌï¥ ÎßàÏßÄÎßâ ÏãúÏ†ê \(T\)Î∂ÄÌÑ∞ Ïó≠ÏàúÏúºÎ°ú Î¶¨ÌÑ¥ \(G_t\)Î•º Í≥ÑÏÇ∞ÌïúÎã§. Î®ºÏ†Ä \(G_T = R_{T+1}\)Î°ú Ï†ïÏùòÌïòÍ≥† Ïù¥ÌõÑ ÏãúÏ†ê \(t\)ÏóêÏÑúÎäî \(G_t = R_{t+1} + \gamma G_{t+1}\)Î°ú Í≥ÑÏÇ∞ÌïúÎã§.

Í∞ÄÏπò Ìï®Ïàò \(V(s)\)Îäî First-Visit Monte Carlo Î∞©ÏãùÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏ÌïúÎã§. Í≤ΩÎ°úÏóêÏÑú ÏÉÅÌÉú \(s\)Î•º Ï≤òÏùå Î∞©Î¨∏Ìïú ÏãúÏ†ê \(t\)Ïóê ÎåÄÌï¥ Î∞©Î¨∏ ÌöüÏàò \(N(s)\)Î•º 1 Ï¶ùÍ∞ÄÏãúÌÇ§Í≥†
\[
V(s) \leftarrow V(s) + \frac{G_t - V(s)}{N(s)}
\]
ÏúºÎ°ú Í∞±Ïã†ÌïúÎã§.

Ï†ÑÏ≤¥ ÏïåÍ≥†Î¶¨Ï¶ò ÌùêÎ¶ÑÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§.
1. Î™®Îì† ÏÉÅÌÉú \(s\)Ïóê ÎåÄÌï¥ \(V(s)\)Î•º ÏûÑÏùòÏùò Í∞íÏúºÎ°ú Ï¥àÍ∏∞ÌôîÌïòÍ≥† \(N(s)\)Î•º 0ÏúºÎ°ú ÏÑ§Ï†ïÌïúÎã§.
2. ÏóêÌîºÏÜåÎìúÎ•º ÏÉùÏÑ±Ìï¥ ÏÉÅÌÉú, ÌñâÎèô, Î≥¥ÏÉÅ ÏãúÌÄÄÏä§Î•º Í∏∞Î°ùÌïúÎã§.
3. Ïó≠ÏàúÏúºÎ°ú \(G_t = R_{t+1} + \gamma G_{t+1}\)Î•º Í≥ÑÏÇ∞ÌïúÎã§.
4. Í∞Å ÏÉÅÌÉú \(s\)Ïùò Ï≤´ Î∞©Î¨∏ ÏãúÏ†êÎßàÎã§ \(N(s)\)Î•º Ï¶ùÍ∞ÄÏãúÌÇ§Í≥† \(V(s)\)Î•º Í∞±Ïã†ÌïúÎã§.
5. Ï∂©Î∂ÑÌïú ÏóêÌîºÏÜåÎìúÎ•º Î∞òÎ≥µÌïòÏó¨ \(V(s)\)Í∞Ä ÏàòÎ†¥Ìï† ÎïåÍπåÏßÄ Í≥ºÏ†ïÏùÑ Î∞òÎ≥µÌïúÎã§.

Ïù¥ Í≥ºÏ†ïÏùÑ ÌÜµÌï¥ ÏñªÏùÄ \(V(s)\)Îäî Í∞Å ÏÉÅÌÉúÏóêÏÑú Î™©Ï†ÅÏßÄÍπåÏßÄÏùò ÌèâÍ∑† Î¶¨ÌÑ¥ÏùÑ Í∑ºÏÇ¨ÌïúÎã§.
</details>
</details>

<details>
<summary>10. Q-Ìï®Ïàò ÏàòÎ†¥ Î¨∏Ï†ú</summary>

Monte Carlo ControlÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏµúÏ†Å Q-Ìï®Ïàò \(Q^*(s, a)\)ÏùÑ Í∑ºÏÇ¨Ìï† Îïå, Îã§Ïùå ÏÉÅÌô©ÏóêÏÑú Q-Ìï®ÏàòÍ∞Ä ÏàòÎ†¥ÌïòÏßÄ ÏïäÏùÑ Ïàò ÏûàÎäî Ïù¥Ïú†Î•º ÏÑúÏà†ÌïòÎùº:
- Ï†ïÏ±ÖÏù¥ Ìï≠ÏÉÅ ÌÉêÏöïÏ†Å(greedy)Ïù¥Í≥†
- ÎèôÏùº ÏÉÅÌÉúÏóêÏÑúÏùò ÌñâÎèô ÏÑ†ÌÉùÏù¥ Ìï≠ÏÉÅ ÎèôÏùºÌïòÎ©∞
- ÏóêÌîºÏÜåÎìú ÏàòÍ∞Ä Ï†úÌïúÏ†ÅÏùº Îïå

Í∑∏Î¶¨Í≥† Ïù¥Î•º Î∞©ÏßÄÌïòÍ∏∞ ÏúÑÌïú Îëê Í∞ÄÏßÄ Î∞©Î≤ïÏùÑ Ï†úÏãúÌïòÎùº.

<details>
<summary>Ï†ïÎãµ</summary>
Ï∂îÍ∞ÄÏ†ÅÏù∏ ÌÉêÌóò Í∏∞Î≤ïÏóêÎäî Ïó¨Îü¨ Í∞ÄÏßÄÍ∞Ä ÏûàÎã§. Ï≤´ Î≤àÏß∏Î°ú, Ï§ëÏöîÎèÑÍ∞Ä ÎÜíÏùÄ ÏóêÌîºÏÜåÎìúÎ•º Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú Îã§Ïãú ÏÇ¨Ïö©ÌïòÎäî Î∞©Î≤ïÏù¥ ÏûàÎã§. Ïù¥ Î∞©Î≤ïÏùÄ Î™®Îì† Í≤ΩÌóòÏùÑ ÎòëÍ∞ôÏù¥ ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ ÏïÑÎãàÎùº, ÏóêÏù¥Ï†ÑÌä∏Ïùò ÌïôÏäµÏóê Îçî ÌÅ∞ ÏòÅÌñ•ÏùÑ Ï§Ñ Ïàò ÏûàÎäî Í≤ΩÌóòÏùÑ Îçî ÏûêÏ£º ÏÑ†ÌÉùÌï¥ ÌïôÏäµÏóê Î∞òÏòÅÌïúÎã§. ÏòàÎ•º Îì§Ïñ¥, Î≥¥ÏÉÅÏù¥ ÌÅ¨Í±∞ÎÇò ÏòàÏ∏°Í≥º Ïã§Ï†ú Í≤∞Í≥ºÏùò Ï∞®Ïù¥Í∞Ä ÌÅ∞ ÏóêÌîºÏÜåÎìúÎ•º Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú ÏÉòÌîåÎßÅÌïòÎ©¥, Ï§ëÏöîÌïú Ï†ïÎ≥¥Î•º Îçî Îπ†Î•¥Í≤å Î∞òÏòÅÌï† Ïàò ÏûàÎã§.

Îëê Î≤àÏß∏Î°ú, UCB(Upper Confidence Bound) Î∞©ÏãùÏù¥ ÏûàÎã§. Ïù¥ Î∞©Î≤ïÏùÄ Q(s,a) Í∞íÏóê Ï∂îÍ∞ÄÏ†ÅÏù∏ Î≥¥Ï†ïÌï≠ÏùÑ ÎçîÌï¥Ï§ÄÎã§. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú, Q(s,a)Ïóê c Í≥±ÌïòÍ∏∞ Î£®Ìä∏ Î°úÍ∑∏ N(s)Î•º N(s,a)Î°ú ÎÇòÎàà Í∞íÏùÑ ÎçîÌïúÎã§. Ïó¨Í∏∞ÏÑú N(s)Îäî ÏÉÅÌÉú sÍ∞Ä ÏÑ†ÌÉùÎêú Ï¥ù ÌöüÏàòÏù¥Í≥†, N(s,a)Îäî ÏÉÅÌÉú sÏóêÏÑú ÌñâÎèô aÍ∞Ä ÏÑ†ÌÉùÎêú ÌöüÏàòÏù¥Îã§. Ïù¥ Î≥¥Ï†ïÌï≠ÏùÄ ÏûêÏ£º ÏÑ†ÌÉùÎêòÏßÄ ÏïäÏùÄ ÌñâÎèôÏùºÏàòÎ°ù Í∞íÏù¥ Ïª§ÏßÄÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏñ¥ ÏûàÎã§. Îî∞ÎùºÏÑú QÍ∞íÏù¥ ÎÇÆÎçîÎùºÎèÑ ÏïÑÏßÅ Ï∂©Î∂ÑÌûà ÏãúÎèÑÎêòÏßÄ ÏïäÏùÄ ÌñâÎèôÏùÄ Îçî ÎÜíÏùÄ Ïö∞ÏÑ†ÏàúÏúÑÎ•º Í∞ñÍ≤å ÎêòÏñ¥, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Îã§ÏñëÌïú ÌñâÎèôÏùÑ ÏãúÎèÑÌï† Ïàò ÏûàÎèÑÎ°ù Ïú†ÎèÑÌïúÎã§. Ïù¥ Î∞©ÏãùÏùÄ Îã®ÏàúÌûà Î¨¥ÏûëÏúÑÎ°ú ÌñâÎèôÏùÑ Í≥†Î•¥Îäî Í≤ÉÎ≥¥Îã§, ÏïÑÏßÅ Ï†ïÎ≥¥Í∞Ä Î∂ÄÏ°±Ìïú ÌñâÎèôÏùÑ Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú ÌÉêÌóòÌï† Ïàò ÏûàÍ≤å Ìï¥Ï§ÄÎã§.
</details>
</details>

<h2 id="td-method">TD method</h2>

<p>TDÎ≤ïÏùÑ ÏãúÏûëÌïòÍ∏∞ Ï†ÑÏóê ÏïûÏóê ÎÇ¥Ïö©ÏùÑ Î¨∏Ï†úÎ•º ÌíÄÎ©∞ Î≥µÏäµÏùÑ Ìï¥Î≥¥Ïûê.</p>

<details>
<summary>1. MDPÏùò Íµ¨ÏÑ± ÏöîÏÜåÎ•º Ï†ïÏùòÌïòÍ≥†, Í∞Å ÏöîÏÜåÍ∞Ä ÏóêÏù¥Ï†ÑÌä∏ ÌïôÏäµÏóêÏÑú Ïñ¥Îñ§ Ïó≠Ìï†ÏùÑ ÌïòÎäîÏßÄ ÎÖºÏùòÌïòÏãúÏò§.</summary>

MDPÎ•º Ïù¥Î£®Îäî ÏÉÅÌÉú ÏßëÌï© \(\mathcal{S}\), ÌñâÎèô ÏßëÌï© \(\mathcal{A}\), ÏÉÅÌÉú Ï†ÑÏù¥ ÌôïÎ•† \(P(s'\mid s,a)\), Î≥¥ÏÉÅ Ìï®Ïàò \(r(s,a,s')\), Ìï†Ïù∏Ïú® \(\gamma\)Î•º ÏàòÏãùÍ≥º Ìï®Íªò Ï†ïÏùòÌïòÍ≥†, Í∞Å ÏöîÏÜåÍ∞Ä ÏóêÏù¥Ï†ÑÌä∏ ÌïôÏäµÏóêÏÑú Ïñ¥Îñ§ Ïó≠Ìï†ÏùÑ ÌïòÎäîÏßÄ ÎÖºÏùòÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
ÎßàÎ•¥ÏΩîÌîÑ Í≤∞Ï†ï Í≥ºÏ†ï(MDP)ÏùÄ ÏàúÏ∞®Ï†Å ÏùòÏÇ¨Í≤∞Ï†ï Î¨∏Ï†úÎ•º ÏàòÌïôÏ†ÅÏúºÎ°ú Î™®Îç∏ÎßÅÌïòÎäî ÌîÑÎ†àÏûÑÏõåÌÅ¨Î°ú, Îã§ÏÑØ Í∞ÄÏßÄ ÌïµÏã¨ ÏöîÏÜåÎ°ú Íµ¨ÏÑ±ÎêúÎã§.<br />

ÏÉÅÌÉú ÏßëÌï© \(\mathcal{S}\)Îäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Í¥ÄÏ∞∞Ìï† Ïàò ÏûàÎäî Î™®Îì† ÌôòÍ≤Ω ÏÉÅÌÉúÎì§Ïùò ÏßëÌï©Ïù¥Îã§. Ïù¥Îäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌòÑÏû¨ ÌôòÍ≤ΩÏóêÏÑú Ïñ¥Îñ§ ÏÉÅÌô©Ïóê Ï≤òÌï¥ ÏûàÎäîÏßÄÎ•º ÌëúÌòÑÌïòÎ©∞, ÌïôÏäµ Í≥ºÏ†ïÏóêÏÑú ÏÉÅÌÉú Ïù∏ÏãùÍ≥º ÌëúÌòÑÏùò Í∏∞Ï¥àÍ∞Ä ÎêúÎã§. ÏÉÅÌÉú Ï†ïÎ≥¥Ïùò ÌíàÏßàÏùÄ ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏµúÏ†Å ÌñâÎèôÏùÑ Í≤∞Ï†ïÌïòÎäî Îç∞ ÏßÅÏ†ëÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏ÏπúÎã§.<br />

ÌñâÎèô ÏßëÌï© \(\mathcal{A}\)Îäî Í∞Å ÏÉÅÌÉúÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏÑ†ÌÉùÌï† Ïàò ÏûàÎäî Î™®Îì† ÌñâÎèôÎì§Ïùò ÏßëÌï©Ïù¥Îã§. Ïù¥Î•º ÌÜµÌï¥ ÏóêÏù¥Ï†ÑÌä∏Îäî ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÎ©∞, Îã§ÏñëÌïú ÌñâÎèôÏùÑ ÌÉêÏÉâÌï®ÏúºÎ°úÏç® ÏµúÏ†ÅÏùò ÌñâÎèô Î∞©Ïπ®ÏùÑ ÌïôÏäµÌïúÎã§. ÌñâÎèô ÏÑ†ÌÉùÏùÄ Ï†ïÏ±Ö Í∞úÏÑ† Í≥ºÏ†ïÏùò ÌïµÏã¨Ïù¥Î©∞, ÌñâÎèô Í≥µÍ∞ÑÏùò Íµ¨Ï°∞Îäî ÌïôÏäµ Î≥µÏû°ÎèÑÏóê ÏßÅÏ†ëÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ Ï§ÄÎã§.<br />

ÏÉÅÌÉú Ï†ÑÏù¥ ÌôïÎ•† \(P(s'\mid s,a) = \Pr\{S_{t+1}=s' \mid S_t=s, A_t=a\}\)ÏùÄ ÌòÑÏû¨ ÏÉÅÌÉú \(s\)ÏóêÏÑú ÌñâÎèô \(a\)Î•º Ï∑®ÌñàÏùÑ Îïå Îã§Ïùå ÏÉÅÌÉú \(s'\)Î°ú Ï†ÑÏù¥Îê† ÌôïÎ•†ÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. Ïù¥Îäî ÌôòÍ≤ΩÏùò ÎèôÏ†Å ÌäπÏÑ±ÏùÑ ÌôïÎ•†Ï†ÅÏúºÎ°ú Î™®Îç∏ÎßÅÌïòÎ©∞, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌñâÎèôÏùò Í≤∞Í≥ºÎ•º ÏòàÏ∏°ÌïòÍ≥† Í≥ÑÌöçÏùÑ ÏÑ∏Ïö∏ Ïàò ÏûàÍ≤å ÌïúÎã§. ÏÉÅÌÉú Ï†ÑÏù¥ ÌôïÎ•†ÏùÄ Î™®Îç∏ Í∏∞Î∞ò ÌïôÏäµÏóêÏÑú ÌôòÍ≤Ω ÏãúÎÆ¨Î†àÏù¥ÏÖòÏùò Í∏∞Î∞òÏù¥ ÎêúÎã§.<br />

Î≥¥ÏÉÅ Ìï®Ïàò \(r(s,a,s') = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a, S_{t+1}=s']\)Îäî ÏÉÅÌÉú \(s\)ÏóêÏÑú ÌñâÎèô \(a\)Î•º Ï∑®Ìï¥ ÏÉÅÌÉú \(s'\)Î°ú Ï†ÑÏù¥Îê† Îïå Î∞õÏùÑ Í≤ÉÏúºÎ°ú Í∏∞ÎåÄÎêòÎäî Ï¶âÍ∞ÅÏ†Å Î≥¥ÏÉÅÏùÑ Ï†ïÏùòÌïúÎã§. Î≥¥ÏÉÅ Ìï®ÏàòÎäî ÏóêÏù¥Ï†ÑÌä∏Ïùò Î™©ÌëúÎ•º Î™ÖÏãúÏ†ÅÏúºÎ°ú Ï†ïÏùòÌïòÍ≥†, ÌïôÏäµ Í≥ºÏ†ïÏóêÏÑú ÌñâÎèôÏùò Í∞ÄÏπòÎ•º ÌèâÍ∞ÄÌïòÎäî ÌïµÏã¨ Ïã†Ìò∏Í∞Ä ÎêúÎã§. Ï†ÅÏ†àÌïú Î≥¥ÏÉÅ ÏÑ§Í≥ÑÎäî Í∞ïÌôîÌïôÏäµÏùò ÏÑ±Í≥µÏóê Í≤∞Ï†ïÏ†ÅÏù∏ Ïó≠Ìï†ÏùÑ ÌïúÎã§.<br />

Ìï†Ïù∏Ïú® \(\gamma\in[0,1)\)ÏùÄ ÎØ∏Îûò Î≥¥ÏÉÅÏùò ÌòÑÏû¨ Í∞ÄÏπòÎ•º Í≤∞Ï†ïÌïòÎäî ÌååÎùºÎØ∏ÌÑ∞Ïù¥Îã§. Ïù¥Îäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Îã®Í∏∞Ï†Å Î≥¥ÏÉÅÍ≥º Ïû•Í∏∞Ï†Å Î≥¥ÏÉÅ ÏÇ¨Ïù¥Ïùò Í∑†ÌòïÏùÑ ÎßûÏ∂îÎäî Îç∞ Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ ÌïúÎã§. \(\gamma\)Í∞Ä 0Ïóê Í∞ÄÍπåÏö∏ÏàòÎ°ù Ï¶âÍ∞ÅÏ†ÅÏù∏ Î≥¥ÏÉÅÏùÑ Ï§ëÏãúÌïòÍ≥†, 1Ïóê Í∞ÄÍπåÏö∏ÏàòÎ°ù ÎØ∏ÎûòÏùò Î≥¥ÏÉÅÏùÑ ÌòÑÏû¨ÏôÄ Í±∞Ïùò ÎèôÎì±ÌïòÍ≤å Í∞ÄÏπò ÏûàÍ≤å ÌèâÍ∞ÄÌïúÎã§.<br />

Ïù¥ Îã§ÏÑØ ÏöîÏÜåÍ∞Ä Ïú†Í∏∞Ï†ÅÏúºÎ°ú Í≤∞Ìï©ÎêòÏñ¥ ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ïû•Í∏∞Ï†Å Î≥¥ÏÉÅÏùÑ ÏµúÎåÄÌôîÌïòÎäî ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ ÌïôÏäµÌï† Ïàò ÏûàÎäî ÏàòÌïôÏ†Å Í∏∞Î∞òÏùÑ Ï†úÍ≥µÌïúÎã§. MDP ÌîÑÎ†àÏûÑÏõåÌÅ¨Îäî Í∞ïÌôîÌïôÏäµÏùò Ïù¥Î°†Ï†Å ÌÜ†ÎåÄÎ•º ÌòïÏÑ±ÌïòÎ©∞, Ïã§Ï†ú Î¨∏Ï†úÎ•º Ïù¥ ÌîÑÎ†àÏûÑÏõåÌÅ¨Î°ú Ï†ïÌôïÌûà Î™®Îç∏ÎßÅÌïòÎäî Í≤ÉÏù¥ Í∞ïÌôîÌïôÏäµ ÏùëÏö©Ïùò Ï≤´ Î≤àÏß∏ Îã®Í≥ÑÏù¥Îã§.
</details>
</details>

<details>
<summary>2. ÎßàÎ•¥ÏΩîÌîÑ ÏÑ±ÏßàÏù¥ Î¨¥ÏóáÏù∏ÏßÄ Ï†ïÏùòÌïòÍ≥†, "ÎØ∏Îûò ÏÉÅÌÉúÍ∞Ä Ïò§ÏßÅ ÌòÑÏû¨ ÏÉÅÌÉúÏóêÎßå ÏùòÏ°¥ÌïúÎã§"Îäî Ï†êÏùÑ ÏòàÏãúÎ•º Îì§Ïñ¥ ÏÑ§Î™ÖÌïòÏãúÏò§.</summary>

ÎßàÎ•¥ÏΩîÌîÑ ÏÑ±ÏßàÏù¥ Î¨¥ÏóáÏù∏ÏßÄ Ï†ïÏùòÌïòÍ≥†, "ÎØ∏Îûò ÏÉÅÌÉúÍ∞Ä Ïò§ÏßÅ ÌòÑÏû¨ ÏÉÅÌÉúÏóêÎßå ÏùòÏ°¥ÌïúÎã§"Îäî Ï†êÏùÑ Í∞ÑÎã®Ìïú ÏòàÏãú(Ïòà: 2√ó2 Í∑∏Î¶¨ÎìúÏõîÎìú)Î°ú ÏÑ§Î™ÖÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
ÎßàÎ•¥ÏΩîÌîÑ ÏÑ±ÏßàÏùÄ ÏãúÏä§ÌÖúÏùò ÎØ∏Îûò ÏÉÅÌÉúÍ∞Ä Í≥ºÍ±∞Ïùò Î™®Îì† ÏÉÅÌÉúÏôÄ ÌñâÎèôÏùò Ïù¥Î†•Ïù¥ ÏïÑÎãàÎùº Ïò§ÏßÅ ÌòÑÏû¨ ÏÉÅÌÉúÏóêÎßå ÏùòÏ°¥ÌïúÎã§Îäî ÏÜçÏÑ±Ïù¥Îã§. ÏàòÌïôÏ†ÅÏúºÎ°úÎäî ÏÉÅÌÉú Ï†ÑÏù¥Ïùò Ï°∞Í±¥Î∂Ä ÌôïÎ•† Î∂ÑÌè¨Í∞Ä ÌòÑÏû¨ ÏÉÅÌÉúÏôÄ ÌñâÎèôÎßåÏúºÎ°ú Í≤∞Ï†ïÎêúÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏ÌïúÎã§. Ï¶â, \(\Pr(S_{t+1}\mid S_{0:t},A_{0:t})=\Pr(S_{t+1}\mid S_t,A_t)\)Î°ú ÌëúÌòÑÎêúÎã§.<br />

Ïù¥Îü¨Ìïú ÎßàÎ•¥ÏΩîÌîÑ ÏÑ±ÏßàÏùÄ Î≥µÏû°Ìïú ÏùòÏÇ¨Í≤∞Ï†ï Í≥ºÏ†ïÏùÑ Îã®ÏàúÌôîÌïòÎäî Îß§Ïö∞ Í∞ïÎ†•Ìïú Í∞ÄÏ†ïÏù¥Îã§. Ïù¥Îäî ÏãúÏä§ÌÖúÏù¥ 'Í∏∞ÏñµÏù¥ ÏóÜÎäî' ÏÜçÏÑ±ÏùÑ Í∞ÄÏßÑÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏ÌïòÎ©∞, ÌòÑÏû¨ ÏÉÅÌÉúÍ∞Ä ÎØ∏Îûò ÏòàÏ∏°Ïóê ÌïÑÏöîÌïú Î™®Îì† Ï†ïÎ≥¥Î•º Ìè¨Ìï®ÌïòÍ≥† ÏûàÎã§Í≥† Í∞ÄÏ†ïÌïúÎã§.<br />

Íµ¨Ï≤¥Ï†ÅÏù∏ ÏòàÎ°ú, 2√ó2 Í∑∏Î¶¨ÎìúÏõîÎìú ÌôòÍ≤ΩÏùÑ ÏÉùÍ∞ÅÌï¥ Î≥¥Ïûê. Ïù¥ ÌôòÍ≤ΩÏóêÎäî A, B, C, D ÎÑ§ Í∞úÏùò Í≤©Ïûê Ïπ∏Ïù¥ ÏûàÍ≥†, ÏóêÏù¥Ï†ÑÌä∏Îäî ÏÉÅ, Ìïò, Ï¢å, Ïö∞Î°ú Ïù¥ÎèôÌï† Ïàò ÏûàÎã§. ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌòÑÏû¨ C Ïπ∏Ïóê ÏûàÎã§Í≥† Í∞ÄÏ†ïÌï¥ Î≥¥Ïûê. ÎßàÎ•¥ÏΩîÌîÑ ÏÑ±ÏßàÏóê Îî∞Î•¥Î©¥, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Îã§ÏùåÏóê Ïñ¥Îäê Ïπ∏ÏúºÎ°ú Ïù¥ÎèôÌï†ÏßÄÎäî Ïò§ÏßÅ ÌòÑÏû¨ ÏúÑÏπòÏù∏ C Ïπ∏Í≥º ÏÑ†ÌÉùÌïú ÌñâÎèô(Ïòà: 'ÏúÑÎ°ú Ïù¥Îèô')ÏóêÎßå ÏùòÏ°¥ÌïúÎã§.<br />

Ïù¥Îäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ïñ¥Îñ§ Í≤ΩÎ°úÎ•º ÌÜµÌï¥ C Ïπ∏Ïóê ÎèÑÎã¨ÌñàÎäîÏßÄÎäî Ï†ÑÌòÄ Ï§ëÏöîÌïòÏßÄ ÏïäÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏ÌïúÎã§. ÏóêÏù¥Ï†ÑÌä∏Í∞Ä A ‚Üí B ‚Üí C Í≤ΩÎ°úÎ°ú ÏôîÎì†, D ‚Üí C Í≤ΩÎ°úÎ°ú ÏôîÎì†, ÎòêÎäî CÏóêÏÑú Ïó¨Îü¨ Î≤à Ï†úÏûêÎ¶¨Ïóê Î®∏Î¨ºÎ†ÄÎì† ÏÉÅÍ¥ÄÏóÜÏù¥, ÌòÑÏû¨ CÏóê ÏûàÎã§Îäî ÏÇ¨Ïã§Í≥º ÏÑ†ÌÉùÌïú ÌñâÎèôÎßåÏù¥ Îã§Ïùå ÏÉÅÌÉúÎ•º Í≤∞Ï†ïÌïúÎã§.<br />

Ïù¥Îü¨Ìïú ÎßàÎ•¥ÏΩîÌîÑ ÏÑ±ÏßàÏùÄ Í∞ïÌôîÌïôÏäµ ÏïåÍ≥†Î¶¨Ï¶òÏóêÏÑú Í≥ÑÏÇ∞ Ìö®Ïú®ÏÑ±ÏùÑ ÌÅ¨Í≤å ÎÜíÏù∏Îã§. ÏãúÏä§ÌÖúÏùò Î™®Îì† Ïù¥Ï†Ñ Ïù¥Î†•ÏùÑ Ï†ÄÏû•ÌïòÍ≥† Ï≤òÎ¶¨ÌïòÎäî ÎåÄÏã†, ÌòÑÏû¨ ÏÉÅÌÉúÎßå Í≥†Î†§ÌïòÎ©¥ ÎêòÍ∏∞ ÎïåÎ¨∏Ïóê Î©îÎ™®Î¶¨ ÏöîÍµ¨ÏÇ¨Ìï≠Í≥º Í≥ÑÏÇ∞ Î≥µÏû°ÎèÑÍ∞Ä ÌÅ¨Í≤å Í∞êÏÜåÌïúÎã§. ÎòêÌïú Í∞ÄÏπò Ìï®ÏàòÏôÄ Ï†ïÏ±ÖÏùÑ ÌòÑÏû¨ ÏÉÅÌÉúÏóêÎßå ÏùòÏ°¥ÌïòÎäî Ìï®ÏàòÎ°ú Ï†ïÏùòÌï† Ïàò ÏûàÍ≤å Ìï¥Ï£ºÏñ¥, Í∞ïÌôîÌïôÏäµÏùò Ïù¥Î°†Ï†Å Í∏∞Î∞òÏù¥ ÎêòÎäî Î≤®Îßå Î∞©Ï†ïÏãùÏùÑ Ïú†ÎèÑÌï† Ïàò ÏûàÍ≤å ÌïúÎã§.
</details>
</details>

<details>
<summary>3. Í≤∞Ï†ïÎ°†Ï†Å Ï†ÑÏù¥ÏôÄ ÌôïÎ•†Î°†Ï†Å Ï†ÑÏù¥Ïùò Ï∞®Ïù¥Î•º ÏàòÏãùÏúºÎ°ú ÎπÑÍµêÌïòÍ≥†, Í∞Å Î∞©ÏãùÏù¥ Î™®Îç∏ÎßÅÏóê ÎØ∏ÏπòÎäî Ïû•Îã®Ï†êÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

Í≤∞Ï†ïÎ°†Ï†Å Ï†ÑÏù¥ Ìï®Ïàò \(s'=f(s,a)\)ÏôÄ ÌôïÎ•†Î°†Ï†Å Ï†ÑÏù¥ ÌôïÎ•† \(P(s'\mid s,a)\)Ïùò Ï∞®Ïù¥Î•º ÏàòÏãùÏúºÎ°ú ÎπÑÍµêÌïòÍ≥†, Í∞Å Î∞©ÏãùÏù¥ Î™®Îç∏ÎßÅÏóê ÎØ∏ÏπòÎäî Ïû•Îã®Ï†êÏùÑ ÎÖºÏùòÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Í≤∞Ï†ïÎ°†Ï†Å Ï†ÑÏù¥ÏôÄ ÌôïÎ•†Î°†Ï†Å Ï†ÑÏù¥Îäî ÌôòÍ≤Ω Î™®Îç∏ÎßÅÏùò Îëê Í∞ÄÏßÄ Í∑ºÎ≥∏Ï†ÅÏúºÎ°ú Îã§Î•∏ Ï†ëÍ∑º Î∞©ÏãùÏù¥Îã§. Í≤∞Ï†ïÎ°†Ï†Å Ï†ÑÏù¥Îäî Ìï®Ïàò \(f:\mathcal{S}\times\mathcal{A}\to\mathcal{S}\)Î°ú ÌëúÌòÑÎêòÎ©∞, Ïù¥Îäî ÌòÑÏû¨ ÏÉÅÌÉú \(s\)ÏôÄ ÌñâÎèô \(a\)Í∞Ä Ï£ºÏñ¥Ï°åÏùÑ Îïå Îã§Ïùå ÏÉÅÌÉúÍ∞Ä Ï†ïÌôïÌûà \(s'=f(s,a)\)Î°ú Í≤∞Ï†ïÎêúÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏ÌïúÎã§. Î∞òÎ©¥, ÌôïÎ•†Î°†Ï†Å Ï†ÑÏù¥Îäî \(P(s'\mid s,a)\)Î°ú ÌëúÌòÑÎêòÎäî ÌôïÎ•† Î∂ÑÌè¨Î°ú, Í∞Å Í∞ÄÎä•Ìïú Îã§Ïùå ÏÉÅÌÉú \(s'\)Î°ú Ï†ÑÏù¥Îê† ÌôïÎ•†ÏùÑ Ï†úÍ≥µÌïòÎ©∞, Î™®Îì† ÏÉÅÌÉúÏóê ÎåÄÌï¥ Ìï©ÏÇ∞ÌïòÎ©¥ 1Ïù¥ ÎêúÎã§ (\(\sum_{s'}P(s'\mid s,a)=1\)).<br />

Í≤∞Ï†ïÎ°†Ï†Å Ï†ÑÏù¥ Î™®Îç∏ÏùÄ Í≥ÑÏÇ∞ Ìö®Ïú®ÏÑ±Ïù¥ Ï£ºÏöî Ïû•Ï†êÏù¥Îã§. Îã§Ïùå ÏÉÅÌÉúÍ∞Ä Î™ÖÌôïÌïòÍ≤å Í≤∞Ï†ïÎêòÎØÄÎ°ú, Í≥ÑÌöç ÏïåÍ≥†Î¶¨Ï¶òÏù¥ÎÇò ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏóêÏÑú Í∞Å ÏÉÅÌÉú-ÌñâÎèô ÏåçÏóê ÎåÄÌï¥ ÌïòÎÇòÏùò Îã§Ïùå ÏÉÅÌÉúÎßå Í≥†Î†§ÌïòÎ©¥ ÎêúÎã§. Ïù¥Îäî Í≥ÑÏÇ∞ÎüâÏùÑ ÌÅ¨Í≤å Ï§ÑÏó¨Ï£ºÍ≥†, Íµ¨ÌòÑÏùÑ Îã®ÏàúÌôîÌïúÎã§. Í∑∏Îü¨ÎÇò Í≤∞Ï†ïÎ°†Ï†Å Î™®Îç∏ÏùÄ ÌôòÍ≤ΩÏùò Î∂àÌôïÏã§ÏÑ±Ïù¥ÎÇò ÎÖ∏Ïù¥Ï¶àÎ•º ÌëúÌòÑÌï† Ïàò ÏóÜÎã§Îäî Ï§ëÎåÄÌïú ÌïúÍ≥ÑÍ∞Ä ÏûàÎã§. ÌòÑÏã§ ÏÑ∏Í≥ÑÏùò ÎßéÏùÄ ÏùëÏö© Î∂ÑÏïºÏóêÏÑúÎäî ÎèôÏùºÌïú ÏÉÅÌÉúÏôÄ ÌñâÎèôÏóêÏÑúÎèÑ Îã§ÏñëÌïú Í≤∞Í≥ºÍ∞Ä Î∞úÏÉùÌï† Ïàò ÏûàÏúºÎØÄÎ°ú, Ïù¥Îü¨Ìïú Î™®Îç∏ÏùÄ ÌòÑÏã§ÏùÑ ÏßÄÎÇòÏπòÍ≤å Îã®ÏàúÌôîÌï† Ïàò ÏûàÎã§.<br />

Î∞òÎ©¥, ÌôïÎ•†Î°†Ï†Å Ï†ÑÏù¥ Î™®Îç∏ÏùÄ ÌôòÍ≤ΩÏùò ÎÇ¥Ïû¨Ï†Å Î∂àÌôïÏã§ÏÑ±ÏùÑ Î™ÖÏãúÏ†ÅÏúºÎ°ú Î™®Îç∏ÎßÅÌï† Ïàò ÏûàÎã§. Ïù¥Îäî Î°úÎ¥á Ï†úÏñ¥, ÏûêÏú® Ï£ºÌñâ, Í∏àÏúµ ÏùòÏÇ¨Í≤∞Ï†ïÍ≥º Í∞ôÏù¥ Î∂àÌôïÏã§ÏÑ±Ïù¥ Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ ÌïòÎäî Î≥µÏû°Ìïú Ïã§Ï†ú Î¨∏Ï†úÏóê Îçî Ï†ÅÌï©ÌïòÎã§. ÌôïÎ•†Î°†Ï†Å Î™®Îç∏ÏùÄ ÎòêÌïú ÌÉêÏÉâÍ≥º ÌôúÏö© ÏÇ¨Ïù¥Ïùò Í∑†ÌòïÏùÑ ÏûêÏó∞Ïä§ÎüΩÍ≤å Í∞ÄÎä•ÌïòÍ≤å ÌïòÏó¨, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Î∂àÌôïÏã§Ìïú ÌñâÎèôÏùò Í≤∞Í≥ºÎ•º ÌÉêÏÉâÌïòÎèÑÎ°ù Ïû•Î†§ÌïúÎã§. Í∑∏Îü¨ÎÇò Í≥ÑÏÇ∞ ÎπÑÏö©Ïù¥ ÌÅ¨Í≤å Ï¶ùÍ∞ÄÌïúÎã§Îäî Îã®Ï†êÏù¥ ÏûàÎã§. ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏóêÏÑú ÌôïÎ•†Î°†Ï†Å Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï† ÎïåÎäî Í∞Å ÏÉÅÌÉú-ÌñâÎèô ÏåçÏóê ÎåÄÌï¥ Î™®Îì† Í∞ÄÎä•Ìïú Îã§Ïùå ÏÉÅÌÉúÎ•º Í≥†Î†§Ìï¥Ïïº ÌïòÎ©∞, Ïù¥Î°ú Ïù∏Ìï¥ Í≥ÑÏÇ∞ Î≥µÏû°ÎèÑÍ∞Ä \(O(|S|^2|A|)\)ÍπåÏßÄ Ï¶ùÍ∞ÄÌï† Ïàò ÏûàÎã§.<br />

Ïã§Ï†ú ÏùëÏö©ÏóêÏÑúÎäî ÌôòÍ≤ΩÏùò Î≥µÏû°ÏÑ±Í≥º ÏöîÍµ¨ÎêòÎäî Ï†ïÌôïÎèÑÏóê Îî∞Îùº Ï†ÅÏ†àÌïú Î™®Îç∏ÏùÑ ÏÑ†ÌÉùÌï¥Ïïº ÌïúÎã§. Îã®ÏàúÌïú Í∑∏Î¶¨Îìú ÏõîÎìúÎÇò Í≤∞Ï†ïÎ°†Ï†Å Í≤åÏûÑÍ≥º Í∞ôÏùÄ ÌôòÍ≤ΩÏóêÏÑúÎäî Í≤∞Ï†ïÎ°†Ï†Å Î™®Îç∏Ïù¥ Ìö®Ïú®Ï†ÅÏù¥Í≥† Ï∂©Î∂ÑÌï† Ïàò ÏûàÎã§. Î∞òÎ©¥, ÎÇ†Ïî® ÏòÅÌñ•ÏùÑ Î∞õÎäî ÏãúÏä§ÌÖú, Í∏àÏúµ ÏãúÏû•, ÎòêÎäî Îã§Ï§ë ÏóêÏù¥Ï†ÑÌä∏ ÌôòÍ≤ΩÍ≥º Í∞ôÏù¥ Î≥∏ÏßàÏ†ÅÏúºÎ°ú Î∂àÌôïÏã§Ìïú ÌôòÍ≤ΩÏóêÏÑúÎäî ÌôïÎ•†Î°†Ï†Å Î™®Îç∏Ïù¥ ÌïÑÏàòÏ†ÅÏù¥Îã§.
</details>
</details>

<details>
<summary>4. Î≥¥ÏÉÅ Ìï®ÏàòÏùò Ï†ïÏùòÏôÄ Ï¶âÏãú Î≥¥ÏÉÅÍ≥º Ïû•Í∏∞ Î≥¥ÏÉÅÏùò Ï∞®Ïù¥Î•º ÎÖºÏùòÌïòÏãúÏò§.</summary>

Î≥¥ÏÉÅ Ìï®Ïàò \(r(s,a,s')\)Ïùò Ï†ïÏùòÏôÄ, "Ï¶âÏãú Î≥¥ÏÉÅ(immediate reward)"Í≥º "Ïû•Í∏∞ Î≥¥ÏÉÅ(long-term return)"Ïùò Ï∞®Ïù¥Î•º ÏÑ§Î™ÖÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Î≥¥ÏÉÅ Ìï®Ïàò \(r(s,a,s')\)Îäî Í∞ïÌôîÌïôÏäµÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏Ïùò Î™©ÌëúÎ•º ÏàòÌïôÏ†ÅÏúºÎ°ú Ï†ïÏùòÌïòÎäî ÌïµÏã¨ ÏöîÏÜåÏù¥Îã§. Ïù¥ Ìï®ÏàòÎäî ÏÉÅÌÉú \(s\)ÏóêÏÑú ÌñâÎèô \(a\)Î•º Ï∑®ÌïòÏó¨ ÏÉàÎ°úÏö¥ ÏÉÅÌÉú \(s'\)Î°ú Ï†ÑÏù¥Îê† Îïå ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Î∞õÎäî Ï¶âÍ∞ÅÏ†ÅÏù∏ ÌîºÎìúÎ∞±ÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. ÏàòÌïôÏ†ÅÏúºÎ°úÎäî \(r(s,a,s') = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a, S_{t+1}=s']\)Î°ú Ï†ïÏùòÎêòÎ©∞, Ïù¥Îäî ÌäπÏ†ï ÏÉÅÌÉú Ï†ÑÏù¥Ïóê ÎåÄÌïú Î≥¥ÏÉÅÏùò Í∏∞ÎåÄÍ∞íÏùÑ ÏùòÎØ∏ÌïúÎã§.<br />

Ï¶âÏãú Î≥¥ÏÉÅ(immediate reward)ÏùÄ ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ìïú Îã®Í≥ÑÏùò ÌñâÎèô ÌõÑ Ï¶âÍ∞ÅÏ†ÅÏúºÎ°ú Î∞õÎäî Î≥¥ÏÉÅÏùÑ ÏùòÎØ∏ÌïúÎã§. Ïù¥Îäî \(R_{t+1}\)Î°ú ÌëúÍ∏∞ÎêòÎ©∞, ÏãúÍ∞Ñ \(t\)ÏóêÏÑúÏùò ÏÉÅÌÉúÏôÄ ÌñâÎèô Ïù¥ÌõÑ ÏßÅÏ†ëÏ†ÅÏúºÎ°ú Í¥ÄÏ∞∞ÎêòÎäî Í∞ÄÏπòÏù¥Îã§. Ï¶âÏãú Î≥¥ÏÉÅÏùÄ ÌñâÎèôÏùò ÏßÅÏ†ëÏ†ÅÏù∏ Í≤∞Í≥ºÎ•º ÌèâÍ∞ÄÌïòÎäî Î∞©Î≤ïÏùÑ Ï†úÍ≥µÌïòÏßÄÎßå, Ïû•Í∏∞Ï†ÅÏù∏ Í≤∞Í≥ºÎ•º Í≥†Î†§ÌïòÏßÄ ÏïäÎäîÎã§. ÏòàÎ•º Îì§Ïñ¥, Ï≤¥Ïä§ Í≤åÏûÑÏóêÏÑú Îßê ÌïòÎÇòÎ•º Ïû°Îäî Í≤ÉÏùÄ Ï¶âÏãú Î≥¥ÏÉÅÏùÑ Í∞ÄÏ†∏Ïò§ÏßÄÎßå, Í∑∏ Í≤∞Í≥ºÎ°ú ÏûêÏã†Ïùò Ï§ëÏöîÌïú ÎßêÏù¥ ÏúÑÌóòÏóê Ï≤òÌï† Ïàò ÏûàÎã§.<br />

Î∞òÎ©¥, Ïû•Í∏∞ Î≥¥ÏÉÅ(long-term return)ÏùÄ ÏãúÍ∞Ñ \(t\)Î∂ÄÌÑ∞ ÏóêÌîºÏÜåÎìú ÎÅùÍπåÏßÄ Î∞õÍ≤å Îê† Î™®Îì† ÎØ∏Îûò Î≥¥ÏÉÅÏùò Ìï†Ïù∏Îêú Ìï©ÏúºÎ°ú Ï†ïÏùòÎêúÎã§. ÏàòÌïôÏ†ÅÏúºÎ°úÎäî \(G_t=\sum_{k=0}^\infty\gamma^kR_{t+k+1}\)Î°ú ÌëúÌòÑÎêúÎã§. Ïó¨Í∏∞ÏÑú \(\gamma\in[0,1)\)Îäî Ìï†Ïù∏Ïú®Î°ú, ÎØ∏Îûò Î≥¥ÏÉÅÏù¥ ÌòÑÏû¨ Í∞ÄÏπòÏóê Í∏∞Ïó¨ÌïòÎäî Ï†ïÎèÑÎ•º Ï°∞Ï†àÌïúÎã§. Ïû•Í∏∞ Î≥¥ÏÉÅÏùÄ ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏùºÎ†®Ïùò ÌñâÎèô Í≤∞Í≥ºÎ•º Ï¢ÖÌï©Ï†ÅÏúºÎ°ú ÌèâÍ∞ÄÌï† Ïàò ÏûàÍ≤å Ìï¥Ï£ºÎ©∞, Ïù¥Îäî Í∞ïÌôîÌïôÏäµÏùò Ï§ëÏã¨ Î™©ÌëúÏù∏ Ïû•Í∏∞Ï†Å Í∞ÄÏπò ÏµúÎåÄÌôîÏóê ÌïÑÏàòÏ†ÅÏù¥Îã§.<br />

Ï¶âÏãú Î≥¥ÏÉÅÍ≥º Ïû•Í∏∞ Î≥¥ÏÉÅÏùò Ï∞®Ïù¥Îäî ÏãúÍ∞ÑÏ†Å Í¥ÄÏ†êÏóêÏÑú Ï§ëÏöîÌïòÎã§. Ï¶âÏãú Î≥¥ÏÉÅÏùÄ ÏßßÏùÄ ÏãúÍ∞Ñ Î≤îÏúÑÏóêÏÑúÏùò ÏÑ±Í≥ºÎßåÏùÑ ÌèâÍ∞ÄÌïòÎØÄÎ°ú, Ïù¥ÏóêÎßå ÏßëÏ§ëÌïòÎ©¥ Í∑ºÏãúÏïàÏ†ÅÏù∏ ÌñâÎèôÏùÑ Ï¥àÎûòÌï† Ïàò ÏûàÎã§. Ïû•Í∏∞ Î≥¥ÏÉÅÏùÄ Îçî ÎÑìÏùÄ ÏãúÍ∞Ñ Î≤îÏúÑÏóêÏÑú ÌñâÎèôÏùò Í≤∞Í≥ºÎ•º Í≥†Î†§ÌïòÎØÄÎ°ú, ÎïåÎ°úÎäî Ï¶âÍ∞ÅÏ†ÅÏù∏ Î≥¥ÏÉÅÏùÑ Ìù¨ÏÉùÌïòÎçîÎùºÎèÑ Ïû•Í∏∞Ï†ÅÏúºÎ°ú Îçî Í∞ÄÏπò ÏûàÎäî Í≤∞Ï†ïÏùÑ ÎÇ¥Î¶¥ Ïàò ÏûàÍ≤å ÌïúÎã§.<br />

Í∞ïÌôîÌïôÏäµÏóêÏÑúÎäî Ïù¥ÏÉÅÏ†ÅÏúºÎ°ú ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ïû•Í∏∞ Î≥¥ÏÉÅÏùÑ ÏµúÎåÄÌôîÌïòÎäî Ï†ïÏ±ÖÏùÑ ÌïôÏäµÌïòÍ≥†Ïûê ÌïúÎã§. Í∑∏Îü¨ÎÇò Ïû•Í∏∞ Î≥¥ÏÉÅÏùÄ ÏßÅÏ†ë Í¥ÄÏ∞∞Ìï† Ïàò ÏóÜÏúºÎØÄÎ°ú, Í∞ÄÏπò Ìï®ÏàòÎÇò Q-Ìï®ÏàòÏôÄ Í∞ôÏùÄ Ï∂îÏ†ïÏπòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÌïôÏäµÌïúÎã§. Ï¶âÏãú Î≥¥ÏÉÅÏùÄ Ïù¥Îü¨Ìïú Í∞ÄÏπò Ìï®Ïàò ÏóÖÎç∞Ïù¥Ìä∏Ïùò Í∏∞Ï¥àÍ∞Ä ÎêòÎ©∞, Î≤®Îßå Î∞©Ï†ïÏãùÏùÑ ÌÜµÌï¥ Îã®Í∏∞Ï†Å ÌîºÎìúÎ∞±ÏùÑ Ïû•Í∏∞Ï†Å Í∞ÄÏπò Ï∂îÏ†ïÏúºÎ°ú Ïó∞Í≤∞ÌïúÎã§.
</details>
</details>

<details>
<summary>5. Í≤∞Ï†ïÏ†Å Ï†ïÏ±ÖÍ≥º ÌôïÎ•†Ï†Å Ï†ïÏ±ÖÏùò Ï†ïÏùòÎ•º ÎπÑÍµêÌïòÍ≥†, Í∞ÅÍ∞ÅÏùò Ïû•Îã®Ï†êÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

Í≤∞Ï†ïÏ†Å(deterministic) Ï†ïÏ±Ö \(\pi(s)=a\)ÏôÄ ÌôïÎ•†Ï†Å(stochastic) Ï†ïÏ±Ö \(\pi(a\mid s)\)ÏùÑ Ï†ïÏùòÌïòÍ≥†, Í∞ÅÍ∞ÅÏùò Ïû•Îã®Ï†êÏùÑ ÏÇ¨Î°ÄÏôÄ Ìï®Íªò ÏÑ§Î™ÖÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Í∞ïÌôîÌïôÏäµÏóêÏÑú Ï†ïÏ±ÖÏùÄ ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Í∞Å ÏÉÅÌÉúÏóêÏÑú Ïñ¥Îñ§ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï†ÏßÄ Í≤∞Ï†ïÌïòÎäî Í∑úÏπôÏùÑ ÏùòÎØ∏ÌïúÎã§. Í≤∞Ï†ïÏ†Å Ï†ïÏ±ÖÍ≥º ÌôïÎ•†Ï†Å Ï†ïÏ±ÖÏùÄ Ïù¥ Í∑úÏπôÏùÑ Ï†ïÏùòÌïòÎäî Îëê Í∞ÄÏßÄ ÏÑúÎ°ú Îã§Î•∏ Î∞©ÏãùÏù¥Îã§.<br />

Í≤∞Ï†ïÏ†Å Ï†ïÏ±Ö(deterministic policy)ÏùÄ Í∞Å ÏÉÅÌÉúÏóêÏÑú Ï†ïÌôïÌûà ÌïòÎÇòÏùò ÌñâÎèôÎßåÏùÑ Îß§ÌïëÌïòÎäî Ìï®Ïàò \(\pi: \mathcal{S} \to \mathcal{A}\)Î°ú Ï†ïÏùòÎêúÎã§. Ï¶â, \(\pi(s)=a\)Îäî ÏÉÅÌÉú \(s\)ÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ìï≠ÏÉÅ ÌñâÎèô \(a\)Î•º ÏÑ†ÌÉùÌïúÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏ÌïúÎã§. Í≤∞Ï†ïÏ†Å Ï†ïÏ±ÖÏùò Ï£ºÏöî Ïû•Ï†êÏùÄ Íµ¨ÌòÑÏù¥ Îã®ÏàúÌïòÍ≥† Í≥ÑÏÇ∞ Ìö®Ïú®ÏÑ±Ïù¥ ÎÜíÎã§Îäî Ï†êÏù¥Îã§. Í∞Å ÏÉÅÌÉúÏóêÏÑú ÌïòÎÇòÏùò ÌñâÎèôÎßå Í≥†Î†§ÌïòÎ©¥ ÎêòÎØÄÎ°ú, Í≥ÑÌöç ÏïåÍ≥†Î¶¨Ï¶òÏù¥ÎÇò Ï†ïÏ±Ö ÌèâÍ∞Ä Ïãú Í≥ÑÏÇ∞ Î≥µÏû°ÎèÑÍ∞Ä ÎÇÆÎã§. ÎòêÌïú ÌïôÏäµÎêú ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Ïã§ÌñâÌï† Îïå ÏùºÍ¥ÄÎêú ÌñâÎèôÏùÑ Î≥¥Ïû•ÌïúÎã§. Í∑∏Îü¨ÎÇò Í≤∞Ï†ïÏ†Å Ï†ïÏ±ÖÏùò Í∞ÄÏû• ÌÅ∞ Îã®Ï†êÏùÄ ÌÉêÌóò Îä•Î†•Ïù¥ Ï†úÌïúÎêúÎã§Îäî Í≤ÉÏù¥Îã§. ÏÉàÎ°úÏö¥ ÌñâÎèôÏùÑ ÏãúÎèÑÌïòÏßÄ ÏïäÏúºÎ©¥ Îçî ÎÇòÏùÄ Ï†ÑÎûµÏùÑ Î∞úÍ≤¨ÌïòÏßÄ Î™ªÌï† Ïàò ÏûàÏúºÎ©∞, Ïù¥Î°ú Ïù∏Ìï¥ ÏßÄÏó≠ ÏµúÏ†ÅÌï¥(local optima)Ïóê Í∞áÌûê ÏúÑÌóòÏù¥ ÏûàÎã§.<br />

Î∞òÎ©¥, ÌôïÎ•†Ï†Å Ï†ïÏ±Ö(stochastic policy)ÏùÄ Í∞Å ÏÉÅÌÉúÏóêÏÑú Í∞ÄÎä•Ìïú Î™®Îì† ÌñâÎèôÏóê ÎåÄÌïú ÌôïÎ•† Î∂ÑÌè¨Î°ú Ï†ïÏùòÎêúÎã§. ÏàòÌïôÏ†ÅÏúºÎ°úÎäî \(\pi(a\mid s)=\Pr\{A_t=a\mid S_t=s\}\)Î°ú ÌëúÌòÑÎêòÎ©∞, Ïù¥Îäî ÏÉÅÌÉú \(s\)ÏóêÏÑú ÌñâÎèô \(a\)Î•º ÏÑ†ÌÉùÌï† ÌôïÎ•†ÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. ÌôïÎ•†Ï†Å Ï†ïÏ±ÖÏùò Í∞ÄÏû• ÌÅ∞ Ïû•Ï†êÏùÄ ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌÉêÌóò Î©îÏª§ÎãàÏ¶òÏùÑ Ï†úÍ≥µÌïúÎã§Îäî Í≤ÉÏù¥Îã§. Îã§ÏñëÌïú ÌñâÎèôÏóê ÏùºÏ†ï ÌôïÎ•†ÏùÑ Ìï†ÎãπÌï®ÏúºÎ°úÏç®, ÏóêÏù¥Ï†ÑÌä∏Îäî ÏµúÏÑ†ÏúºÎ°ú ÏÉùÍ∞ÅÎêòÎäî ÌñâÎèô Ïô∏ÏóêÎèÑ Îã§Î•∏ ÌñâÎèôÏùÑ ÏãúÎèÑÌï† Ïàò ÏûàÎã§. Ïù¥Îäî Î≥µÏû°Ìïú ÌôòÍ≤ΩÏóêÏÑú Îçî ÎÇòÏùÄ Ï†ÑÎûµÏùÑ Î∞úÍ≤¨Ìï† Í∞ÄÎä•ÏÑ±ÏùÑ ÎÜíÏù¥Í≥†, ÏßÄÏó≠ ÏµúÏ†ÅÌï¥Î•º ÌîºÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêúÎã§. ÎòêÌïú ÌôïÎ•†Ï†Å Ï†ïÏ±ÖÏùÄ Î©ÄÌã∞ ÏóêÏù¥Ï†ÑÌä∏ ÌôòÍ≤ΩÏù¥ÎÇò Î∂ÄÎ∂Ñ Í¥ÄÏ∞∞ Í∞ÄÎä•Ìïú ÌôòÍ≤ΩÏóêÏÑú Îçî Í∞ïÍ±¥Ìïú ÏÑ±Îä•ÏùÑ Î≥¥Ïùº Ïàò ÏûàÎã§. Í∑∏Îü¨ÎÇò ÌôïÎ•†Ï†Å Ï†ïÏ±ÖÏùÄ ÌïôÏäµ ÏÜçÎèÑÍ∞Ä ÏÉÅÎåÄÏ†ÅÏúºÎ°ú ÎäêÎ¶¥ Ïàò ÏûàÏúºÎ©∞, ÏµúÏ¢Ö Ï†ïÏ±ÖÏù¥ Í≤∞Ï†ïÏ†Å Ï†ïÏ±ÖÎßåÌÅº ÏµúÏ†ÅÌôîÎêòÏßÄ ÏïäÏùÑ Ïàò ÏûàÎã§.<br />

Ïã§Ï†ú ÏÇ¨Î°ÄÎ°ú, ÎØ∏Î°ú ÌÉêÏÉâ Î¨∏Ï†úÎ•º ÏÉùÍ∞ÅÌï¥ Î≥¥Ïûê. Í≤∞Ï†ïÏ†Å Ï†ïÏ±ÖÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥ ÏóêÏù¥Ï†ÑÌä∏Îäî Í∞Å ÏúÑÏπòÏóêÏÑú Ìï≠ÏÉÅ Í∞ôÏùÄ Î∞©Ìñ•ÏúºÎ°ú Ïù¥ÎèôÌïúÎã§. Ïù¥Îäî Ìïú Î≤à Í≤ΩÎ°úÎ•º Ï∞æÏïòÎã§Î©¥ Ìö®Ïú®Ï†ÅÏúºÎ°ú Î™©ÌëúÏóê ÎèÑÎã¨Ìï† Ïàò ÏûàÏßÄÎßå, Îçî ÎÇòÏùÄ Í≤ΩÎ°úÍ∞Ä ÏûàÏùÑ Ïàò ÏûàÏùåÏóêÎèÑ Í≥ÑÏÜç Í∞ôÏùÄ Í≤ΩÎ°úÎßå ÌÉêÏÉâÌïúÎã§. ÌôïÎ•†Ï†Å Ï†ïÏ±ÖÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥ ÏóêÏù¥Ï†ÑÌä∏Îäî Ï£ºÎ°ú Í∞ÄÏû• Ïú†ÎßùÌïú Î∞©Ìñ•ÏúºÎ°ú Ïù¥ÎèôÌïòÏßÄÎßå, Í∞ÄÎÅî Îã§Î•∏ Î∞©Ìñ•ÎèÑ ÏãúÎèÑÌïúÎã§. Ïù¥Î°ú Ïù∏Ìï¥ Ï≤òÏùåÏóêÎäî ÎπÑÌö®Ïú®Ï†ÅÏùº Ïàò ÏûàÏßÄÎßå, Í≤∞Íµ≠ ÏµúÏ†ÅÏùò Í≤ΩÎ°úÎ•º Î∞úÍ≤¨Ìï† Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏïÑÏßÑÎã§.<br />

Í∞ïÌôîÌïôÏäµ Ïã§Î¨¥ÏóêÏÑúÎäî Ï¢ÖÏ¢Ö ÌïôÏäµ Ï¥àÍ∏∞ÏóêÎäî ÌÉêÌóòÏùÑ Ï¥âÏßÑÌïòÍ∏∞ ÏúÑÌï¥ ÌôïÎ•†Ï†Å Ï†ïÏ±ÖÏùÑ ÏÇ¨Ïö©ÌïòÍ≥†, ÌïôÏäµÏù¥ ÏßÑÌñâÎê®Ïóê Îî∞Îùº Ï†êÏ∞® Í≤∞Ï†ïÏ†Å Ï†ïÏ±ÖÏúºÎ°ú Ï†ÑÌôòÌïòÎäî Ï†ÑÎûµÏùÑ Ï±ÑÌÉùÌïúÎã§. Ïù¥Î•º ÌÜµÌï¥ ÌÉêÌóòÍ≥º ÌôúÏö©Ïùò Í∑†ÌòïÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Ï°∞Ï†àÌï† Ïàò ÏûàÎã§.
</details>
</details>

<details>
<summary>6. Ìï†Ïù∏Îêú Î¶¨ÌÑ¥Ïù¥ ÏàòÎ†¥ÌïòÍ∏∞ ÏúÑÌïú ÏàòÌïôÏ†Å Ï°∞Í±¥ÏùÑ Ïú†ÎèÑÌïòÍ≥†, \(\gamma\) Í∞íÏóê Îî∞Î•∏ ÌïôÏäµ Í≤∞Í≥ºÏùò Ï∞®Ïù¥Î•º ÎÖºÏùòÌïòÏãúÏò§.</summary>

Î¶¨ÌÑ¥ \(G_t=\sum_{k=0}^\infty\gamma^kR_{t+k+1}\)Ïù¥ ÏàòÎ†¥ÌïòÍ∏∞ ÏúÑÌïú ÏàòÌïôÏ†Å Ï°∞Í±¥ÏùÑ Ïú†ÎèÑÌïòÍ≥†, \(\gamma\) Í∞íÏù¥ 0Ïóê Í∞ÄÍπåÏö∏ ÎïåÏôÄ 1Ïóê Í∞ÄÍπåÏö∏ Îïå ÌïôÏäµ Í≤∞Í≥ºÏóê Ïñ¥Îñ§ Ï∞®Ïù¥Í∞Ä ÎÇòÎäîÏßÄ ÏÑúÏà†ÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Í∞ïÌôîÌïôÏäµÏóêÏÑú Ìï†Ïù∏Îêú Î¶¨ÌÑ¥(discounted return)ÏùÄ ÏãúÍ∞Ñ \(t\)Î∂ÄÌÑ∞ ÏãúÏûëÌïòÏó¨ ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Î∞õÍ≤å Îê† Î™®Îì† ÎØ∏Îûò Î≥¥ÏÉÅÏùò Í∞ÄÏ§ë Ìï©ÏúºÎ°ú Ï†ïÏùòÎêúÎã§. ÏàòÌïôÏ†ÅÏúºÎ°ú Ïù¥Îäî \(G_t=\sum_{k=0}^\infty\gamma^kR_{t+k+1}\)Î°ú ÌëúÌòÑÎêúÎã§. Ïù¥ Î¨¥Ìïú Ìï©Ïù¥ Ïú†ÌïúÌïú Í∞íÏúºÎ°ú ÏàòÎ†¥ÌïòÍ∏∞ ÏúÑÌïú Ï°∞Í±¥ÏùÑ ÏÇ¥Ìé¥Î≥¥Ïûê.<br />

Î®ºÏ†Ä, Î≥¥ÏÉÅ \(R_t\)Í∞Ä Ïñ¥Îñ§ ÏÉÅÏàò \(R_{max}\)Î°ú Ï†úÌïúÎêúÎã§Í≥† Í∞ÄÏ†ïÌïúÎã§. Ï¶â, Î™®Îì† ÏãúÍ∞Ñ \(t\)Ïóê ÎåÄÌï¥ \(|R_t| \leq R_{max}\)Ïù¥Îã§. Ïù¥Ï†ú Ìï†Ïù∏Îêú Î¶¨ÌÑ¥Ïùò Ï†àÎåÄÍ∞íÏóê ÎåÄÌïú ÏÉÅÌïúÏùÑ Í≥ÑÏÇ∞Ìï¥ Î≥¥Î©¥:<br />

\(|G_t| = |\sum_{k=0}^\infty\gamma^kR_{t+k+1}| \leq \sum_{k=0}^\infty\gamma^k|R_{t+k+1}| \leq R_{max}\sum_{k=0}^\infty\gamma^k\)<br />

ÎßàÏßÄÎßâ Ìï© \(\sum_{k=0}^\infty\gamma^k\)Îäî Í∏∞ÌïòÍ∏âÏàòÏù¥Îã§. Ïù¥ Í∏âÏàòÍ∞Ä ÏàòÎ†¥ÌïòÍ∏∞ ÏúÑÌïú ÌïÑÏöîÏ∂©Î∂ÑÏ°∞Í±¥ÏùÄ \(|\gamma| &lt; 1\)Ïù¥Îã§. Ïù¥ Ï°∞Í±¥Ïù¥ ÎßåÏ°±ÎêòÎ©¥, Í∏âÏàòÎäî \(\frac{1}{1-\gamma}\)Î°ú ÏàòÎ†¥ÌïúÎã§. Îî∞ÎùºÏÑú \(|G_t| \leq \frac{R_{max}}{1-\gamma}\)Í∞Ä ÎêòÏñ¥ Ìï†Ïù∏Îêú Î¶¨ÌÑ¥Ïù¥ Ïú†ÌïúÌïú Í∞íÏúºÎ°ú ÏàòÎ†¥ÌïúÎã§.<br />

Ïã§Ï†ú Í∞ïÌôîÌïôÏäµÏóêÏÑúÎäî \(\gamma \in [0,1)\)Ïùò Î≤îÏúÑÎ•º ÏÇ¨Ïö©ÌïòÎ©∞, \(\gamma\) Í∞íÏùò ÏÑ†ÌÉùÏùÄ ÌïôÏäµ Í≤∞Í≥ºÏóê Ï§ëÏöîÌïú ÏòÅÌñ•ÏùÑ ÎØ∏ÏπúÎã§.<br />

\(\gamma\) Í∞íÏù¥ 0Ïóê Í∞ÄÍπåÏö∏ Îïå(Ïòà: \(\gamma = 0.1\)), ÏóêÏù¥Ï†ÑÌä∏Îäî Ï¶âÍ∞ÅÏ†ÅÏù∏ Î≥¥ÏÉÅÎßåÏùÑ Í≥†Î†§ÌïúÎã§. Ïù¥Îäî Î¶¨ÌÑ¥Ïù¥ \(G_t \approx R_{t+1}\)Î°ú Îã®ÏàúÌôîÎêòÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§. Ïù¥Î°ú Ïù∏Ìï¥ ÏóêÏù¥Ï†ÑÌä∏Îäî Îã®Í∏∞Ï†Å Ïù¥ÏùµÏùÑ Ï∂îÍµ¨ÌïòÍ≥†, Ïû•Í∏∞Ï†Å Í≤∞Í≥ºÎ•º Î¨¥ÏãúÌïòÎäî Í∑ºÏãúÏïàÏ†ÅÏù∏ ÌñâÎèôÏùÑ ÌïòÍ≤å ÎêúÎã§. Ïù¥Îü¨Ìïú ÏÑ§Ï†ïÏùÄ Î≥¥ÏÉÅ Ïã†Ìò∏Í∞Ä Î™ÖÌôïÌïòÍ≥† Ï¶âÍ∞ÅÏ†ÅÏù∏ ÌîºÎìúÎ∞±Ïù¥ Ï§ëÏöîÌïú Îã®ÏàúÌïú ÌôòÍ≤ΩÏóêÏÑú Ïú†Ïö©Ìï† Ïàò ÏûàÎã§. ÎòêÌïú ÌïôÏäµ ÏÜçÎèÑÍ∞Ä Îπ†Î•¥Îã§Îäî Ïû•Ï†êÏù¥ ÏûàÏßÄÎßå, Î≥µÏû°Ìïú Î¨∏Ï†úÏóêÏÑúÎäî ÏµúÏ†ÅÏù¥ ÏïÑÎãå Ï†ïÏ±ÖÏùÑ ÌïôÏäµÌï† Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÎã§.<br />

Î∞òÎ©¥, \(\gamma\) Í∞íÏù¥ 1Ïóê Í∞ÄÍπåÏö∏ Îïå(Ïòà: \(\gamma = 0.99\)), ÏóêÏù¥Ï†ÑÌä∏Îäî Î®º ÎØ∏ÎûòÏùò Î≥¥ÏÉÅÎèÑ ÌòÑÏû¨ÏôÄ Í±∞Ïùò ÎèôÎì±ÌïòÍ≤å Ï§ëÏöîÌïòÍ≤å Í≥†Î†§ÌïúÎã§. Ïù¥Îäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ïû•Í∏∞Ï†ÅÏù∏ Í≤∞Í≥ºÎ•º ÏúÑÌï¥ Îã®Í∏∞Ï†Å Î≥¥ÏÉÅÏùÑ Ìù¨ÏÉùÌï† Ïàò ÏûàÍ≤å Ìï¥Ï§ÄÎã§. Î≥µÏû°Ìïú ÌôòÍ≤ΩÏù¥ÎÇò ÏßÄÏó∞Îêú Î≥¥ÏÉÅÏù¥ ÏûàÎäî Î¨∏Ï†úÏóêÏÑúÎäî ÎÜíÏùÄ \(\gamma\) Í∞íÏù¥ ÌïÑÏöîÌïòÎã§. ÏòàÎ•º Îì§Ïñ¥, Ï≤¥Ïä§ÎÇò Î∞îÎëëÍ≥º Í∞ôÏùÄ Í≤åÏûÑÏóêÏÑúÎäî Í≤åÏûÑÏù¥ ÎÅùÎÇ† ÎïåÍπåÏßÄ ÏµúÏ¢Ö Í≤∞Í≥ºÎ•º Ïïå Ïàò ÏóÜÏúºÎØÄÎ°ú, ÏùºÎ†®Ïùò ÌñâÎèôÏùò Ïû•Í∏∞Ï†Å Í∞ÄÏπòÎ•º ÌèâÍ∞ÄÌïòÍ∏∞ ÏúÑÌï¥ ÎÜíÏùÄ \(\gamma\) Í∞íÏù¥ Ï§ëÏöîÌïòÎã§. Í∑∏Îü¨ÎÇò ÎÜíÏùÄ \(\gamma\) Í∞íÏùÄ Î≥¥ÏÉÅÏùò Î∂ÑÏÇ∞ÏùÑ Ï¶ùÍ∞ÄÏãúÌÇ§Í≥† ÌïôÏäµÏùÑ Î∂àÏïàÏ†ïÌïòÍ≤å ÎßåÎì§ Ïàò ÏûàÏúºÎ©∞, ÏàòÎ†¥ ÏÜçÎèÑÍ∞Ä ÎäêÎ†§Ïßà Ïàò ÏûàÎã§.<br />

Ïã§Î¨¥ÏóêÏÑúÎäî Î¨∏Ï†úÏùò ÌäπÏÑ±Ïóê Îî∞Îùº Ï†ÅÏ†àÌïú \(\gamma\) Í∞íÏùÑ ÏÑ†ÌÉùÌïòÎäî Í≤ÉÏù¥ Ï§ëÏöîÌïòÎã§. ÏùºÎ∞òÏ†ÅÏúºÎ°ú \(\gamma = 0.9\)ÏóêÏÑú \(\gamma = 0.99\) ÏÇ¨Ïù¥Ïùò Í∞íÏù¥ ÎßéÏù¥ ÏÇ¨Ïö©ÎêòÎ©∞, Ïù¥Îäî Îã®Í∏∞Ï†Å ÌñâÎèôÍ≥º Ïû•Í∏∞Ï†Å Í≥ÑÌöç ÏÇ¨Ïù¥Ïùò Ìï©Î¶¨Ï†ÅÏù∏ Í∑†ÌòïÏùÑ Ï†úÍ≥µÌïúÎã§. ÎòêÌïú, ÌïôÏäµ Ï¥àÍ∏∞ÏóêÎäî ÎÇÆÏùÄ \(\gamma\) Í∞íÏúºÎ°ú ÏãúÏûëÌïòÏó¨ ÌïôÏäµÏù¥ ÏßÑÌñâÎê®Ïóê Îî∞Îùº Ï†êÏßÑÏ†ÅÏúºÎ°ú Ï¶ùÍ∞ÄÏãúÌÇ§Îäî Î∞©Î≤ïÎèÑ Ìö®Í≥ºÏ†ÅÏùº Ïàò ÏûàÎã§.
</details>
</details>

<details>
<summary>7. ÏÉÅÌÉú Í∞ÄÏπò Ìï®ÏàòÏùò Ï†ïÏùòÏôÄ Í∏∞ÎåÄÍ∞í Í¥ÄÏ†êÏóêÏÑúÏùò Ìï¥ÏÑùÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

Ï†ïÏ±Ö \(\pi\)ÌïòÏóêÏÑú ÏÉÅÌÉú \(s\)Ïùò Í∞ÄÏπò Ìï®Ïàò \(V_\pi(s)\)Î•º ÏàòÏãùÏúºÎ°ú Ï†ïÏùòÌïòÍ≥†, Í∏∞ÎåÄÍ∞í(\(\mathbb{E}_\pi\)) Í¥ÄÏ†êÏóêÏÑú Ìï¥ÏÑùÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V_\pi(s)\)Îäî Í∞ïÌôîÌïôÏäµÏùò ÌïµÏã¨ Í∞úÎÖêÏúºÎ°ú, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ï†ïÏ±Ö \(\pi\)Î•º Îî∞Î•º Îïå ÏÉÅÌÉú \(s\)ÏóêÏÑú Í∏∞ÎåÄÌï† Ïàò ÏûàÎäî ÎØ∏Îûò Î≥¥ÏÉÅÏùò Ï¥ùÌï©ÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. ÏàòÌïôÏ†ÅÏúºÎ°ú Ïù¥Îäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÎêúÎã§:<br />

\[
V_\pi(s)=\mathbb{E}_\pi\Bigl[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\mid S_t=s\Bigr]
\]<br />

Ïù¥ ÏàòÏãùÏùÄ ÏÉÅÌÉú \(s\)ÏóêÏÑú ÏãúÏûëÌïòÏó¨ Ï†ïÏ±Ö \(\pi\)Î•º Îî∞Î•º Îïå ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Î∞õÏùÑ Í≤ÉÏúºÎ°ú Í∏∞ÎåÄÎêòÎäî Ìï†Ïù∏Îêú ÎàÑÏ†Å Î≥¥ÏÉÅ(discounted cumulative reward)ÏùÑ ÏùòÎØ∏ÌïúÎã§. Ïó¨Í∏∞ÏÑú \(\mathbb{E}_\pi\)Îäî Ï†ïÏ±Ö \(\pi\)Î•º Îî∞Î•º ÎïåÏùò Í∏∞ÎåÄÍ∞í Ïó∞ÏÇ∞ÏûêÏù¥Í≥†, \(\gamma\)Îäî ÎØ∏Îûò Î≥¥ÏÉÅÏùò Ï§ëÏöîÎèÑÎ•º Ï°∞Ï†àÌïòÎäî Ìï†Ïù∏Ïú®Ïù¥Îã§.<br />

Í∏∞ÎåÄÍ∞í Í¥ÄÏ†êÏóêÏÑú ÏÉÅÌÉú Í∞ÄÏπò Ìï®ÏàòÎäî ÌäπÏ†ï ÏÉÅÌÉúÏóêÏÑú ÏãúÏûëÌïòÏó¨ Ï†ïÏ±ÖÏùÑ Îî∞Î•º Îïå Î∞úÏÉùÌï† Ïàò ÏûàÎäî Î™®Îì† Í∞ÄÎä•Ìïú Í∂§Ï†Å(trajectory)Ïóê ÎåÄÌïú Í∞ÄÏ§ë ÌèâÍ∑†ÏúºÎ°ú Ìï¥ÏÑùÌï† Ïàò ÏûàÎã§. Í∞Å Í∂§Ï†ÅÏùÄ ÏÉÅÌÉúÏôÄ ÌñâÎèôÏùò ÏãúÌÄÄÏä§Î°ú, ÏÑúÎ°ú Îã§Î•∏ ÌôòÍ≤Ω Ïó≠Ìïô(dynamics)Í≥º Ï†ïÏ±ÖÏùò ÌôïÎ•†Ï†Å ÌäπÏÑ±ÏúºÎ°ú Ïù∏Ìï¥ Îã§ÏñëÌïú Í∂§Ï†ÅÏù¥ Î∞úÏÉùÌï† Ïàò ÏûàÎã§.<br />

Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú, Ï†ïÏ±Ö \(\pi\)Í∞Ä Í∞Å ÏÉÅÌÉúÏóêÏÑú ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï† ÌôïÎ•† \(\pi(a|s)\)Î•º Í≤∞Ï†ïÌïòÍ≥†, ÌôòÍ≤ΩÏùò Ïó≠ÌïôÏù¥ ÏÉÅÌÉú Ï†ÑÏù¥ ÌôïÎ•† \(P(s'|s,a)\)Î•º Í≤∞Ï†ïÌïúÎã§. Ïù¥Îü¨Ìïú ÌôïÎ•†Îì§Ïùò Ï°∞Ìï©ÏúºÎ°ú Îã§ÏñëÌïú Í∂§Ï†ÅÏù¥ ÏÉùÏÑ±ÎêòÎ©∞, Í∞Å Í∂§Ï†ÅÏùÄ ÏûêÏã†ÎßåÏùò Î≥¥ÏÉÅ ÏãúÌÄÄÏä§Î•º Í∞ÄÏßÑÎã§. Í∞ÄÏπò Ìï®Ïàò \(V_\pi(s)\)Îäî Ïù¥Îü¨Ìïú Î™®Îì† Í∞ÄÎä•Ìïú Í∂§Ï†ÅÏùò Ìï†Ïù∏Îêú Î≥¥ÏÉÅ Ìï©ÏùÑ Í∞Å Í∂§Ï†ÅÏù¥ Î∞úÏÉùÌï† ÌôïÎ•†Î°ú Í∞ÄÏ§ë ÌèâÍ∑†Ìïú Í≤ÉÏù¥Îã§.<br />

Ïù¥Îü¨Ìïú Í∏∞ÎåÄÍ∞í Í¥ÄÏ†êÏùÄ Í∞ïÌôîÌïôÏäµÏóêÏÑú Î™á Í∞ÄÏßÄ Ï§ëÏöîÌïú ÏùòÎØ∏Î•º Í∞ñÎäîÎã§. Ï≤´Ïß∏, ÏÉÅÌÉú Í∞ÄÏπò Ìï®ÏàòÎäî Ï†ïÏ±ÖÏùò ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌïòÎäî Í∞ùÍ¥ÄÏ†ÅÏù∏ Ï≤ôÎèÑÎ•º Ï†úÍ≥µÌïúÎã§. ÏÑúÎ°ú Îã§Î•∏ Ï†ïÏ±Ö ÌïòÏóêÏÑú ÏÉÅÌÉúÏùò Í∞ÄÏπòÎ•º ÎπÑÍµêÌï®ÏúºÎ°úÏç®, Ïñ¥Îñ§ Ï†ïÏ±ÖÏù¥ Îçî ÎÇòÏùÄÏßÄ ÌåêÎã®Ìï† Ïàò ÏûàÎã§. ÎëòÏß∏, Í∏∞ÎåÄÍ∞í Í¥ÄÏ†êÏùÄ ÌôïÎ•†Ï†Å ÌôòÍ≤ΩÍ≥º Ï†ïÏ±ÖÏùÑ ÏûêÏó∞Ïä§ÎüΩÍ≤å Îã§Î£∞ Ïàò ÏûàÍ≤å Ìï¥Ï§ÄÎã§. ÌôòÍ≤ΩÏù¥ÎÇò Ï†ïÏ±ÖÏù¥ Í≤∞Ï†ïÏ†ÅÏù¥ÏßÄ ÏïäÎçîÎùºÎèÑ, Í∞ÄÏπò Ìï®ÏàòÎäî Ïó¨Ï†ÑÌûà ÏùòÎØ∏ ÏûàÎäî ÌèâÍ∞ÄÎ•º Ï†úÍ≥µÌïúÎã§. ÏÖãÏß∏, Ïù¥ Í¥ÄÏ†êÏùÄ Î≤®Îßå Î∞©Ï†ïÏãùÍ≥º Í∞ôÏùÄ Ïû¨Í∑ÄÏ†Å Í¥ÄÍ≥ÑÎ•º Ïú†ÎèÑÌïòÎäî Îç∞ ÌïÑÏàòÏ†ÅÏù¥Îã§. Ïù¥Î•º ÌÜµÌï¥ ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏù¥ÎÇò ÏãúÍ∞ÑÏ∞® ÌïôÏäµÍ≥º Í∞ôÏùÄ Ìö®Ïú®Ï†ÅÏù∏ Í≥ÑÏÇ∞ Î∞©Î≤ïÏù¥ Í∞ÄÎä•Ìï¥ÏßÑÎã§.<br />

Ïã§Ï†ú Í∞ïÌôîÌïôÏäµ ÏïåÍ≥†Î¶¨Ï¶òÏóêÏÑúÎäî Ïù¥ Í∞ÄÏπò Ìï®ÏàòÎ•º ÏßÅÏ†ë Í≥ÑÏÇ∞ÌïòÍ±∞ÎÇò Í∑ºÏÇ¨ÌïòÎäî Í≤ÉÏù¥ ÌïµÏã¨ Í≥ºÏ†úÏù¥Îã§. Ï†ïÏ±Ö ÌèâÍ∞Ä(policy evaluation)Îäî Ï£ºÏñ¥ÏßÑ Ï†ïÏ±ÖÏóê ÎåÄÌïú Í∞ÄÏπò Ìï®ÏàòÎ•º Í≥ÑÏÇ∞ÌïòÎäî Í≥ºÏ†ïÏù¥Î©∞, Ï†ïÏ±Ö Í∞úÏÑ†(policy improvement)ÏùÄ Í≥ÑÏÇ∞Îêú Í∞ÄÏπò Ìï®ÏàòÎ•º Í∏∞Î∞òÏúºÎ°ú Îçî ÎÇòÏùÄ Ï†ïÏ±ÖÏùÑ Ï∞æÎäî Í≥ºÏ†ïÏù¥Îã§. Ïù¥ Îëê Í≥ºÏ†ïÏù¥ Î∞òÎ≥µÎêòÎ©¥ÏÑú ÏµúÏ†Å Ï†ïÏ±ÖÏóê Ï†ëÍ∑ºÌïòÍ≤å ÎêúÎã§.
</details>
</details>

<details>
<summary>8. ÌñâÎèô Í∞ÄÏπò Ìï®ÏàòÏùò Ï†ïÏùòÏôÄ ÏÉÅÌÉú Í∞ÄÏπò Ìï®ÏàòÏôÄÏùò Í¥ÄÍ≥ÑÎ•º ÏàòÏãùÏúºÎ°ú ÏÑ§Î™ÖÌïòÍ≥†, QÌï®ÏàòÏùò Ïú†Ïö©ÏÑ±ÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

\(Q_\pi(s,a)\)Î•º ÏàòÏãùÏúºÎ°ú Ï†ïÏùòÌïòÍ≥†, ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V_\pi(s)\)ÏôÄÏùò Í¥ÄÍ≥ÑÎ•º ÏàòÏãùÏúºÎ°ú Î≥¥Ïó¨Ï§Ä Îí§, ÏÇ¨Î°ÄÎ•º ÌÜµÌï¥ QÌï®ÏàòÏùò Ïú†Ïö©ÏÑ±ÏùÑ ÏÑ§Î™ÖÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
ÌñâÎèô Í∞ÄÏπò Ìï®Ïàò \(Q_\pi(s,a)\), ÏùºÎ∞òÏ†ÅÏúºÎ°ú Q-Ìï®ÏàòÎùºÍ≥† Î∂àÎ¶¨Îäî Ïù¥ Í∞úÎÖêÏùÄ ÏÉÅÌÉú \(s\)ÏóêÏÑú ÌñâÎèô \(a\)Î•º Ï∑®Ìïú ÌõÑ Ï†ïÏ±Ö \(\pi\)Î•º Îî∞Î•º Îïå Í∏∞ÎåÄÎêòÎäî ÎØ∏Îûò ÎàÑÏ†Å Î≥¥ÏÉÅÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. ÏàòÌïôÏ†ÅÏúºÎ°ú Ïù¥Îäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÎêúÎã§:<br />

\[
Q_\pi(s,a)=\mathbb{E}_\pi\Bigl[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\mid S_t=s,A_t=a\Bigr]
\]<br />

Ïù¥ Ï†ïÏùòÎäî ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V_\pi(s)\)ÏôÄ Ïú†ÏÇ¨ÌïòÏßÄÎßå, Ï§ëÏöîÌïú Ï∞®Ïù¥Ï†êÏùÄ Q-Ìï®ÏàòÎäî Ï¥àÍ∏∞ ÌñâÎèô \(a\)Í∞Ä Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏßÄÏ†ïÎêúÎã§Îäî Ï†êÏù¥Îã§. Ïù¥Îäî ÌäπÏ†ï ÏÉÅÌÉúÏóêÏÑú ÌäπÏ†ï ÌñâÎèôÏùÑ Ï∑®ÌñàÏùÑ ÎïåÏùò Í∞ÄÏπòÎ•º ÌèâÍ∞ÄÌï† Ïàò ÏûàÍ≤å Ìï¥Ï§ÄÎã§.<br />

ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V_\pi(s)\)ÏôÄ ÌñâÎèô Í∞ÄÏπò Ìï®Ïàò \(Q_\pi(s,a)\) ÏÇ¨Ïù¥ÏóêÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ Ï§ëÏöîÌïú Í¥ÄÍ≥ÑÍ∞Ä ÏûàÎã§:<br />

\[
V_\pi(s)=\sum_a\pi(a\mid s)Q_\pi(s,a)
\]<br />

Ïù¥ ÏàòÏãùÏùÄ ÏÉÅÌÉú \(s\)Ïùò Í∞ÄÏπòÍ∞Ä Ìï¥Îãπ ÏÉÅÌÉúÏóêÏÑú Ï†ïÏ±Ö \(\pi\)Ïóê Îî∞Îùº Ï∑®Ìï† Ïàò ÏûàÎäî Í∞Å ÌñâÎèôÏùò Í∞ÄÏπòÎ•º Ìï¥Îãπ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï† ÌôïÎ•†Î°ú Í∞ÄÏ§ë ÌèâÍ∑†Ìïú Í≤ÉÏûÑÏùÑ Î≥¥Ïó¨Ï§ÄÎã§. Îã§Ïãú ÎßêÌï¥, \(V_\pi(s)\)Îäî Ï†ïÏ±Ö \(\pi\)Ïóê Îî∞Î•∏ \(Q_\pi(s,a)\)Ïùò Í∏∞ÎåÄÍ∞íÏù¥Îã§.<br />

Q-Ìï®ÏàòÎäî Í∞ïÌôîÌïôÏäµÏóêÏÑú Ïó¨Îü¨ Í∞ÄÏßÄ Ï§ëÏöîÌïú Ïù¥Ïú†Î°ú ÌäπÌûà Ïú†Ïö©ÌïòÎã§. Ï≤´Ïß∏, Q-Ìï®ÏàòÎäî ÏßÅÏ†ëÏ†ÅÏúºÎ°ú ÌñâÎèô ÏÑ†ÌÉùÏóê ÏÇ¨Ïö©Îê† Ïàò ÏûàÎã§. ÌäπÏ†ï ÏÉÅÌÉúÏóêÏÑú Í∞ÄÎä•Ìïú Î™®Îì† ÌñâÎèôÏùò Q-Í∞íÏùÑ ÎπÑÍµêÌï®ÏúºÎ°úÏç®, ÏóêÏù¥Ï†ÑÌä∏Îäî Í∞ÄÏû• Í∞ÄÏπò ÏûàÎäî ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï† Ïàò ÏûàÎã§. Ïù¥Îäî ÏµúÏ†Å Ï†ïÏ±Ö \(\pi^*\)Î•º Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÌï† Ïàò ÏûàÍ≤å ÌïúÎã§: \(\pi^*(s) = \arg\max_a Q^*(s,a)\).<br />

ÎëòÏß∏, Q-Ìï®ÏàòÎäî Î™®Îç∏ ÏóÜÎäî(model-free) ÌïôÏäµÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïúÎã§. ÌôòÍ≤ΩÏùò Ï†ÑÏù¥ ÌôïÎ•†Ïù¥ÎÇò Î≥¥ÏÉÅ Ìï®ÏàòÎ•º Î™®Î•¥ÎçîÎùºÎèÑ, Í≤ΩÌóòÏùÑ ÌÜµÌï¥ ÏßÅÏ†ë Q-Í∞íÏùÑ Ï∂îÏ†ïÌï† Ïàò ÏûàÎã§. Ïù¥Îäî Ïã§Ï†ú ÏÑ∏Í≥ÑÏùò Î≥µÏû°Ìïú Î¨∏Ï†úÏóêÏÑú ÌäπÌûà Ï§ëÏöîÌïúÎç∞, Ïù¥Îü¨Ìïú ÌôòÍ≤ΩÏóêÏÑúÎäî Ï†ïÌôïÌïú Î™®Îç∏ÏùÑ Íµ¨ÌïòÍ∏∞ Ïñ¥Î†µÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§.<br />

Ïã§Ï†ú ÏÇ¨Î°ÄÎ°ú, Q-Ìï®ÏàòÏùò Ïú†Ïö©ÏÑ±ÏùÑ Î≥¥Ïó¨Ï£ºÎäî ÎåÄÌëúÏ†ÅÏù∏ ÏïåÍ≥†Î¶¨Ï¶òÏù¥ Q-Îü¨Îãù(Q-learning)Ïù¥Îã§. ÏûêÏú®Ï£ºÌñâÏ∞® ÏãúÎÇòÎ¶¨Ïò§Î•º Í≥†Î†§Ìï¥Î≥¥Ïûê. Ï∞®ÎüâÏùÄ ÌòÑÏû¨ ÎèÑÎ°ú ÏÉÅÌô©(ÏÉÅÌÉú \(s\))ÏóêÏÑú Í∞ÄÏÜç, Í∞êÏÜç, Ï¢åÌöåÏ†Ñ, Ïö∞ÌöåÏ†Ñ Îì±Ïùò ÌñâÎèô(ÌñâÎèô \(a\))ÏùÑ ÏÑ†ÌÉùÌï† Ïàò ÏûàÎã§. Q-Îü¨ÎãùÏùÑ ÌÜµÌï¥ Ï∞®ÎüâÏùÄ Í∞Å ÏÉÅÌô©ÏóêÏÑú Í∞Å ÌñâÎèôÏùò Q-Í∞íÏùÑ ÌïôÏäµÌïúÎã§. ÏòàÎ•º Îì§Ïñ¥, ÏïûÏóê Î≥¥ÌñâÏûêÍ∞Ä ÏûàÎäî ÏÉÅÌô©ÏóêÏÑú Í∞êÏÜçÌïòÎäî ÌñâÎèôÏùÄ ÎÜíÏùÄ Q-Í∞íÏùÑ Í∞ÄÏßà Í≤ÉÏù¥Í≥†, Í∞ÄÏÜçÌïòÎäî ÌñâÎèôÏùÄ ÎÇÆÏùÄ Q-Í∞íÏùÑ Í∞ÄÏßà Í≤ÉÏù¥Îã§. ÌïôÏäµÏù¥ ÏôÑÎ£åÎêòÎ©¥, Ï∞®ÎüâÏùÄ Í∞Å ÏÉÅÌô©ÏóêÏÑú ÏµúÍ≥†Ïùò Q-Í∞íÏùÑ Í∞ÄÏßÑ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï®ÏúºÎ°úÏç® ÏïàÏ†ÑÌïòÍ≥† Ìö®Ïú®Ï†ÅÏù∏ Ï£ºÌñâÏùÑ Ìï† Ïàò ÏûàÎã§.<br />

Îòê Îã§Î•∏ ÏòàÎ°ú, ÎπÑÎîîÏò§ Í≤åÏûÑÏóêÏÑúÏùò Í∞ïÌôîÌïôÏäµ ÏóêÏù¥Ï†ÑÌä∏Î•º ÏÉùÍ∞ÅÌï¥Î≥º Ïàò ÏûàÎã§. DQN(Deep Q-Network)Í≥º Í∞ôÏùÄ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ Îî•Îü¨ÎãùÍ≥º Q-Ìï®ÏàòÎ•º Í≤∞Ìï©ÌïòÏó¨ ÌîΩÏÖÄ Îç∞Ïù¥ÌÑ∞Î°úÎ∂ÄÌÑ∞ ÏßÅÏ†ë Í≤åÏûÑ ÌîåÎ†àÏù¥Î•º ÌïôÏäµÌïúÎã§. ÏóêÏù¥Ï†ÑÌä∏Îäî Í≤åÏûÑ ÌôîÎ©¥(ÏÉÅÌÉú)ÏóêÏÑú Ïó¨Îü¨ ÌñâÎèô(Î≤ÑÌäº ÏûÖÎ†•)Ïùò Q-Í∞íÏùÑ ÏòàÏ∏°ÌïòÍ≥†, Í∞ÄÏû• ÎÜíÏùÄ Q-Í∞íÏùÑ Í∞ÄÏßÑ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïúÎã§. Ïù¥Î•º ÌÜµÌï¥ Î™®Îç∏ Ï†ïÎ≥¥ ÏóÜÏù¥ÎèÑ ÏïÑÌÉÄÎ¶¨ÎÇò Ïä§ÌÉÄÌÅ¨ÎûòÌîÑÌä∏ÏôÄ Í∞ôÏùÄ Î≥µÏû°Ìïú Í≤åÏûÑÏóêÏÑú Ïù∏Í∞Ñ ÏàòÏ§Ä Ïù¥ÏÉÅÏùò ÏÑ±Îä•ÏùÑ Îã¨ÏÑ±Ìï† Ïàò ÏûàÎã§.<br />

ÏöîÏïΩÌïòÎ©¥, Q-Ìï®ÏàòÎäî ÏÉÅÌÉú Í∞ÄÏπò Ìï®ÏàòÎ≥¥Îã§ Îçî ÏÑ∏Î∂ÑÌôîÎêú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÍ≥†, Î™®Îç∏ ÏóÜÎäî ÌïôÏäµÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïòÎ©∞, ÏßÅÏ†ëÏ†ÅÏù∏ ÌñâÎèô ÏÑ†ÌÉùÏóê ÏÇ¨Ïö©Îê† Ïàò ÏûàÏñ¥ Í∞ïÌôîÌïôÏäµÏóêÏÑú ÌïµÏã¨Ï†ÅÏù∏ Ïó≠Ìï†ÏùÑ ÌïúÎã§.
</details>
</details>

<details>
<summary>9. Bellman Í∏∞ÎåÄ Î∞©Ï†ïÏãùÏùÑ return Ï†ïÏùòÏóêÏÑú Ï∂úÎ∞úÌïòÏó¨ Îã®Í≥ÑÎ≥ÑÎ°ú Ïú†ÎèÑÌïòÏãúÏò§.</summary>

Bellman Í∏∞ÎåÄ Î∞©Ï†ïÏãù  
\[
V_\pi(s)=\sum_a\pi(a\mid s)\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma V_\pi(s')]
\]  
Î•º return Ï†ïÏùòÏóêÏÑú Ï∂úÎ∞úÌïòÏó¨ Îã®Í≥ÑÎ≥ÑÎ°ú Ïú†ÎèÑ Í≥ºÏ†ïÏùÑ ÏÉÅÏÑ∏Ìûà ÏÑúÏà†ÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Bellman Í∏∞ÎåÄ Î∞©Ï†ïÏãùÏùÄ Í∞ïÌôîÌïôÏäµÏùò Ïù¥Î°†Ï†Å Í∏∞Î∞òÏùÑ ÌòïÏÑ±ÌïòÎäî ÌïµÏã¨ Î∞©Ï†ïÏãùÏúºÎ°ú, ÏÉÅÌÉú Í∞ÄÏπò Ìï®ÏàòÏùò Ïû¨Í∑ÄÏ†Å ÌäπÏÑ±ÏùÑ ÌëúÌòÑÌïúÎã§. Ïù¥ Î∞©Ï†ïÏãùÏùÑ Î¶¨ÌÑ¥(return) Ï†ïÏùòÏóêÏÑú ÏãúÏûëÌïòÏó¨ Îã®Í≥ÑÎ≥ÑÎ°ú Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú Ïú†ÎèÑÌï¥ Î≥¥Ïûê.<br />

Î®ºÏ†Ä, ÏãúÍ∞Ñ \(t\)ÏóêÏÑúÏùò Î¶¨ÌÑ¥ \(G_t\)Îäî ÎØ∏ÎûòÏùò Î™®Îì† Ìï†Ïù∏Îêú Î≥¥ÏÉÅÏùò Ìï©ÏúºÎ°ú Ï†ïÏùòÎêúÎã§:<br />
\[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]<br />

Ïù¥ Î¶¨ÌÑ¥ÏùÑ ÌòÑÏû¨ Î≥¥ÏÉÅ \(R_{t+1}\)Í≥º ÎØ∏Îûò Î≥¥ÏÉÅÏùò Ìï©ÏúºÎ°ú Î∂ÑÎ¶¨Ìï† Ïàò ÏûàÎã§:<br />
\[
G_t = R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+2}
\]<br />

Ïó¨Í∏∞ÏÑú Îëê Î≤àÏß∏ Ìï≠ÏùÄ \(G_{t+1}\)Ïùò Ï†ïÏùòÏôÄ ÏùºÏπòÌïúÎã§:<br />
\[
G_t = R_{t+1} + \gamma G_{t+1}
\]<br />

Ïù¥Ï†ú ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V_\pi(s)\)Îäî ÏÉÅÌÉú \(s\)ÏóêÏÑú ÏãúÏûëÌïòÏó¨ Ï†ïÏ±Ö \(\pi\)Î•º Îî∞Î•º Îïå Í∏∞ÎåÄÎêòÎäî Î¶¨ÌÑ¥ÏúºÎ°ú Ï†ïÏùòÎêúÎã§:<br />
\[
V_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
\]<br />

ÏúÑÏùò Î¶¨ÌÑ¥ Î∂ÑÌï¥Î•º Ïù¥Ïö©ÌïòÏó¨ Í∞ÄÏπò Ìï®ÏàòÎ•º Îã§Ïãú ÏûëÏÑ±Ìï† Ïàò ÏûàÎã§:<br />
\[
V_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]
\]<br />

Í∏∞ÎåÄÍ∞íÏùò ÏÑ†ÌòïÏÑ±ÏùÑ Ïù¥Ïö©ÌïòÎ©¥ Îã§ÏùåÍ≥º Í∞ôÏù¥ Î∂ÑÎ¶¨Ìï† Ïàò ÏûàÎã§:<br />
\[
V_\pi(s) = \mathbb{E}_\pi[R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
\]<br />

Îëê Î≤àÏß∏ Ìï≠ÏóêÏÑú \(G_{t+1}\)Ïùò Í∏∞ÎåÄÍ∞íÏùÄ \(S_{t+1}\)Ïùò Í∞ÄÏπò Ìï®ÏàòÏôÄ ÎèôÏùºÌïòÎã§. Í∑∏Îü¨ÎÇò ÌòÑÏû¨Îäî \(S_{t+1}\)Ïù¥ Ïñ¥Îñ§ Í∞íÏù¥ Îê†ÏßÄ Ïïå Ïàò ÏóÜÏúºÎØÄÎ°ú, Í∞ÄÎä•Ìïú Î™®Îì† Îã§Ïùå ÏÉÅÌÉúÏóê ÎåÄÌï¥ Ï°∞Í±¥Î∂Ä Í∏∞ÎåÄÍ∞íÏùÑ Í≥ÑÏÇ∞Ìï¥Ïïº ÌïúÎã§. Ï°∞Í±¥Î∂Ä Í∏∞ÎåÄÍ∞íÏùò Î≤ïÏπôÏùÑ Ï†ÅÏö©ÌïòÎ©¥:<br />
\[
\mathbb{E}_\pi[G_{t+1} | S_t = s] = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a, S_{t+1} = s']
\]<br />

ÎßàÎ•¥ÏΩîÌîÑ ÏÜçÏÑ±ÏúºÎ°ú Ïù∏Ìï¥, \(G_{t+1}\)ÏùÄ \(S_t\)ÏôÄ \(A_t\)Í∞Ä Ï£ºÏñ¥Ï°åÏùÑ Îïå Ïò§ÏßÅ \(S_{t+1}\)ÏóêÎßå ÏùòÏ°¥ÌïúÎã§:<br />
\[
\mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a, S_{t+1} = s'] = \mathbb{E}_\pi[G_{t+1} | S_{t+1} = s'] = V_\pi(s')
\]<br />

Îî∞ÎùºÏÑú:<br />
\[
\mathbb{E}_\pi[G_{t+1} | S_t = s] = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) V_\pi(s')
\]<br />

Ïú†ÏÇ¨ÌïòÍ≤å, Ï≤´ Î≤àÏß∏ Ìï≠Ïù∏ \(\mathbb{E}_\pi[R_{t+1} | S_t = s]\)ÎèÑ Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ÑÍ∞úÌï† Ïàò ÏûàÎã§:<br />
\[
\mathbb{E}_\pi[R_{t+1} | S_t = s] = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) r(s,a,s')
\]<br />

Îëê Ìï≠ÏùÑ Í≤∞Ìï©ÌïòÎ©¥:<br />
\[
V_\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [r(s,a,s') + \gamma V_\pi(s')]
\]<br />

Ïù¥Í≤ÉÏù¥ Bellman Í∏∞ÎåÄ Î∞©Ï†ïÏãùÏù¥Îã§. Ïù¥ Î∞©Ï†ïÏãùÏùÄ ÏÉÅÌÉú \(s\)Ïùò Í∞ÄÏπòÍ∞Ä ÌòÑÏû¨ Î≥¥ÏÉÅÍ≥º Îã§Ïùå ÏÉÅÌÉúÏùò Ìï†Ïù∏Îêú Í∞ÄÏπòÏùò Í∏∞ÎåÄÍ∞íÏùò Ìï©ÏûÑÏùÑ Î≥¥Ïó¨Ï§ÄÎã§. Ïù¥Îäî Í∞ÄÏπò Ìï®ÏàòÏùò Ïû¨Í∑ÄÏ†Å ÌäπÏÑ±ÏùÑ Ìè¨Ï∞©ÌïòÎ©∞, ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Í∏∞Î≤ïÏùÑ ÌÜµÌï¥ Í∞ÄÏπò Ìï®ÏàòÎ•º Í≥ÑÏÇ∞ÌïòÎäî Í∏∞Î∞òÏù¥ ÎêúÎã§.<br />

Bellman Í∏∞ÎåÄ Î∞©Ï†ïÏãùÏùÄ Ï†ïÏ±Ö ÌèâÍ∞Ä, Ï†ïÏ±Ö Î∞òÎ≥µ, Í∑∏Î¶¨Í≥† Í∞ÄÏπò Î∞òÎ≥µÍ≥º Í∞ôÏùÄ ÎßéÏùÄ Í∞ïÌôîÌïôÏäµ ÏïåÍ≥†Î¶¨Ï¶òÏùò Í∏∞Ï¥àÍ∞Ä ÎêúÎã§. Ïù¥ Î∞©Ï†ïÏãùÏùÑ ÌÜµÌï¥ Î≥µÏû°Ìïú ÏàúÏ∞®Ï†Å Í≤∞Ï†ï Î¨∏Ï†úÎ•º Ïû¨Í∑ÄÏ†Å ÌïòÏúÑ Î¨∏Ï†úÎ°ú Î∂ÑÌï¥ÌïòÏó¨ Ìö®Ïú®Ï†ÅÏúºÎ°ú Ìï¥Í≤∞Ìï† Ïàò ÏûàÎã§.
</details>
</details>

<details>
<summary>10. Bellman ÏµúÏ†Å Î∞©Ï†ïÏãùÏùò ÏùòÎØ∏ÏôÄ ÌäπÏÑ±ÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

ÏµúÏ†Å ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V^*(s)\)Îäî  
\[
V^*(s)=\max_a\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma V^*(s')]
\]
Î°ú Ï†ïÏùòÎêòÍ≥†,  
ÏµúÏ†Å ÌñâÎèô Í∞ÄÏπò Ìï®Ïàò \(Q^*(s,a)\)Îäî  
\[
Q^*(s,a)=\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma\max_{a'}Q^*(s',a')]
\]
Î°ú ÌëúÌòÑÎêúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Bellman ÏµúÏ†Å Î∞©Ï†ïÏãùÏùÄ Í∞ïÌôîÌïôÏäµÏóêÏÑú ÏµúÏ†Å Í∞ÄÏπò Ìï®ÏàòÏôÄ ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Ï†ïÏùòÌïòÎäî Ï§ëÏöîÌïú ÏàòÌïôÏ†Å ÌëúÌòÑÏù¥Îã§. Ïù¥ Î∞©Ï†ïÏãùÏùÄ ÏµúÏ†Å ÏùòÏÇ¨Í≤∞Ï†ï ÏõêÏπôÏùÑ Î∞òÏòÅÌïòÎ©∞, ÏàúÏ∞®Ï†Å Í≤∞Ï†ï Î¨∏Ï†úÏóêÏÑú Ïû•Í∏∞Ï†Å Î≥¥ÏÉÅÏùÑ ÏµúÎåÄÌôîÌïòÎäî Î∞©Î≤ïÏùÑ ÏÑ§Î™ÖÌïúÎã§.<br />

ÏµúÏ†Å ÏÉÅÌÉú Í∞ÄÏπò Ìï®Ïàò \(V^*(s)\)Îäî ÏÉÅÌÉú \(s\)ÏóêÏÑú ÏãúÏûëÌïòÏó¨ ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Îî∞Î•º Îïå ÏñªÏùÑ Ïàò ÏûàÎäî ÏµúÎåÄ Í∏∞ÎåÄ Î¶¨ÌÑ¥ÏúºÎ°ú Ï†ïÏùòÎêúÎã§. Bellman ÏµúÏ†Å Î∞©Ï†ïÏãùÏóê Îî∞Î•¥Î©¥ Ïù¥Îäî Îã§ÏùåÍ≥º Í∞ôÏù¥ ÌëúÌòÑÎêúÎã§:<br />

\[
V^*(s)=\max_a\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma V^*(s')]
\]<br />

Ïù¥ Î∞©Ï†ïÏãùÏùÄ ÌòÑÏû¨ ÏÉÅÌÉú \(s\)ÏóêÏÑú ÏµúÏ†ÅÏùò Í∞ÄÏπòÎ•º ÏñªÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Í∞ÄÎä•Ìïú Î™®Îì† ÌñâÎèô Ï§ëÏóêÏÑú ÌòÑÏû¨ Î≥¥ÏÉÅÍ≥º Îã§Ïùå ÏÉÅÌÉúÏùò Ìï†Ïù∏Îêú ÏµúÏ†Å Í∞ÄÏπòÏùò Ìï©ÏùÑ ÏµúÎåÄÌôîÌïòÎäî ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï¥Ïïº Ìï®ÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. Ï¶â, ÏµúÏ†Å Í∞ÄÏπò Ìï®ÏàòÎäî Í∞Å ÏÉÅÌÉúÏóêÏÑú 'ÏµúÏÑ†Ïùò ÌñâÎèô'ÏùÑ ÏÑ†ÌÉùÌïòÎäî Í≤ÉÏùÑ Í∞ÄÏ†ïÌïúÎã§.<br />

Ïú†ÏÇ¨ÌïòÍ≤å, ÏµúÏ†Å ÌñâÎèô Í∞ÄÏπò Ìï®Ïàò \(Q^*(s,a)\)Îäî ÏÉÅÌÉú \(s\)ÏóêÏÑú ÌñâÎèô \(a\)Î•º Ï∑®Ìïú ÌõÑ ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Îî∞Î•º Îïå ÏñªÏùÑ Ïàò ÏûàÎäî ÏµúÎåÄ Í∏∞ÎåÄ Î¶¨ÌÑ¥ÏúºÎ°ú Ï†ïÏùòÎêúÎã§:<br />

\[
Q^*(s,a)=\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma\max_{a'}Q^*(s',a')]
\]<br />

Ïù¥ Î∞©Ï†ïÏãùÏóêÏÑú Ï§ëÏöîÌïú Ï†êÏùÄ ÌòÑÏû¨ ÏÉÅÌÉú-ÌñâÎèô Ïåç \((s,a)\)Ïùò ÏµúÏ†Å Í∞ÄÏπòÍ∞Ä Îã§Ïùå ÏÉÅÌÉú \(s'\)ÏóêÏÑú ÏµúÏ†Å ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïúÎã§Îäî Í∞ÄÏ†ï ÌïòÏóê Í≥ÑÏÇ∞ÎêúÎã§Îäî Í≤ÉÏù¥Îã§. Ï¶â, \(\max_{a'}Q^*(s',a')\)Îäî Îã§Ïùå ÏÉÅÌÉúÏóêÏÑú Í∞ÄÎä•Ìïú Î™®Îì† ÌñâÎèô Ï§ë ÏµúÎåÄ Q-Í∞íÏùÑ Í∞ÄÏßÑ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï®ÏùÑ ÏùòÎØ∏ÌïúÎã§.<br />

Bellman ÏµúÏ†Å Î∞©Ï†ïÏãùÏùò Î™á Í∞ÄÏßÄ Ï§ëÏöîÌïú ÌäπÏÑ±Í≥º ÏùòÎØ∏Î•º ÏÇ¥Ìé¥Î≥¥Ïûê:<br />

Ï≤´Ïß∏, Ïù¥ Î∞©Ï†ïÏãùÎì§ÏùÄ ÏµúÏ†Å Í∞ÄÏπò Ìï®ÏàòÏùò Ïû¨Í∑ÄÏ†Å ÌäπÏÑ±ÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. ÏµúÏ†Å Í∞ÄÏπò Ìï®ÏàòÎäî ÏûêÍ∏∞ ÏûêÏã†ÏùÑ Ï∞∏Ï°∞ÌïòÎäî Î∞©Ï†ïÏãùÏúºÎ°ú ÌëúÌòÑÎêòÎ©∞, Ïù¥Îäî ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Î∞©Î≤ïÏùÑ ÌÜµÌï¥ Ïù¥Î•º Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÍ≤å Ìï¥Ï§ÄÎã§.<br />

ÎëòÏß∏, ÏµúÏ†Å Í∞ÄÏπò Ìï®ÏàòÎäî Ïú†ÏùºÌïòÍ≤å Ï°¥Ïû¨ÌïúÎã§. Bellman ÏµúÏ†Å Î∞©Ï†ïÏãùÏùÄ ÏàòÏ∂ï Îß§Ìïë(contraction mapping)Ïùò ÌäπÏÑ±ÏùÑ Í∞ÄÏßÄÎ©∞, Ïù¥Îäî Î∞òÎ≥µÏ†ÅÏù∏ Í≥ÑÏÇ∞ÏùÑ ÌÜµÌï¥ Í≤∞Íµ≠ Ïú†ÏùºÌïú Ìï¥Ïóê ÏàòÎ†¥Ìï®ÏùÑ Î≥¥Ïû•ÌïúÎã§.<br />

ÏÖãÏß∏, ÏµúÏ†Å Ï†ïÏ±ÖÏùÄ ÏµúÏ†Å Í∞ÄÏπò Ìï®ÏàòÎ°úÎ∂ÄÌÑ∞ ÏßÅÏ†ë Ïú†ÎèÑÎê† Ïàò ÏûàÎã§. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú, ÏµúÏ†Å Ï†ïÏ±Ö \(\pi^*(s)\)Îäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÎêúÎã§:<br />

\[
\pi^*(s) = \arg\max_a Q^*(s,a)
\]<br />

ÎòêÎäî ÌôòÍ≤Ω Î™®Îç∏Ïù¥ ÏûàÎäî Í≤ΩÏö∞:<br />

\[
\pi^*(s) = \arg\max_a \sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma V^*(s')]
\]<br />

ÎÑ∑Ïß∏, Ïù¥ Î∞©Ï†ïÏãùÏùÄ ÏµúÏ†ÅÏÑ± ÏõêÏπô(principle of optimality)ÏùÑ Î∞òÏòÅÌïúÎã§. Ïù¥ ÏõêÏπôÏóê Îî∞Î•¥Î©¥, ÏµúÏ†Å Ï†ïÏ±ÖÏùò ÏùºÎ∂ÄÎ∂ÑÏùÄ Í∑∏ ÏûêÏ≤¥Î°úÎèÑ ÏµúÏ†Å Ï†ïÏ±ÖÏù¥Îã§. Ï¶â, ÌòÑÏû¨ ÏÉÅÌÉúÏóêÏÑú ÏµúÏ†Å ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÍ≥†, Í∑∏ Í≤∞Í≥ºÎ°ú ÎèÑÎã¨Ìïú Î™®Îì† ÏÉÅÌÉúÏóêÏÑúÎèÑ Í≥ÑÏÜçÌï¥ÏÑú ÏµúÏ†ÅÏúºÎ°ú ÌñâÎèôÌïúÎã§Î©¥, Ï†ÑÏ≤¥ Ï†ïÏ±ÖÏùÄ ÏµúÏ†ÅÏù¥Îã§.<br />

Îã§ÏÑØÏß∏, Bellman ÏµúÏ†Å Î∞©Ï†ïÏãùÏùÄ Í∞ÄÏπò Î∞òÎ≥µ, Q-Îü¨Îãù, SARSAÏôÄ Í∞ôÏùÄ ÎßéÏùÄ Í∞ïÌôîÌïôÏäµ ÏïåÍ≥†Î¶¨Ï¶òÏùò Ïù¥Î°†Ï†Å Í∏∞Î∞òÏùÑ Ï†úÍ≥µÌïúÎã§. Ïù¥Îü¨Ìïú ÏïåÍ≥†Î¶¨Ï¶òÎì§ÏùÄ ÏßÅÍ∞ÑÏ†ëÏ†ÅÏúºÎ°ú Ïù¥ Î∞©Ï†ïÏãùÏùÑ Í∑ºÏÇ¨ÌïòÍ±∞ÎÇò Ìï¥Í≤∞ÌïòÎäî Î∞©Î≤ïÏùÑ Íµ¨ÌòÑÌïúÎã§.<br />

Ïã§Ï†ú ÏùëÏö©ÏóêÏÑú Bellman ÏµúÏ†Å Î∞©Ï†ïÏãùÏùò Ï£ºÏöî Í≥ºÏ†úÎäî ÎåÄÍ∑úÎ™® ÏÉÅÌÉú Í≥µÍ∞ÑÏóêÏÑúÏùò Í≥ÑÏÇ∞ Î≥µÏû°ÏÑ±Ïù¥Îã§. Ïù¥Î•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ Ìï®Ïàò Í∑ºÏÇ¨, Í≤ΩÌóò Ïû¨ÏÉù, ÌÉÄÍ≤ü ÎÑ§Ìä∏ÏõåÌÅ¨ÏôÄ Í∞ôÏùÄ Îã§ÏñëÌïú Í∏∞Î≤ïÏù¥ Í∞úÎ∞úÎêòÏóàÏúºÎ©∞, Ïù¥Îü¨Ìïú Í∏∞Î≤ïÎì§ÏùÄ Î≥µÏû°Ìïú Ïã§Ï†ú Î¨∏Ï†úÏóêÏÑú Bellman ÏµúÏ†Å Î∞©Ï†ïÏãùÏùò ÏõêÎ¶¨Î•º Ìö®Í≥ºÏ†ÅÏúºÎ°ú Ï†ÅÏö©Ìï† Ïàò ÏûàÍ≤å Ìï¥Ï§ÄÎã§.
</details>
</details>

<details>
<summary>11. Î∞òÎ≥µÏ†Å Ï†ïÏ±Ö ÌèâÍ∞Ä ÏïåÍ≥†Î¶¨Ï¶òÏùò ÎèôÏûë ÏõêÎ¶¨ÏôÄ ÏàòÎ†¥ ÌäπÏÑ±ÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

```pseudo
Initialize V(s) arbitrarily for all s
Repeat:
  Œî=0
  For each state s:
    v=V(s)
    V(s)=Œ£_aœÄ(a|s)Œ£_{s'}P(s'|s,a)[r(s,a,s')+Œ≥V(s')]
    Œî=max(Œî,|v‚àíV(s)|)
Until Œî&lt;Œ∏
```
Ïù¥ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ \(Œ≥&lt;1\) Î∞è Ïú†Ìïú ÏÉÅÌÉúÍ≥µÍ∞ÑÏóêÏÑú ÏàòÎ†¥ÏùÑ Î≥¥Ïû•ÌïúÎã§. Ìïú Ïä§ÏúïÎãπ Í≥ÑÏÇ∞ Î≥µÏû°ÎèÑÎäî \(O(|S|‚ãÖ|A|‚ãÖ|S|)\)Ïù¥Î©∞, \(Œ≥\)ÏôÄ Ï¥àÍ∏∞Í∞íÏù¥ ÌÅ¥ÏàòÎ°ù ÏàòÎ†¥ ÏÜçÎèÑÍ∞Ä ÎäêÎ†§ÏßÑÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Î∞òÎ≥µÏ†Å Ï†ïÏ±Ö ÌèâÍ∞Ä ÏïåÍ≥†Î¶¨Ï¶òÏùÄ Ï£ºÏñ¥ÏßÑ Ï†ïÏ±Ö œÄÏóê ÎåÄÌïú ÏÉÅÌÉú Í∞ÄÏπò Ìï®ÏàòÎ•º Í≥ÑÏÇ∞ÌïòÎäî ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Î∞©Î≤ïÏù¥Îã§. Ïù¥ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ Î≤®Îßå Í∏∞ÎåÄ Î∞©Ï†ïÏãùÏùÑ Î∞òÎ≥µÏ†ÅÏúºÎ°ú Ï†ÅÏö©ÌïòÏó¨ Î™®Îì† ÏÉÅÌÉúÏùò Í∞ÄÏπòÎ•º Ï†êÏßÑÏ†ÅÏúºÎ°ú Í∞úÏÑ†ÌïúÎã§.<br />

ÏïåÍ≥†Î¶¨Ï¶òÏùò ÌïµÏã¨ÏùÄ Í∞Å Î∞òÎ≥µÏóêÏÑú Î™®Îì† ÏÉÅÌÉúÏóê ÎåÄÌï¥ Î≤®Îßå ÏóÖÎç∞Ïù¥Ìä∏Î•º ÏàòÌñâÌïòÎäî Í≤ÉÏù¥Îã§. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú, Í∞Å ÏÉÅÌÉú sÏóêÏÑú ÌòÑÏû¨ Ï†ïÏ±Ö œÄÎ•º Îî∞Îùº ÏÑ†ÌÉù Í∞ÄÎä•Ìïú Î™®Îì† ÌñâÎèôÏùò Í∏∞ÎåÄÍ∞íÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥†, Ïù¥Î•º ÌÜµÌï¥ ÎèÑÎã¨Ìï† Ïàò ÏûàÎäî Î™®Îì† Îã§Ïùå ÏÉÅÌÉúÏùò Í∞ÄÏπòÎ•º Í≥†Î†§ÌïúÎã§. Ïù¥ Í≥ºÏ†ïÏóêÏÑú Ï†ÑÏù¥ ÌôïÎ•†Í≥º Î≥¥ÏÉÅ, Í∑∏Î¶¨Í≥† Îã§Ïùå ÏÉÅÌÉúÏùò ÌòÑÏû¨ Ï∂îÏ†ï Í∞ÄÏπòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏÉÅÌÉú sÏùò ÏÉàÎ°úÏö¥ Í∞ÄÏπòÎ•º Í≥ÑÏÇ∞ÌïúÎã§.<br />

ÏàòÎ†¥ ÌäπÏÑ±Ïóê ÏûàÏñ¥ÏÑú, Ìï†Ïù∏Ïú® Œ≥Í∞Ä 1Î≥¥Îã§ ÏûëÍ≥† ÏÉÅÌÉú Í≥µÍ∞ÑÏù¥ Ïú†ÌïúÌïòÎã§Î©¥, Î∞òÎ≥µÏ†Å Ï†ïÏ±Ö ÌèâÍ∞ÄÎäî ÏßÑÏ†ïÌïú Í∞ÄÏπò Ìï®Ïàò VœÄÎ°ú ÏàòÎ†¥Ìï®Ïù¥ ÏàòÌïôÏ†ÅÏúºÎ°ú Ï¶ùÎ™ÖÎêòÏñ¥ ÏûàÎã§. Ïù¥Îäî Î≤®Îßå Ïó∞ÏÇ∞ÏûêÍ∞Ä ÏàòÏ∂ï Îß§Ìïë(contraction mapping)Ïù¥ÎùºÎäî ÏÑ±ÏßàÏóê Í∏∞Ïù∏ÌïúÎã§. Îß§ Î∞òÎ≥µÏóêÏÑú ÏµúÎåÄ Ïò§Ï∞®Îäî Ï†ÅÏñ¥ÎèÑ Í∞êÏÜåÏú® Œ≥ÎßåÌÅº Ï§ÑÏñ¥Îì§Í≤å ÎêúÎã§.<br />

Í≥ÑÏÇ∞ Î≥µÏû°ÎèÑ Ï∏°Î©¥ÏóêÏÑú, Ìïú Î≤àÏùò ÏôÑÏ†ÑÌïú Ïä§Ïúï(Î™®Îì† ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏)ÏùÄ O(|S|¬≤|A|)Ïùò ÏãúÍ∞Ñ Î≥µÏû°ÎèÑÎ•º Í∞ÄÏßÑÎã§. Ïù¥Îäî Í∞Å ÏÉÅÌÉú sÏóê ÎåÄÌï¥ Î™®Îì† Í∞ÄÎä•Ìïú ÌñâÎèô aÏôÄ Îã§Ïùå ÏÉÅÌÉú s'Ïóê ÎåÄÌïú Í≥ÑÏÇ∞Ïù¥ ÌïÑÏöîÌïòÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§. Ïù¥Îü¨Ìïú Í≥ÑÏÇ∞ÎüâÏùÄ ÏÉÅÌÉú Í≥µÍ∞ÑÍ≥º ÌñâÎèô Í≥µÍ∞ÑÏù¥ Ïª§ÏßàÏàòÎ°ù Í∏âÍ≤©Ìûà Ï¶ùÍ∞ÄÌïúÎã§.<br />

ÏàòÎ†¥ ÏÜçÎèÑÎäî Ïó¨Îü¨ ÏöîÏù∏Ïóê ÏùòÌï¥ ÏòÅÌñ•ÏùÑ Î∞õÎäîÎã§. Ìï†Ïù∏Ïú® Œ≥Í∞Ä 1Ïóê Í∞ÄÍπåÏö∏ÏàòÎ°ù Î®º ÎØ∏ÎûòÏùò Î≥¥ÏÉÅÏù¥ ÌòÑÏû¨ Í∞ÄÏπòÏóê Îçî ÌÅ∞ ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎØÄÎ°ú ÏàòÎ†¥Ïù¥ ÎäêÎ†§ÏßÑÎã§. ÎòêÌïú Ï¥àÍ∏∞ Í∞ÄÏπò Ìï®Ïàò Ï∂îÏ†ïÏπòÍ∞Ä Ïã§Ï†ú Í∞íÍ≥º ÌÅ¨Í≤å Îã§Î•º Í≤ΩÏö∞ÏóêÎèÑ ÏàòÎ†¥ ÏÜçÎèÑÍ∞Ä Ï†ÄÌïòÎêúÎã§. Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Ï†ÅÏ†àÌïú Ï¥àÍ∏∞Ìôî Ï†ÑÎûµÍ≥º Ï¢ÖÎ£å Ï°∞Í±¥(Œ∏)ÏùÑ ÏÑ†ÌÉùÌïòÎäî Í≤ÉÏù¥ Ï§ëÏöîÌïòÎã§.
</details>
</details>

<details>
<summary>12. Ï†ïÏ±Ö Î∞òÎ≥µ ÏïåÍ≥†Î¶¨Ï¶òÏùò ÏûëÎèô ÏõêÎ¶¨ÏôÄ ÏàòÎ†¥ ÌäπÏÑ±ÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

1. Ï†ïÏ±Ö ÌèâÍ∞Ä: Î∞òÎ≥µÏ†Å Ï†ïÏ±Ö ÌèâÍ∞ÄÎ°ú V_œÄ(s) ÏàòÎ†¥  
2. Ï†ïÏ±Ö Í∞úÏÑ†:  
\[
œÄ_{new}(s)=\arg\max_a\sum_{s'}P(s'\mid s,a)[r(s,a,s')+Œ≥V_œÄ(s')]
\]  
Ïù¥ Í≥ºÏ†ïÏùÑ Ï†ïÏ±ÖÏù¥ Î≥ÄÌïòÏßÄ ÏïäÏùÑ ÎïåÍπåÏßÄ Î∞òÎ≥µÌï¥ Ïú†Ìïú MDPÏóêÏÑú ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Ï∞æÎäîÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Ï†ïÏ±Ö Î∞òÎ≥µ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Ï∞æÍ∏∞ ÏúÑÌï¥ Ï†ïÏ±Ö ÌèâÍ∞ÄÏôÄ Ï†ïÏ±Ö Í∞úÏÑ†ÏùÑ Î≤àÍ∞àÏïÑ ÏàòÌñâÌïòÎäî ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Î∞©Î≤ïÏù¥Îã§. Ïù¥ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ Îëê ÌïµÏã¨ Îã®Í≥ÑÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏúºÎ©∞, Ïù¥Î•º ÌÜµÌï¥ Ï†êÏßÑÏ†ÅÏúºÎ°ú Îçî ÎÇòÏùÄ Ï†ïÏ±ÖÏùÑ Ï∞æÏïÑÍ∞ÑÎã§.

Ï≤´ Î≤àÏß∏ Îã®Í≥ÑÏù∏ Ï†ïÏ±Ö ÌèâÍ∞ÄÏóêÏÑúÎäî, ÌòÑÏû¨ Ï†ïÏ±Ö œÄÏóê ÎåÄÌïú Í∞ÄÏπò Ìï®Ïàò VœÄÎ•º Í≥ÑÏÇ∞ÌïúÎã§. Ïù¥Îäî Î∞òÎ≥µÏ†Å Ï†ïÏ±Ö ÌèâÍ∞Ä ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î≤®Îßå Í∏∞ÎåÄ Î∞©Ï†ïÏãùÏù¥ ÏàòÎ†¥Ìï† ÎïåÍπåÏßÄ Î∞òÎ≥µÏ†ÅÏúºÎ°ú Ï†ÅÏö©Ìï®ÏúºÎ°úÏç® Îã¨ÏÑ±ÎêúÎã§. Ïù¥ Í≥ºÏ†ïÏùÑ ÌÜµÌï¥ ÌòÑÏû¨ Ï†ïÏ±ÖÏù¥ ÏñºÎßàÎÇò Ï¢ãÏùÄÏßÄÎ•º Ï†ïÌôïÌûà ÌèâÍ∞ÄÌï† Ïàò ÏûàÎã§.

Îëê Î≤àÏß∏ Îã®Í≥ÑÏù∏ Ï†ïÏ±Ö Í∞úÏÑ†ÏóêÏÑúÎäî, Í≥ÑÏÇ∞Îêú Í∞ÄÏπò Ìï®ÏàòÎ•º Í∏∞Î∞òÏúºÎ°ú Í∞Å ÏÉÅÌÉúÏóêÏÑú Îçî ÎÇòÏùÄ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÏó¨ Ï†ïÏ±ÖÏùÑ Í∞úÏÑ†ÌïúÎã§. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú, Í∞Å ÏÉÅÌÉú sÏóêÏÑú Î™®Îì† Í∞ÄÎä•Ìïú ÌñâÎèô aÏóê ÎåÄÌï¥, Ìï¥Îãπ ÌñâÎèôÏùÑ Ï∑®Ìïú ÌõÑ ÌòÑÏû¨ Í∞ÄÏπò Ìï®ÏàòÏóê Îî∞Îùº Í∏∞ÎåÄÎêòÎäî Î¶¨ÌÑ¥ÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥†, Í∞ÄÏû• ÌÅ∞ Í∏∞ÎåÄ Î¶¨ÌÑ¥ÏùÑ Ï†úÍ≥µÌïòÎäî ÌñâÎèôÏùÑ ÏÉàÎ°úÏö¥ Ï†ïÏ±ÖÏúºÎ°ú ÏÑ†ÌÉùÌïúÎã§.

Ï†ïÏ±Ö Î∞òÎ≥µÏùò ÏàòÎ†¥ ÌäπÏÑ±ÏùÄ Ï†ïÏ±Ö Í∞úÏÑ† Ï†ïÎ¶¨(Policy Improvement Theorem)Ïóê ÏùòÌï¥ Î≥¥Ïû•ÎêúÎã§. Ïù¥ Ï†ïÎ¶¨Îäî Ï†ïÏ±Ö Í∞úÏÑ† Îã®Í≥ÑÏóêÏÑú ÏÉùÏÑ±Îêú ÏÉàÎ°úÏö¥ Ï†ïÏ±Ö œÄ'Ïù¥ Í∏∞Ï°¥ Ï†ïÏ±Ö œÄÎ≥¥Îã§ Ìï≠ÏÉÅ Í∞ôÍ±∞ÎÇò Îçî ÎÇòÏùÄ Í∞ÄÏπòÎ•º Ï†úÍ≥µÌïúÎã§Îäî Í≤ÉÏùÑ Ï¶ùÎ™ÖÌïúÎã§. ÎòêÌïú, Ïú†ÌïúÌïú MDPÏóêÏÑúÎäî Ï†ïÏ±ÖÏùò ÏàòÍ∞Ä Ïú†ÌïúÌïòÎØÄÎ°ú, Ï†ïÏ±Ö Î∞òÎ≥µÏùÄ Ïú†ÌïúÌïú Î∞òÎ≥µ ÌõÑÏóê ÏµúÏ†Å Ï†ïÏ±ÖÏóê ÎèÑÎã¨Ìï®Ïù¥ Î≥¥Ïû•ÎêúÎã§.

Í≥ÑÏÇ∞ Î≥µÏû°ÎèÑ Ï∏°Î©¥ÏóêÏÑú, Ï†ïÏ±Ö Î∞òÎ≥µÏùò Ï¥ù ÎπÑÏö©ÏùÄ Ï†ïÏ±Ö ÌèâÍ∞Ä Îã®Í≥ÑÏùò Î∞òÎ≥µ ÌöüÏàòÏóê ÌÅ¨Í≤å ÏùòÏ°¥ÌïúÎã§. Í∞Å Ï†ïÏ±Ö ÌèâÍ∞ÄÎäî O(|S|¬≤|A|)Ïùò Î≥µÏû°ÎèÑÎ•º Í∞ÄÏßÄÎ©∞, Ï†ïÏ±Ö Í∞úÏÑ† Îã®Í≥ÑÎèÑ Ïú†ÏÇ¨Ìïú Î≥µÏû°ÎèÑÎ•º Í∞ÄÏßÑÎã§. Ïã§Ï†úÎ°úÎäî Ï†ïÏ±Ö ÌèâÍ∞Ä Îã®Í≥ÑÏóêÏÑú ÏôÑÏ†ÑÌïú ÏàòÎ†¥ÏùÑ Í∏∞Îã§Î¶¨ÏßÄ ÏïäÍ≥†, Í∑ºÏÇ¨Ï†ÅÏúºÎ°ú ÏàòÎ†¥Ìïú ÌõÑ Ï†ïÏ±Ö Í∞úÏÑ†ÏùÑ ÏàòÌñâÌïòÎäî Î≥ÄÌòïÎêú ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í≥ÑÏÇ∞ Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏù¥Í∏∞ÎèÑ ÌïúÎã§.
</details>
</details>

<details>
<summary>13. Í∞ÄÏπò Î∞òÎ≥µ ÏïåÍ≥†Î¶¨Ï¶òÏùò ÏûëÎèô ÏõêÎ¶¨ÏôÄ Policy IterationÍ≥ºÏùò Ï∞®Ïù¥Ï†êÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

```pseudo
Initialize V(s)=0 for all s
Repeat:
  Œî=0
  For each state s:
    v=V(s)
    V(s)=max_aŒ£_{s'}P(s'|s,a)[r(s,a,s')+Œ≥V(s')]
    Œî=max(Œî,|v‚àíV(s)|)
Until Œî&lt;Œ∏
```
Í∞ÄÏπò Î∞òÎ≥µÏùÄ Ï†ïÏ±Ö ÌèâÍ∞Ä ÏóÜÏù¥ Bellman ÏµúÏ†Å ÏóÖÎç∞Ïù¥Ìä∏Î•º Î∞òÎ≥µÌï¥ Policy IterationÎ≥¥Îã§ Îçî Ï†ÅÏùÄ Ïó∞ÏÇ∞ÏúºÎ°ú ÏµúÏ†Å Ìï¥Ïóê ÎèÑÎã¨ÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
Í∞ÄÏπò Î∞òÎ≥µ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ ÏµúÏ†Å Í∞ÄÏπò Ìï®ÏàòÎ•º ÏßÅÏ†ë Í≥ÑÏÇ∞ÌïòÎäî ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Î∞©Î≤ïÏúºÎ°ú, Î≤®Îßå ÏµúÏ†ÅÏÑ± Î∞©Ï†ïÏãùÏùÑ Î∞òÎ≥µÏ†ÅÏúºÎ°ú Ï†ÅÏö©ÌïòÏó¨ ÏÉÅÌÉú Í∞ÄÏπòÎ•º Í∞±Ïã†ÌïúÎã§. Ïù¥ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ Ï†ïÏ±ÖÏùÑ Î™ÖÏãúÏ†ÅÏúºÎ°ú Ïú†ÏßÄÌïòÏßÄ ÏïäÍ≥†, ÎåÄÏã† Í∞Å ÏÉÅÌÉúÏùò ÏµúÏ†Å Í∞ÄÏπòÎ•º ÏßÅÏ†ë Í≥ÑÏÇ∞ÌïúÎã§.

Í∞ÄÏπò Î∞òÎ≥µÏùò ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥Îäî Í∞Å ÏÉÅÌÉúÏóêÏÑú Î™®Îì† Í∞ÄÎä•Ìïú ÌñâÎèôÏóê ÎåÄÌïú Í∏∞ÎåÄ ÏàòÏùµÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥†, Í∑∏ Ï§ë ÏµúÎåÄÍ∞íÏùÑ Ìï¥Îãπ ÏÉÅÌÉúÏùò ÏÉàÎ°úÏö¥ Í∞ÄÏπòÎ°ú ÏÑ§Ï†ïÌïòÎäî Í≤ÉÏù¥Îã§. Ïù¥Îäî Î≤®Îßå ÏµúÏ†ÅÏÑ± Î∞©Ï†ïÏãùÏùÑ ÏßÅÏ†ë Ï†ÅÏö©ÌïòÎäî Í≤ÉÏúºÎ°ú, Í∞Å ÏóÖÎç∞Ïù¥Ìä∏Í∞Ä ÏùºÏ¢ÖÏùò 'ÌÉêÏöïÏ†ÅÏù∏' Ï†ïÏ±Ö Í∞úÏÑ†Í≥º Ï†úÌïúÎêú Ï†ïÏ±Ö ÌèâÍ∞ÄÎ•º ÎèôÏãúÏóê ÏàòÌñâÌïòÎäî Ìö®Í≥ºÍ∞Ä ÏûàÎã§.

Ï†ïÏ±Ö Î∞òÎ≥µÍ≥º Í∞ÄÏπò Î∞òÎ≥µÏùò Ï£ºÏöî Ï∞®Ïù¥Ï†êÏùÄ Ï†ïÏ±Ö ÌèâÍ∞Ä Îã®Í≥ÑÏùò Ï≤òÎ¶¨ Î∞©ÏãùÏóê ÏûàÎã§. Ï†ïÏ±Ö Î∞òÎ≥µÏùÄ Í∞Å Î∞òÎ≥µÏóêÏÑú ÌòÑÏû¨ Ï†ïÏ±ÖÏóê ÎåÄÌïú ÏôÑÏ†ÑÌïú Ï†ïÏ±Ö ÌèâÍ∞ÄÎ•º ÏàòÌñâÌïòÏó¨ Ï†ïÌôïÌïú Í∞ÄÏπò Ìï®ÏàòÎ•º Í≥ÑÏÇ∞Ìïú ÌõÑ Ï†ïÏ±ÖÏùÑ Í∞úÏÑ†ÌïúÎã§. Î∞òÎ©¥, Í∞ÄÏπò Î∞òÎ≥µÏùÄ Ï†ïÏ±Ö ÌèâÍ∞ÄÏôÄ Í∞úÏÑ†ÏùÑ Í∞Å ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ÎßàÎã§ Ï¶âÏãú Í≤∞Ìï©ÌïòÏó¨ ÏàòÌñâÌïúÎã§. Ï¶â, Í∞ÄÏπò Î∞òÎ≥µÏùÄ Ï†ïÏ±Ö ÌèâÍ∞ÄÎ•º Ìïú Î≤àÎßå Î∞òÎ≥µÌïú ÌõÑ Î∞îÎ°ú Í∞úÏÑ† Îã®Í≥ÑÎ°ú ÎÑòÏñ¥Í∞ÄÎäî Í≤ÉÍ≥º Ïú†ÏÇ¨ÌïòÎã§.

Ïù¥Îü¨Ìïú Ï∞®Ïù¥Î°ú Ïù∏Ìï¥ Í∞ÄÏπò Î∞òÎ≥µÏùÄ ÏùºÎ∞òÏ†ÅÏúºÎ°ú Ï†ïÏ±Ö Î∞òÎ≥µÎ≥¥Îã§ Í≥ÑÏÇ∞ Ìö®Ïú®ÏÑ±Ïù¥ ÎÜíÎã§. ÌäπÌûà ÏÉÅÌÉú Í≥µÍ∞ÑÏù¥ ÌÅ¨Í≥† Ï†ïÏ±Ö ÌèâÍ∞ÄÍ∞Ä ÎßéÏùÄ Î∞òÎ≥µÏùÑ ÌïÑÏöîÎ°ú Ìï† Îïå Ïù¥Ï†êÏù¥ ÎëêÎìúÎü¨ÏßÑÎã§. ÎòêÌïú Í∞ÄÏπò Î∞òÎ≥µÏùÄ Ìï†Ïù∏Ïú® Œ≥Í∞Ä 1Ïóê Í∞ÄÍπåÏö∏ ÎïåÎèÑ ÏÉÅÎåÄÏ†ÅÏúºÎ°ú Îçî Ìö®Ïú®Ï†ÅÏù∏ Í≤ΩÌñ•Ïù¥ ÏûàÎã§.

ÏàòÎ†¥ Ï∏°Î©¥ÏóêÏÑú, Í∞ÄÏπò Î∞òÎ≥µÏùÄ Ìï†Ïù∏Ïú®Ïù¥ 1Î≥¥Îã§ ÏûëÍ≥† Ïú†ÌïúÌïú MDPÏóêÏÑú ÏµúÏ†Å Í∞ÄÏπò Ìï®ÏàòÎ°ú ÏàòÎ†¥Ìï®Ïù¥ Î≥¥Ïû•ÎêúÎã§. ÏàòÎ†¥ ÌõÑÏóêÎäî ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ ÏâΩÍ≤å Ï∂îÏ∂úÌï† Ïàò ÏûàÏúºÎ©∞, Ïù¥Îäî Í∞Å ÏÉÅÌÉúÏóêÏÑú ÏµúÎåÄ Í∞ÄÏπòÎ•º Ï†úÍ≥µÌïòÎäî ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÎäî Í≤ÉÏúºÎ°ú Íµ¨ÏÑ±ÎêúÎã§.
</details>
</details>

<details>
<summary>14. ÎπÑÎèôÍ∏∞Ï†Å DPÏùò ÏõêÎ¶¨ÏôÄ Ïû•Ï†êÏùÑ ÏÑ§Î™ÖÌïòÏãúÏò§.</summary>

ÎπÑÎèôÍ∏∞Ï†Å DPÎäî Ï†ÑÏ≤¥ ÏÉÅÌÉú Ïä§Ïúï ÎåÄÏã† ÏûÑÏùò ÏàúÏÑúÎ°ú ÏÉÅÌÉúÎ•º ÏÑ†ÌÉùÌï¥ Ï¶âÏãú V(s)Î•º Í∞±Ïã†ÌïòÎäî Î∞©ÏãùÏù¥Îã§.  
Ïù¥ Î∞©Î≤ïÏùÄ Ï§ëÏöîÎèÑÍ∞Ä ÎÜíÏùÄ ÏÉÅÌÉúÎ•º Ïö∞ÏÑ† Í∞±Ïã†Ìï¥ ÏàòÎ†¥ ÏÜçÎèÑÎ•º ÎÜíÏù∏Îã§.
<details>
<summary>Ï†ïÎãµ</summary>
ÎπÑÎèôÍ∏∞Ï†Å ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç(Asynchronous Dynamic Programming)ÏùÄ Ï†ÑÌÜµÏ†ÅÏù∏ ÎèôÍ∏∞Ï†Å DP Î∞©Î≤ïÍ≥º Îã¨Î¶¨ Î™®Îì† ÏÉÅÌÉúÎ•º ÎèôÏãúÏóê ÏóÖÎç∞Ïù¥Ìä∏ÌïòÏßÄ ÏïäÍ≥†, ÌäπÏ†ï ÏàúÏÑúÎÇò ÏÑ†ÌÉù Í∏∞Ï§ÄÏóê Îî∞Îùº ÏÉÅÌÉúÎ•º Í∞úÎ≥ÑÏ†ÅÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎäî Ï†ëÍ∑º Î∞©ÏãùÏù¥Îã§. Ïù¥Îäî Í∞ÄÏö∞Ïä§-ÏûêÏù¥Îç∏(Gauss-Seidel) Î∞©ÏãùÏùò ÏµúÏ†ÅÌôîÏôÄ Ïú†ÏÇ¨ÌïòÍ≤å, ÏÉàÎ°≠Í≤å ÏóÖÎç∞Ïù¥Ìä∏Îêú Í∞ÄÏπò Ï†ïÎ≥¥Î•º Ï¶âÏãú Îã§Î•∏ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏Ïóê ÌôúÏö©ÌïúÎã§.

ÎπÑÎèôÍ∏∞Ï†Å DPÏùò ÌïµÏã¨ ÏõêÎ¶¨Îäî Î™®Îì† ÏÉÅÌÉúÎ•º ÏßÄÏÜçÏ†ÅÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎêò, Í∞±Ïã†Îêú Í∞íÏùÑ Ï¶âÏãú ÏÇ¨Ïö©ÌïúÎã§Îäî Í≤ÉÏù¥Îã§. ÏòàÎ•º Îì§Ïñ¥, ÏÉÅÌÉú s‚ÇÅÏùÑ ÏóÖÎç∞Ïù¥Ìä∏Ìïú ÌõÑ, Í∑∏ Í∞±Ïã†Îêú Í∞ÄÏπòÎ•º ÏÉÅÌÉú s‚ÇÇ ÏóÖÎç∞Ïù¥Ìä∏Ïóê Î∞îÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎã§. Ïù¥Îü¨Ìïú Î∞©ÏãùÏùÄ Ï†ïÎ≥¥Í∞Ä Îçî Îπ†Î•¥Í≤å Ï†ÑÌååÎêòÎèÑÎ°ù ÌïòÎ©∞, ÌäπÌûà Í∞ÄÏπò Ï†ïÎ≥¥Í∞Ä ÌäπÏ†ï Î∞©Ìñ•ÏúºÎ°ú ÌùêÎ•¥Îäî Í≤ΩÏö∞Ïóê Ìö®Í≥ºÏ†ÅÏù¥Îã§.

ÎπÑÎèôÍ∏∞Ï†Å DPÏùò Ï£ºÏöî Ïû•Ï†ê Ï§ë ÌïòÎÇòÎäî Í≥ÑÏÇ∞ ÏûêÏõêÏùÑ Îçî Ìö®Ïú®Ï†ÅÏúºÎ°ú Ìï†ÎãπÌï† Ïàò ÏûàÎã§Îäî Ï†êÏù¥Îã§. Î™®Îì† ÏÉÅÌÉúÎ•º Í∑†Îì±ÌïòÍ≤å Ï≤òÎ¶¨ÌïòÎäî ÎåÄÏã†, ÏóêÏù¥Ï†ÑÌä∏Ïùò ÌòÑÏû¨ Í¥ÄÏã¨ ÏòÅÏó≠Ïù¥ÎÇò Í∞ÄÏπò Î≥ÄÌôîÍ∞Ä ÌÅ∞ ÏòÅÏó≠Ïóê Îçî ÎßéÏùÄ Í≥ÑÏÇ∞ ÏûêÏõêÏùÑ Ìà¨ÏûÖÌï† Ïàò ÏûàÎã§. Ïù¥Îäî ÌäπÌûà ÎåÄÍ∑úÎ™® ÏÉÅÌÉú Í≥µÍ∞ÑÏóêÏÑú Ï§ëÏöîÌïú Ïù¥Ï†êÏù¥ ÎêúÎã§.

ÎòêÌïú, ÎπÑÎèôÍ∏∞Ï†Å DPÎäî Ïã§ÏãúÍ∞Ñ ÌïôÏäµ ÏÉÅÌô©ÏóêÏÑúÎèÑ Ïú†Ïö©ÌïòÎã§. ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÎäî ÎèôÏïà Ï†ëÍ∑ºÌïòÎäî ÏÉÅÌÉúÎßåÏùÑ ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏Ìï®ÏúºÎ°úÏç®, Ï†ÑÏ≤¥ ÏÉÅÌÉú Í≥µÍ∞ÑÏùÑ Îã§Î£∞ ÌïÑÏöî ÏóÜÏù¥ Í¥ÄÎ†® ÏòÅÏó≠Ïóê ÏßëÏ§ëÌï† Ïàò ÏûàÎã§. Ïù¥Î•º ÌÜµÌï¥ Ïã§ÏãúÍ∞Ñ ÏÑ±Îä•ÏùÑ Í∞úÏÑ†ÌïòÍ≥† Í≥ÑÏÇ∞ ÎπÑÏö©ÏùÑ Ï†àÍ∞êÌï† Ïàò ÏûàÎã§.

Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò Ïä§ÏúÑÌïë(Prioritized Sweeping)Í≥º Í∞ôÏùÄ Î∞úÏ†ÑÎêú ÎπÑÎèôÍ∏∞Ï†Å DP Î∞©Î≤ïÏùÄ Î≤®Îßå Ïò§Ï∞®Í∞Ä ÌÅ∞ ÏÉÅÌÉúÏóê Ïö∞ÏÑ†ÏàúÏúÑÎ•º Î∂ÄÏó¨ÌïòÏó¨ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÌïôÏäµÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïúÎã§. Ïù¥Îü¨Ìïú Î∞©ÏãùÏùÄ Í∞ÄÏπò Ï†ïÎ≥¥Í∞Ä Îπ†Î•¥Í≤å Î≥ÄÌïòÎäî ÏÉÅÌÉúÏóê Í≥ÑÏÇ∞ ÏûêÏõêÏùÑ ÏßëÏ§ëÏãúÏºú Ï†ÑÏ≤¥Ï†ÅÏù∏ ÏàòÎ†¥ ÏÜçÎèÑÎ•º ÎÜíÏùº Ïàò ÏûàÎã§.
</details>
</details>

<details>
<summary>15. DPÏùò ÌïúÍ≥ÑÏ†êÍ≥º Ïù¥Î•º Í∑πÎ≥µÌïòÍ∏∞ ÏúÑÌïú ÎåÄÏïàÏ†Å Ï†ëÍ∑ºÎ≤ïÏùÑ ÎÖºÏùòÌïòÏãúÏò§.</summary>

Î™®Îç∏ Í∏∞Î∞ò DPÎäî Ï†ÑÏù¥ ÌôïÎ•†Í≥º Î≥¥ÏÉÅ Ìï®ÏàòÎ•º Î™®Îëê ÏïåÏïÑÏïºÎßå ÎèôÏûëÌïúÎã§. Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî Ïù¥ Ï†ïÎ≥¥Î•º ÏïåÍ∏∞ Ïñ¥Î†§Ïö∞ÎØÄÎ°ú, Monte Carlo, TD ÌïôÏäµ, Dyna Î∞©ÏãùÍ≥º Í∞ôÏù¥ Î™®Îç∏Ïù¥ ÏóÜÏñ¥ÎèÑ ÌïôÏäµÌïòÍ±∞ÎÇò Î™®Îç∏ÏùÑ Í∑ºÏÇ¨Ìï¥ Í≥ÑÌöçÍ≥º ÌïôÏäµÏùÑ Í≤∞Ìï©ÌïòÎäî Í∏∞Î≤ïÏùÑ ÏÇ¨Ïö©ÌïúÎã§.
<details>
<summary>Ï†ïÎãµ</summary>
ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç(DP)ÏùÄ ÎßàÎ•¥ÏΩîÌîÑ Í≤∞Ï†ï Í≥ºÏ†ï(MDP)Ïùò ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Ï∞æÎäî Í∞ïÎ†•Ìïú Î∞©Î≤ïÏù¥ÏßÄÎßå, Ïã§Ï†ú ÏùëÏö©Ïóê ÏûàÏñ¥ Ïó¨Îü¨ Ï§ëÏöîÌïú ÌïúÍ≥ÑÏ†êÏùÑ Í∞ÄÏßÄÍ≥† ÏûàÎã§. Ïù¥Îü¨Ìïú ÌïúÍ≥ÑÏ†êÏùÑ Ïù¥Ìï¥ÌïòÍ≥† Ïù¥Î•º Í∑πÎ≥µÌïòÍ∏∞ ÏúÑÌïú ÎåÄÏïàÏ†Å Ï†ëÍ∑ºÎ≤ïÏùÑ ÏÇ¥Ìé¥Î≥¥Îäî Í≤ÉÏùÄ Ïã§Ïö©Ï†ÅÏù∏ Í∞ïÌôîÌïôÏäµ ÏãúÏä§ÌÖú Í∞úÎ∞úÏóê ÌïÑÏàòÏ†ÅÏù¥Îã§.

DPÏùò Í∞ÄÏû• ÌÅ∞ ÌïúÍ≥ÑÏ†êÏùÄ ÌôòÍ≤Ω Î™®Îç∏Ïóê ÎåÄÌïú ÏôÑÏ†ÑÌïú ÏßÄÏãùÏùÑ ÌïÑÏöîÎ°ú ÌïúÎã§Îäî Ï†êÏù¥Îã§. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú, Î™®Îì† ÏÉÅÌÉú Ï†ÑÏù¥ ÌôïÎ•† P(s'|s,a)ÏôÄ Î≥¥ÏÉÅ Ìï®Ïàò r(s,a,s')Î•º Ï†ïÌôïÌûà ÏïåÏïÑÏïº ÌïúÎã§. Í∑∏Îü¨ÎÇò Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî Ïù¥Îü¨Ìïú Ï†ïÎ≥¥Î•º Ï†ïÌôïÌïòÍ≤å ÏïåÍ∏∞ Ïñ¥Î†§Ïö¥ Í≤ΩÏö∞Í∞Ä ÎßéÎã§. Î≥µÏû°Ìïú Î¨ºÎ¶¨Ï†Å ÏãúÏä§ÌÖú, Ïù∏Í∞Ñ ÌñâÎèôÏù¥ Í¥ÄÏó¨Îêú ÌôòÍ≤Ω, ÎòêÎäî ÎØ∏ÏßÄÏùò Î≥ÄÏàòÍ∞Ä ÎßéÏùÄ ÏÉÅÌô©ÏóêÏÑúÎäî Ï†ïÌôïÌïú Î™®Îç∏ÏùÑ Íµ¨Ï∂ïÌïòÎäî Í≤ÉÏù¥ Î∂àÍ∞ÄÎä•Ìï† Ïàò ÏûàÎã§.

Îòê Îã§Î•∏ Ï§ëÏöîÌïú ÌïúÍ≥ÑÎäî ÏÉÅÌÉú Í≥µÍ∞ÑÍ≥º ÌñâÎèô Í≥µÍ∞ÑÏùò ÌÅ¨Í∏∞Î°ú Ïù∏Ìïú Í≥ÑÏÇ∞ Î≥µÏû°ÏÑ±Ïù¥Îã§. DP ÏïåÍ≥†Î¶¨Ï¶òÏùò Í≥ÑÏÇ∞ Î≥µÏû°ÎèÑÎäî ÏÉÅÌÉú ÏàòÏôÄ ÌñâÎèô ÏàòÏóê ÎπÑÎ°ÄÌïòÏó¨ Ï¶ùÍ∞ÄÌïòÎØÄÎ°ú, ÌÅ∞ Í∑úÎ™®Ïùò Î¨∏Ï†úÏóêÏÑúÎäî Í≥ÑÏÇ∞ÎüâÏù¥ Í∞êÎãπÌïòÍ∏∞ Ïñ¥Î†§Ïö∏ Ï†ïÎèÑÎ°ú Ï¶ùÍ∞ÄÌïúÎã§. Ïù¥Î•∏Î∞î 'Ï∞®ÏõêÏùò Ï†ÄÏ£º'Î°ú Ïù∏Ìï¥ ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞Ñ ÎÇ¥Ïóê Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ Ïñ¥Î†§ÏõåÏßÑÎã§.

ÎòêÌïú DPÎäî Ïó∞ÏÜçÏ†ÅÏù∏ ÏÉÅÌÉú Î∞è ÌñâÎèô Í≥µÍ∞ÑÏùÑ ÏßÅÏ†ë Îã§Î£®Í∏∞ Ïñ¥Î†µÎã§Îäî ÌïúÍ≥ÑÍ∞Ä ÏûàÎã§. Ï†ÑÌÜµÏ†ÅÏù∏ DP ÏïåÍ≥†Î¶¨Ï¶òÏùÄ Ïù¥ÏÇ∞Ï†ÅÏù∏ ÏÉÅÌÉúÏôÄ ÌñâÎèôÏùÑ Í∞ÄÏ†ïÌïòÎØÄÎ°ú, Ïó∞ÏÜçÏ†ÅÏù∏ Í≥µÍ∞ÑÏùÑ Îã§Î£®Í∏∞ ÏúÑÌï¥ÏÑúÎäî Ïù¥ÏÇ∞ÌôîÍ∞Ä ÌïÑÏöîÌïòÏßÄÎßå, Ïù¥Îäî Ï†ïÌôïÏÑ± ÏÜêÏã§Ïù¥ÎÇò Í≥ÑÏÇ∞ Î≥µÏû°ÏÑ± Ï¶ùÍ∞ÄÎ°ú Ïù¥Ïñ¥Ïßà Ïàò ÏûàÎã§.

Ïù¥Îü¨Ìïú ÌïúÍ≥ÑÎ•º Í∑πÎ≥µÌïòÍ∏∞ ÏúÑÌï¥ Ïó¨Îü¨ ÎåÄÏïàÏ†Å Ï†ëÍ∑ºÎ≤ïÏù¥ Í∞úÎ∞úÎêòÏóàÎã§. Î™®Îç∏ ÏóÜÎäî(Model-free) Î∞©Î≤ïÎ°†Ïù∏ Î™¨ÌÖåÏπ¥Î•ºÎ°ú Î∞©Î≤ïÍ≥º ÏãúÍ∞ÑÏ∞® ÌïôÏäµÏùÄ ÌôòÍ≤Ω Î™®Îç∏Ïóê ÎåÄÌïú ÏÇ¨Ï†Ñ ÏßÄÏãù ÏóÜÏù¥ ÏßÅÏ†ë Í≤ΩÌóòÏúºÎ°úÎ∂ÄÌÑ∞ ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ ÌïôÏäµÌïúÎã§. Î™¨ÌÖåÏπ¥Î•ºÎ°ú Î∞©Î≤ïÏùÄ ÏôÑÏ†ÑÌïú ÏóêÌîºÏÜåÎìúÏùò Í≤ΩÌóòÏùÑ ÌÜµÌï¥ Í∞ÄÏπòÎ•º Ï∂îÏ†ïÌïòÍ≥†, ÏãúÍ∞ÑÏ∞® ÌïôÏäµÏùÄ Î∂ÄÌä∏Ïä§Ìä∏ÎûòÌïëÏùÑ ÌÜµÌï¥ Ïò®ÎùºÏù∏ÏúºÎ°ú Í∞ÄÏπòÎ•º ÏóÖÎç∞Ïù¥Ìä∏ÌïúÎã§.

Ìï®Ïàò Í∑ºÏÇ¨ Î∞©Î≤ïÎ°†ÏùÄ Ïã†Í≤ΩÎßùÍ≥º Í∞ôÏùÄ ÌååÎùºÎØ∏ÌÑ∞ÌôîÎêú Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Í∞ÄÏπò Ìï®ÏàòÎÇò Ï†ïÏ±ÖÏùÑ ÌëúÌòÑÌï®ÏúºÎ°úÏç® ÎåÄÍ∑úÎ™® ÎòêÎäî Ïó∞ÏÜçÏ†ÅÏù∏ ÏÉÅÌÉú Í≥µÍ∞ÑÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Îã§Î£¨Îã§. Ïù¥ Î∞©Î≤ïÏùÄ ÏÉÅÌÉú Í≥µÍ∞ÑÏùò ÏùºÎ∞òÌôîÎ•º ÌÜµÌï¥ Ï∞®ÏõêÏùò Ï†ÄÏ£º Î¨∏Ï†úÎ•º ÏôÑÌôîÌïúÎã§.

Î™®Îç∏ Í∏∞Î∞ò ÌïôÏäµÍ≥º Î™®Îç∏ ÏóÜÎäî ÌïôÏäµÏùÑ Í≤∞Ìï©Ìïú ÌïòÏù¥Î∏åÎ¶¨Îìú Ï†ëÍ∑ºÎ≤ïÏù∏ Dyna ÏïÑÌÇ§ÌÖçÏ≤òÎäî Ïã§Ï†ú Í≤ΩÌóòÏóêÏÑú ÌôòÍ≤Ω Î™®Îç∏ÏùÑ Ï†êÏßÑÏ†ÅÏúºÎ°ú ÌïôÏäµÌïòÍ≥†, Ïù¥ Î™®Îç∏ÏùÑ ÏãúÎÆ¨Î†àÏù¥ÏÖòÌïòÏó¨ Ï∂îÍ∞ÄÏ†ÅÏù∏ Í≥ÑÌöçÏùÑ ÏàòÌñâÌïúÎã§. Ïù¥Î•º ÌÜµÌï¥ ÏßÅÏ†ë Í≤ΩÌóòÏùò Îç∞Ïù¥ÌÑ∞ Ìö®Ïú®ÏÑ±Í≥º Î™®Îç∏ Í∏∞Î∞ò Í≥ÑÌöçÏùò Ïû•Ï†êÏùÑ Î™®Îëê ÌôúÏö©Ìï† Ïàò ÏûàÎã§.

Ïã§ÏãúÍ∞Ñ ÎèôÏ†Å ÌîÑÎ°úÍ∑∏ÎûòÎ∞çÏùÄ ÌòÑÏû¨ ÏÉÅÌÉúÏôÄ Í¥ÄÎ†®Îêú Ï†úÌïúÎêú ÏòÅÏó≠Îßå ÏóÖÎç∞Ïù¥Ìä∏Ìï®ÏúºÎ°úÏç® Í≥ÑÏÇ∞ Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏù∏Îã§. Ïù¥Îäî ÎåÄÍ∑úÎ™® ÏÉÅÌÉú Í≥µÍ∞ÑÏóêÏÑú Ìö®Ïú®Ï†ÅÏù∏ ÏùòÏÇ¨Í≤∞Ï†ïÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïúÎã§.
</details>
</details>

<h3 id="what-is-td-method">What is TD Method?</h3>

<p>Í∞ïÌôîÌïôÏäµÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏Îäî ÏÉÅÌÉúÎ•º Í¥ÄÏ∞∞ÌïòÍ≥† ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÎ©∞, Í∑∏ Í≤∞Í≥ºÎ°úÎ∂ÄÌÑ∞ ÏûêÏã†Ïù¥ Îî∞Î•¥Îäî Ï†ïÏ±ÖÏù¥ ÏñºÎßàÎÇò Ï¢ãÏùÄÏßÄÎ•º ÌïôÏäµÌïòÍ≤å ÎêúÎã§. Ïù¥ ÌïôÏäµ Í≥ºÏ†ïÏóêÏÑú Í∞ÄÏû• ÌïµÏã¨Ïù¥ ÎêòÎäî Íµ¨ÏÑ± ÏöîÏÜå Ï§ë ÌïòÎÇòÍ∞Ä Î∞îÎ°ú <strong>Í∞ÄÏπò Ìï®Ïàò(value function)</strong>Ïù¥Î©∞, Ïù¥Îäî Ï£ºÏñ¥ÏßÑ ÏÉÅÌÉú ÎòêÎäî ÏÉÅÌÉú-ÌñâÎèô ÏåçÏù¥ ÏñºÎßàÎÇò Ï¢ãÏùÄÏßÄÎ•º ÏàòÏπòÎ°ú ÎÇòÌÉÄÎÇ¥Îäî Ïó≠Ìï†ÏùÑ ÌïúÎã§.</p>

<p>Í∞ÄÏπò Ìï®ÏàòÎ•º Ï∂îÏ†ïÌïòÎäî ÎåÄÌëúÏ†ÅÏù∏ Î∞©ÏãùÏóêÎäî ÏÑ∏ Í∞ÄÏßÄÍ∞Ä ÏûàÎã§. Ï≤´Ïß∏Îäî Dynamic Programming(DP) Î∞©ÏãùÏúºÎ°ú, ÌôòÍ≤ΩÏùò Î™®Îì† Ï†ïÎ≥¥‚ÄîÏ¶â ÏÉÅÌÉú Ï†ÑÏù¥ ÌôïÎ•†Í≥º Î≥¥ÏÉÅ Ìï®Ïàò‚ÄîÍ∞Ä ÏôÑÏ†ÑÌûà Ï£ºÏñ¥ÏßÑ Í≤ΩÏö∞ÏóêÎßå ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎã§. ÎëòÏß∏Îäî Monte Carlo(MC) Î∞©ÏãùÏúºÎ°ú, ÌôòÍ≤Ω Î™®Îç∏Ïù¥ ÏóÜÏñ¥ÎèÑ ÏûëÎèôÌïòÏßÄÎßå Ìïú ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇ† ÎïåÍπåÏßÄ Í∏∞Îã§Î¶∞ ÌõÑ Ïã§Ï†úÎ°ú ÏñªÏùÄ ÎàÑÏ†Å Î≥¥ÏÉÅÏùÑ ÌÜµÌï¥ ÌïôÏäµÌïúÎã§. ÎßàÏßÄÎßâÏúºÎ°ú Ïù¥ Îëê Î∞©ÏãùÏùò Ï§ëÍ∞ÑÏóê ÏúÑÏπòÌïòÎäî Í≤ÉÏù¥ Î∞îÎ°ú Temporal Difference(TD) Î∞©ÏãùÏù¥Îã§.</p>

<p>TD Î∞©Î≤ïÏùÄ Monte CarloÏ≤òÎüº ÌôòÍ≤Ω Î™®Îç∏ ÏóÜÏù¥ÎèÑ ÌïôÏäµÌï† Ïàò ÏûàÏßÄÎßå, Dynamic ProgrammingÏ≤òÎüº Î∂ÄÌä∏Ïä§Ìä∏ÎûòÌïë(bootstrapping) Í∏∞Î≤ïÏùÑ ÏÇ¨Ïö©ÌïúÎã§Îäî Ï†êÏù¥ ÌäπÏßïÏù¥Îã§. Î∂ÄÌä∏Ïä§Ìä∏ÎûòÌïëÏù¥ÎûÄ, Îßê Í∑∏ÎåÄÎ°ú ‚ÄúÏä§Ïä§Î°ú ÎÅåÏñ¥Ïò¨Î¶¨Îäî‚Äù Í≤ÉÏùÑ ÏùòÎØ∏ÌïòÎ©∞, ÏôÑÏ†ÑÌïú Ï†ïÎãµÏùÑ Í∏∞Îã§Î¶¨Îäî ÎåÄÏã† ÌòÑÏû¨ Í∞ÄÏßÄÍ≥† ÏûàÎäî ÏòàÏ∏°Í∞íÏùÑ Ïù¥Ïö©ÌïòÏó¨ ÏòàÏ∏°ÏùÑ Í∞±Ïã†ÌïòÎäî Î∞©ÏãùÏù¥Îã§.</p>

<p>TD ÌïôÏäµÏùÄ ÏóêÌîºÏÜåÎìúÍ∞Ä Ï¢ÖÎ£åÎê† ÎïåÍπåÏßÄ Í∏∞Îã§Î¶¨ÏßÄ ÏïäÎäîÎã§. ÏóêÏù¥Ï†ÑÌä∏Îäî ÏÉÅÌÉú Ï†ÑÏù¥ÏôÄ Î≥¥ÏÉÅÏùÑ ÌïòÎÇò Í¥ÄÏ∞∞Ìï† ÎïåÎßàÎã§ Í≥ßÎ∞îÎ°ú ÏûêÏã†Ïùò ÏòàÏ∏°Í∞íÏùÑ Í∞±Ïã†Ìï† Ïàò ÏûàÎã§. ÏòàÎ•º Îì§Ïñ¥, ÌòÑÏû¨ ÏÉÅÌÉú 
\(S_t\)
\(V(S_t)\)
 Ïóê ÎåÄÌïú Í∞ÄÏπò</p>

\[V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)\]

<p>ÏúÑ ÏàòÏãùÏóêÏÑú Í¥ÑÌò∏ ÏïàÏùò Ìï≠ÏùÄ <strong>TD Ïò§Ï∞®(Temporal Difference Error)</strong>ÎùºÍ≥† Î∂àÎ¶¨Î©∞, Îã§Ïùå ÏÉÅÌÉú 
\(S_{t+1}\)
 Ïóê ÎåÄÌïú ÌòÑÏû¨ Ï∂îÏ†ïÍ∞íÍ≥º Ïã§Ï†ú Î≥¥ÏÉÅÏùÑ ÎçîÌïú Í∞íÏù¥ ÌòÑÏû¨ ÏÉÅÌÉúÏùò Ï∂îÏ†ïÍ∞íÍ≥º ÏñºÎßàÎÇò Ï∞®Ïù¥Í∞Ä ÎÇòÎäîÏßÄÎ•º ÎÇòÌÉÄÎÇ∏Îã§. ÎßåÏïΩ Ïù¥ Í∞íÏù¥ ÏñëÏàòÎùºÎ©¥ ÌòÑÏû¨ Í∞ÄÏπòÍ∞Ä Í≥ºÏÜåÌèâÍ∞ÄÎêòÏóàÎã§Îäî ÎúªÏù¥Í≥†, ÏùåÏàòÎùºÎ©¥ Í≥ºÎåÄÌèâÍ∞ÄÎêòÏóàÎã§Îäî ÎúªÏù¥Îã§. TD ÌïôÏäµÏùÄ Ïù¥ Ïò§Ï∞®Î•º Ï§ÑÏù¥Îäî Î∞©Ìñ•ÏúºÎ°ú ÏòàÏ∏°Í∞íÏùÑ Ï°∞Ï†ïÌïúÎã§.</p>

<p>Ïù¥Îü¨Ìïú Î∞©ÏãùÏùÄ ÎßéÏùÄ Ïû•Ï†êÏùÑ Í∞ñÍ≥† ÏûàÎã§. Ï≤´Ïß∏, Ïã§ÏãúÍ∞Ñ ÌïôÏäµÏù¥ Í∞ÄÎä•ÌïòÎã§. ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÎäî ÎèôÏïà Îß§ ÏãúÏ†êÎßàÎã§ ÏòàÏ∏°Í∞íÏùÑ Ï°∞Í∏àÏî© Í∞±Ïã†Ìï† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê, ÏóêÌîºÏÜåÎìúÍ∞Ä Í∏∏Í±∞ÎÇò ÎÅùÎÇòÏßÄ ÏïäÎäî ÌôòÍ≤ΩÏóêÏÑúÎèÑ Ïú†Ïö©ÌïòÎã§. ÎëòÏß∏, ÏàòÎ†¥ ÏÜçÎèÑÍ∞Ä Îπ†Î•¥Îã§. ÏôÑÏ†ÑÌïú Î¶¨ÌÑ¥Ïù¥ ÏïÑÎãàÎùº Î∂ÄÎ∂ÑÏ†ÅÏù∏ Ï∂îÏ†ïÍ∞íÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê, Î∞òÎ≥µÏ†ÅÏúºÎ°ú Îçî ÎßéÏùÄ ÏÉÅÌÉúÎ•º Í≤ΩÌóòÌïòÎäî Í≥ºÏ†ïÏóêÏÑú ÏòàÏ∏°Ïù¥ Ï†êÏ†ê ÏïàÏ†ïÎêòÏñ¥ Í∞ÑÎã§. ÏÖãÏß∏, Monte CarloÏóê ÎπÑÌï¥ Î∂ÑÏÇ∞Ïù¥ ÏûëÎã§. Î¨ºÎ°†, Î∂ÄÌä∏Ïä§Ìä∏ÎûòÌïëÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Ïñ¥Îäê Ï†ïÎèÑÏùò Ìé∏Ìñ•ÏùÄ Ï°¥Ïû¨ÌïòÏßÄÎßå, ÏïàÏ†ïÏ†ÅÏù∏ ÏàòÎ†¥ÏóêÎäî Ïú†Î¶¨Ìïú Íµ¨Ï°∞Îã§.</p>

<h3 id="bootstrap">Bootstrap</h3>

<p>Î∂ÄÌä∏Ïä§Ìä∏ÎûòÌïëÎ∂ÄÌä∏Ïä§Ìä∏ÎûòÌïëÏùÄ Îã§Ïùå ÏÉÅÌÉúÏùò ÌòÑÏû¨ Ï∂îÏ†ïÍ∞íÏùÑ Ïù¥Ïö©ÌïòÏó¨ ÌòÑÏû¨ ÏÉÅÌÉúÏùò Í∞ÄÏπòÎ•º Í∞±Ïã†ÌïòÎäî Î∞©Î≤ïÏù¥Îã§.
Monte Carlo, Dynamic Programming, Í∑∏Î¶¨Í≥† TD(0)Î•º ÎπÑÍµê¬∑Ï¶ùÎ™ÖÌïòÍ≥† ÌäπÏßïÏùÑ ÏÇ¥Ìé¥Î≥¥Ïûê.</p>

<h3 id="1-monte-carlo-mc-Î∞©Î≤ï">1. Monte Carlo (MC) Î∞©Î≤ï</h3>

<ul>
  <li><strong>Ï†ÑÏ≤¥ ÏóêÌîºÏÜåÎìú Î¶¨ÌÑ¥</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    G_t = \sum_{k=0}^{T-t-1} \gamma^k\,r_{t+k+1}
  \]
</div>
<ul>
  <li><strong>ÏóÖÎç∞Ïù¥Ìä∏ Ïãù</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    V(s_t) \leftarrow V(s_t) + \alpha\bigl[G_t - V(s_t)\bigr]
  \]
</div>
<ul>
  <li><strong>Ï¶ùÎ™Ö ÏöîÏïΩ</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    \mathbb{E}[G_t]
    = \sum_{k=0}^{T-t-1}\gamma^k\,\mathbb{E}[r_{t+k+1}\mid s_t=s]
    = v_\pi(s)
  \]
</div>

<pre><code class="language-mermaid">flowchart LR
    classDef circleStyle fill:#fff,stroke:#000,stroke-width:2px;

    Start(("ÏãúÏûë"))
    Init(("Ï¥àÍ∏∞Ìôî: V(s) Î∞è returns Î¶¨Ïä§Ìä∏"))
    Episode(("ÏóêÌîºÏÜåÎìú ÏÉùÏÑ±"))
    Compute(("Ï¥ù Î¶¨ÌÑ¥ G‚Çú Í≥ÑÏÇ∞"))
    Update(("V(s‚Çú) ‚Üê V(s‚Çú) + Œ±¬∑[G‚Çú ‚àí V(s‚Çú)]"))
    Check(("Ï∂îÍ∞Ä ÏóêÌîºÏÜåÎìú?"))
    End(("Ï¢ÖÎ£å"))

    class Start,Init,Episode,Compute,Update,Check,End circleStyle;

    Start --&gt; Init --&gt; Episode --&gt; Compute --&gt; Update --&gt; Check
    Check --&gt;|"Ïòà"| Init
    Check --&gt;|"ÏïÑÎãàÏò§"| End
</code></pre>

<h3 id="2-dynamic-programming-dp">2. Dynamic Programming (DP)</h3>
<ul>
  <li><strong>Bellman Í∏∞ÎåÄ Î∞©Ï†ïÏãù</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    v_\pi(s) = \mathbb{E}\bigl[r_{t+1} + \gamma\,v_\pi(s_{t+1}) \mid s_t=s\bigr]
  \]
</div>
<ul>
  <li><strong>Î∞òÎ≥µ Í∞±Ïã† (Ï†ïÏ±Ö ÌèâÍ∞Ä)</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    V_{k+1}(s) = \mathbb{E}\bigl[r_{t+1} + \gamma V_k(s_{t+1}) \mid s_t=s\bigr]
  \]
</div>
<ul>
  <li><strong>ÏàòÎ†¥ÏÑ± (Contractive mapping)</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    T[V](s) = \mathbb{E}[r_{t+1} + \gamma V(s_{t+1}) \mid s_t=s]
  \]
  \[
    \|T[V] - T[V']\|_\infty \le \gamma\,\|V - V'\|_\infty
  \]
</div>

<pre><code class="language-mermaid">flowchart LR
    classDef circleStyle fill:#fff,stroke:#000,stroke-width:2px;

    Start(("ÏãúÏûë"))
    Init(("Ï¥àÍ∏∞Ìôî: V‚ÇÄ(s)"))
    Iterate(("Í∞í Î∞òÎ≥µ Í∞±Ïã†:\nV‚Çñ‚Çä‚ÇÅ(s) = E[r + Œ≥¬∑V‚Çñ(s')]"))
    Converge(("‚ÄñV‚Çñ‚Çä‚ÇÅ ‚àí V‚Çñ‚Äñ &lt; Œ∏ ?"))
    End(("Ï¢ÖÎ£å"))

    class Start,Init,Iterate,Converge,End circleStyle;

    Start --&gt; Init --&gt; Iterate --&gt; Converge
    Converge --&gt;|"ÏïÑÎãàÏò§"| Iterate
    Converge --&gt;|"Ïòà"| End

</code></pre>

<h3 id="3-temporal-difference-td0">3. Temporal-Difference (TD(0))</h3>

<p>MCÏôÄ DPÏùò Ï†àÏ∂©: Î∂ÄÌä∏Ïä§Ìä∏ÎûòÌïëÏùÑ ÌôúÏö©Ìïú Ïò®ÎùºÏù∏ ÏóÖÎç∞Ïù¥Ìä∏</p>
<ul>
  <li><strong>TD Ïò§Ï∞® (Temporal Difference Error)</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    \delta_t = r_{t+1} + \gamma\,V(s_{t+1}) - V(s_t)
  \]
</div>

<ul>
  <li><strong>ÏóÖÎç∞Ïù¥Ìä∏ Ïãù</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    V(s_t) \leftarrow V(s_t) + \alpha\,\delta_t
  \]
</div>

<ul>
  <li>
    <p><strong>ÌäπÏßï</strong></p>

    <ul>
      <li>Ïò®ÎùºÏù∏ ÏóÖÎç∞Ïù¥Ìä∏: Ìïú Ïä§ÌÖùÎßàÎã§ Ï¶âÏãú Í∞±Ïã†</li>
      <li>Î∂ÄÌä∏Ïä§Ìä∏ÎûòÌïëÏùÑ ÌôúÏö©</li>
      <li>Ìé∏Ìñ•‚Üë, Î∂ÑÏÇ∞‚Üì (MCÏôÄ Î∞òÎåÄ)</li>
    </ul>
  </li>
</ul>

<pre><code class="language-mermaid">
flowchart LR
    classDef circleStyle fill:#fff,stroke:#000,stroke-width:2px;

    Start(("ÏãúÏûë"))
    Init(("Ï¥àÍ∏∞Ìôî V(s)"))
    Observe(("ÏÉÅÌÉú Í¥ÄÏ∏°: s‚Çú"))
    Action(("ÌñâÎèô ÏÑ†ÌÉù: a‚Çú ‚Üê œÄ(s‚Çú)"))
    Step(("Î≥¥ÏÉÅ Î∞è Îã§Ïùå ÏÉÅÌÉú Í¥ÄÏ∏°:\nr‚Çú‚Çä‚ÇÅ, s‚Çú‚Çä‚ÇÅ"))
    Update(("TD ÏóÖÎç∞Ïù¥Ìä∏:\nŒ¥‚Çú Í≥ÑÏÇ∞ Î∞è V(s‚Çú) Í∞±Ïã†"))
    Term(("Ï¢ÖÎ£å Ïó¨Î∂Ä ÌôïÏù∏"))
    End(("Ï¢ÖÎ£å"))

    class Start,Init,Observe,Action,Step,Update,Term,End circleStyle;

    Start --&gt; Init --&gt; Observe --&gt; Action --&gt; Step --&gt; Update --&gt; Term
    Term --&gt;|"ÏïÑÎãàÏò§"| Observe
    Term --&gt;|"Ïòà"| End

</code></pre>

<div align="center">
  <img src="/images/backups.png" alt="bandit2" style="width: 100%;" />
</div>

<h3 id="on---policy-sarsa">On - policy SARSA</h3>

<p>TD Î≤ïÏùÄ Îã§Ïùå ÏãùÏùÑ Îî∞Î•∏Îã§.</p>

<div style="overflow-x: auto;">
  \[
    V(s_t) \gets V(s_t) + \alpha \big(R_{t} + \gamma V(s_{t+1}) - V(s_t)\big)
  \]
</div>

<div style="overflow-x: auto;">
  \[
    Q_\pi(S_t, A_t) \gets Q_\pi(S_t, A_t) + \alpha \big(R_{t} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) - Q_\pi(S_t, A_t)\big)
  \]
</div>

<div align="center"> 
<div class="mermaid">
graph LR
st((s<sub>t</sub>)) --&gt; at((a<sub>t</sub>))
at --&gt; |Rt| st1((s<sub>t+1</sub>))
st1 --&gt; at1((a<sub>t+1</sub>))
</div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>
<span class="kn">from</span> <span class="n">common.utils</span> <span class="kn">import</span> <span class="n">greedy_probs</span>


<span class="k">class</span> <span class="nc">SarsaAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># deque ÏÇ¨Ïö©
</span>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>  <span class="c1"># piÏóêÏÑú ÏÑ†ÌÉù
</span>        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">next_q</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">]</span>  <span class="c1"># Îã§Ïùå Q Ìï®Ïàò
</span>
        <span class="c1"># TDÎ≤ïÏúºÎ°ú self.Q Í∞±Ïã†
</span>        <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>
        
        <span class="c1"># Ï†ïÏ±Ö Í∞úÏÑ†
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span>


<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">SarsaAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>  <span class="c1"># Îß§Î≤à Ìò∏Ï∂ú
</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># Î™©ÌëúÏóê ÎèÑÎã¨ÌñàÏùÑ ÎïåÎèÑ Ìò∏Ï∂ú
</span>            <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">Q</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="off-sarsa">off SARSA</h3>

<p>Ïò§ÌîÑÏ†ïÏ±ÖÏóêÎäî ÌñâÎèôÏ†ïÏ±ÖÍ≥º ÎåÄÏÉÅÏ†ïÏ±ÖÏùÑ Îî∞Î°ú Í∞ÄÏßÄÍ≥† ÏûàÎã§. ÌñâÎèôÏ†ïÏ±ÖÏùÄ Îã§ÏñëÌïú ÌñâÎèôÏùÑ ÏãúÎèÑÌïòÎ©∞ ÏÉòÌîå Îç∞Ïù¥ÌÑ∞Î•º Ìè≠ÎÑìÍ≤å ÏàòÏßëÌïòÎäî Îç∞ Ï¥àÏ†êÏùÑ ÎßûÏ∂òÎã§. Ïù¥Î•º ÌÜµÌï¥ ÌôòÍ≤ΩÏóê ÎåÄÌïú ÌÉêÏÉâÏùÑ Í∑πÎåÄÌôîÌïúÎã§. Î∞òÎ©¥, ÎåÄÏÉÅÏ†ïÏ±ÖÏùÄ ÌÉêÏöïÏ†ïÏ±ÖÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌïòÏó¨ ÏµúÏ†ÅÏùò ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÍ≥† Í∞±Ïã†ÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêúÎã§. Ïù¥Îü¨Ìïú Íµ¨Ï°∞Îäî ÌñâÎèôÏ†ïÏ±ÖÍ≥º ÎåÄÏÉÅÏ†ïÏ±ÖÏùò Ïó≠Ìï†ÏùÑ Î∂ÑÎ¶¨ÌïòÏó¨ ÌïôÏäµÏùò Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏù¥Îäî Îç∞ Í∏∞Ïó¨ÌïúÎã§.</p>

<div style="overflow-x: auto;">
  \[
    Q_\pi(S_t, A_t) \gets Q_\pi(S_t, A_t) + \alpha \big(R_{t} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) - Q_\pi(S_t, A_t)\big)
  \]
</div>

<p>Í∞ïÌôîÌïôÏäµÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏Îäî ÌôòÍ≤ΩÍ≥ºÏùò ÏÉÅÌò∏ÏûëÏö©ÏùÑ ÌÜµÌï¥ ÏµúÏ†ÅÏùò Ï†ïÏ±ÖÏùÑ ÌïôÏäµÌïòÍ≤å ÎêúÎã§. Ïù¥Îïå, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Îî∞Î•¥Îäî Ï†ïÏ±ÖÍ≥º ÌïôÏäµÏóê ÏÇ¨Ïö©ÎêòÎäî Ï†ïÏ±ÖÏù¥ Í∞ôÎã§Î©¥ Ïù¥Î•º <strong>on-policy</strong>, Îã§Î•¥Îã§Î©¥ <strong>off-policy</strong>ÎùºÍ≥† Î∂ÄÎ•∏Îã§. Off-policy ÌïôÏäµÏùÄ Ï†ïÏ±Ö ÌèâÍ∞ÄÏôÄ Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÏùò Ï£ºÏ≤¥Î•º Î∂ÑÎ¶¨Ìï®ÏúºÎ°úÏç® Îçî Ïú†Ïó∞ÌïòÍ≥† Í∞ïÎ†•Ìïú ÌïôÏäµÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïúÎã§. ÎåÄÌëúÏ†ÅÏù∏ ÏòàÎ°úÎäî Q-learning, Expected SARSA, Í∑∏Î¶¨Í≥† Ïù¥ Í∏ÄÏóêÏÑú Îã§Î£∞ <strong>Off-policy SARSA</strong>Í∞Ä ÏûàÎã§.</p>

<p>Off-policy SARSAÎäî Í∏∞Ï°¥Ïùò SARSA Î∞©ÏãùÍ≥º Îã¨Î¶¨, ÌñâÎèôÏùÑ ÏÉùÏÑ±ÌïòÎäî Ï†ïÏ±ÖÍ≥º ÌïôÏäµÏùÑ ÏúÑÌïú ÏóÖÎç∞Ïù¥Ìä∏Ïóê ÏÇ¨Ïö©ÎêòÎäî Ï†ïÏ±ÖÏùÑ Î∂ÑÎ¶¨ÌïúÎã§. <strong>ÌñâÎèôÏ†ïÏ±Ö(behavior policy)</strong>ÏùÄ Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßëÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÎ©∞, Î≥¥ÌÜµ Œµ-greedyÏôÄ Í∞ôÏù¥ ÏùºÏ†ï ÌôïÎ•†Î°ú Î¨¥ÏûëÏúÑ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÎäî <strong>ÌÉêÌóò Ï§ëÏã¨Ïùò Ï†ïÏ±Ö</strong>Ïù¥Îã§. Î∞òÎ©¥, <strong>ÎåÄÏÉÅÏ†ïÏ±Ö(target policy)</strong>ÏùÄ Ïã§Ï†úÎ°ú Q Í∞íÏùÑ ÏóÖÎç∞Ïù¥Ìä∏Ìï† Îïå Í∏∞Ï§ÄÏù¥ ÎêòÎäî Ï†ïÏ±ÖÏù¥Î©∞, ÏùºÎ∞òÏ†ÅÏúºÎ°ú greedy Ï†ïÏ±ÖÏù¥ÎÇò soft policyÍ∞Ä ÏÇ¨Ïö©ÎêúÎã§. Ïù¥Îü¨Ìïú Íµ¨Ï°∞Îäî Îã§ÏñëÌïú ÌñâÎèôÏùÑ ÏãúÎèÑÌïòÎ©¥ÏÑúÎèÑ ÏµúÏ†ÅÏùò Ï†ïÏ±ÖÏùÑ ÌïôÏäµÌïòÎäî Îç∞ Ïú†Î¶¨Ìïú Ï°∞Í±¥ÏùÑ Ï†úÍ≥µÌïúÎã§.</p>

<p>Í∑∏Îü¨ÎÇò ÌñâÎèôÏ†ïÏ±ÖÍ≥º ÎåÄÏÉÅÏ†ïÏ±ÖÏù¥ Îã§Î•¥Í∏∞ ÎïåÎ¨∏Ïóê, ÏàòÏßëÌïú Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïôÏäµÏóê ÏßÅÏ†ëÏ†ÅÏúºÎ°ú Î∞òÏòÅÎêòÍ∏∞ÏóêÎäî Ï∞®Ïù¥Í∞Ä Ï°¥Ïû¨ÌïúÎã§. Ïù¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ <strong>Ï§ëÏöîÎèÑ ÎπÑÏú®(Importance Sampling Ratio)</strong>Ïù¥ ÎèÑÏûÖÎêúÎã§. Ï§ëÏöîÎèÑ ÎπÑÏú®ÏùÄ <em>‚ÄúÏù¥ ÌñâÎèôÏù¥ ÎåÄÏÉÅÏ†ïÏ±ÖÏù¥ÏóàÎã§Î©¥ ÏñºÎßàÎÇò ÏùºÏñ¥ÎÇ¨ÏùÑÍπå?‚Äù</em>Î•º ÌôïÎ•†Ï†ÅÏúºÎ°ú Î≥¥Ï†ïÌï¥Ï£ºÎäî Í≥ÑÏàòÏù¥Î©∞, ÏàòÏãùÏúºÎ°úÎäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÎêúÎã§:</p>

<div style="overflow-x: auto;">
$$
\rho_t = \frac{\pi(A_t \mid S_t)}{\mu(A_t \mid S_t)}
$$
</div>

<p>Ïó¨Í∏∞ÏÑú \(\pi\)Îäî ÎåÄÏÉÅÏ†ïÏ±Ö, \(\mu\)Îäî ÌñâÎèôÏ†ïÏ±ÖÏùÑ ÏùòÎØ∏ÌïúÎã§. Ïù¥ ÎπÑÏú®ÏùÑ ÏóÖÎç∞Ïù¥Ìä∏ ÏãùÏóê Í≥±Ìï¥Ï§åÏúºÎ°úÏç®, ÌñâÎèôÏ†ïÏ±ÖÏúºÎ°ú ÏàòÏßëÌïú Îç∞Ïù¥ÌÑ∞Î•º ÎåÄÏÉÅÏ†ïÏ±Ö Í¥ÄÏ†êÏóêÏÑú Ìï¥ÏÑùÌï† Ïàò ÏûàÍ≤å ÎêúÎã§.</p>

<p>Off-policy SARSAÏùò ÏóÖÎç∞Ïù¥Ìä∏ ÏãùÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§:</p>

<div style="overflow-x: auto;">
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \, \rho_t \left( R_t + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right)
$$
</div>

<p>ÏúÑ ÏãùÏóêÏÑú \(\rho_t\)Îäî ÏïûÏÑú ÏÑ§Î™ÖÌïú Ï§ëÏöîÎèÑ ÎπÑÏú®Ïù¥Î©∞, Ïù¥Î•º ÌÜµÌï¥ ÎåÄÏÉÅÏ†ïÏ±Ö Í∏∞Ï§ÄÏúºÎ°ú TD Ïò§Î•ò Ìï≠ÏùÑ Ï°∞Ï†ïÌïòÍ≤å ÎêúÎã§. ÎßåÏïΩ \(\rho_t = 1\)Ïù¥ÎùºÎ©¥, Ïù¥Îäî on-policy ÏÉÅÌô©Í≥º ÎèôÏùºÌï¥ÏßÑÎã§. Î∞òÎåÄÎ°ú, ÌñâÎèôÏ†ïÏ±ÖÍ≥º ÎåÄÏÉÅÏ†ïÏ±ÖÏù¥ Îã§Î•ºÏàòÎ°ù \(\rho_t\)Îäî 1ÏóêÏÑú Î©ÄÏñ¥ÏßÄÎ©∞, Î≥¥Ï†ïÏùò ÏòÅÌñ•Î†•Ïù¥ Ïª§ÏßÑÎã§.</p>

<p>Ïù¥Îü¨Ìïú Î∞©ÏãùÏùÄ Ïù¥Î°†Ï†ÅÏúºÎ°ú Îß§Ïö∞ Í∞ïÎ†•ÌïòÏßÄÎßå, Ïã§Ïö©Ï†ÅÏúºÎ°úÎäî Ìïú Í∞ÄÏßÄ Ï£ºÏùòÌï† Ï†êÏù¥ ÏûàÎã§. \(\rho_t\)Í∞Ä ÏßÄÎÇòÏπòÍ≤å ÌÅ¨Í±∞ÎÇò ÏûëÏïÑÏßà Í≤ΩÏö∞, ÌïôÏäµ Í≥ºÏ†ïÏóêÏÑú <strong>Î∂ÑÏÇ∞Ïù¥ Ïª§ÏßÄÍ≥† Î∂àÏïàÏ†ïÌï¥Ïßà Ïàò ÏûàÎã§.</strong> Ïù¥Î•º Î∞©ÏßÄÌïòÍ∏∞ ÏúÑÌï¥ <strong>ÌÅ¥Î¶¨Ìïë(clipping)</strong>Ïù¥ÎÇò <strong>ÌèâÍ∑† Ï†ïÍ∑úÌôî(mean normalization)</strong> Îì±Ïùò Í∏∞Î≤ïÏù¥ ÏÇ¨Ïö©ÎêòÍ∏∞ÎèÑ ÌïúÎã§. ÌäπÌûà Ïó¨Îü¨ ÏãúÏ†êÏóê Í±∏Ï≥ê Ï§ëÏöîÎèÑ ÎπÑÏú®ÏùÑ ÎàÑÏ†ÅÌï¥ÏÑú ÏÇ¨Ïö©ÌïòÎäî Í≤ΩÏö∞(Ïòà: SARSA(\(\lambda\)))ÏóêÎäî Î∂ÑÏÇ∞ Î¨∏Ï†úÍ∞Ä ÎçîÏö± Ïã¨Í∞ÅÌï¥ÏßÄÎØÄÎ°ú Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌïòÎã§.</p>

<p>Off-policy SARSAÎäî Ï†ïÏ±ÖÏùò Ïú†Ïó∞ÏÑ±Í≥º Îç∞Ïù¥ÌÑ∞ Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•ÏÑ±ÏùÑ Í∑πÎåÄÌôîÌï† Ïàò ÏûàÎã§Îäî Ï†êÏóêÏÑú Îß§Ïö∞ Ïã§Ïö©Ï†ÅÏù∏ Ï†ëÍ∑ºÎ≤ïÏù¥Î©∞, Îã§ÏñëÌïú Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎèÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÌôúÏö©Îê† Ïàò ÏûàÎã§.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>
<span class="kn">from</span> <span class="n">common.utils</span> <span class="kn">import</span> <span class="n">greedy_probs</span>


<span class="k">class</span> <span class="nc">SarsaOffPolicyAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>  <span class="c1"># ÌñâÎèô Ï†ïÏ±ÖÏóêÏÑú Í∞ÄÏ†∏Ïò¥
</span>        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">rho</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">]</span>
            <span class="n">rho</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">next_state</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="p">[</span><span class="n">next_state</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span>  <span class="c1"># Í∞ÄÏ§ëÏπò rho Í≥ÑÏÇ∞
</span>
        <span class="c1"># rhoÎ°ú TD Î™©Ìëú Î≥¥Ï†ï
</span>        <span class="n">target</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>

        <span class="c1"># Í∞ÅÍ∞ÅÏùò Ï†ïÏ±Ö Í∞úÏÑ†
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span>


<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">SarsaOffPolicyAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">Q</span><span class="p">)</span>
</code></pre></div></div>

<p>start Point is (0, 0)
<br />
end Point is (5, 5)</p>

<div align="center">
  <img src="/images/sarsa.png" alt="bandit1" style="width: 90%;" />
</div>

<div align="center">
  <img src="/images/sarsa2.png" alt="bandit2" style="width: 90%;" />
</div>

<h3 id="q-learning">Q-learning</h3>

<p>Q-learningÏùÄ Í∞ïÌôîÌïôÏäµÏóêÏÑú Í∞ÄÏû• ÎÑêÎ¶¨ Ïì∞Ïù¥Îäî ÏïåÍ≥†Î¶¨Ï¶ò Ï§ë ÌïòÎÇòÎ°ú, off-policy TD Î∞©Î≤ïÏóê ÏÜçÌïúÎã§. ÏóêÏù¥Ï†ÑÌä∏Îäî ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÎ©∞ Q Ìï®ÏàòÎ•º ÏóÖÎç∞Ïù¥Ìä∏ÌïòÏßÄÎßå, ÏóÖÎç∞Ïù¥Ìä∏ ÎåÄÏÉÅÏùÄ Ïã§Ï†úÎ°ú ÏÑ†ÌÉùÌïú ÌñâÎèôÏù¥ ÏïÑÎãå ÎØ∏ÎûòÏóê Í∞ÄÏû• ÎÜíÏùÄ Q Í∞íÏùÑ Í∞ñÎäî ÌñâÎèôÏóê Í∏∞Î∞òÌïúÎã§. Ïù¥Î°ú Ïù∏Ìï¥ Q-learningÏùÄ Ïã§Ï†ú ÌñâÎèôÍ≥ºÎäî Î¨¥Í¥ÄÌïòÍ≤å ÌÉêÏöïÏ†ÅÏù∏(target) Ï†ïÏ±ÖÏùÑ Îî∞Î•¥Îäî ÌïôÏäµÏù¥ Í∞ÄÎä•Ìï¥ÏßÑÎã§.</p>

<p>ÎåÄÌëúÏ†ÅÏù∏ ÌäπÏßïÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§.</p>

<p><code class="language-plaintext highlighter-rouge">TD(Temporal Difference) ÌïôÏäµ</code>: Î∂ÄÌä∏Ïä§Ìä∏ÎûòÌïë Î∞©ÏãùÏúºÎ°ú, ÎØ∏Îûò ÏÉÅÌÉúÏùò Q Í∞íÏùÑ Ïù¥Ïö©Ìï¥ ÌòÑÏû¨ Q Í∞íÏùÑ Ï†êÏßÑÏ†ÅÏúºÎ°ú Í∞±Ïã†ÌïúÎã§.</p>

<p><code class="language-plaintext highlighter-rouge">Off-policy ÌïôÏäµ</code>: Ïã§Ï†ú ÌñâÎèôÏùÄ Œµ-greedy Ï†ïÏ±Ö Îì± ÌÉêÌóòÏ†ÅÏù∏ ÌñâÎèôÏ†ïÏ±ÖÏùÑ Îî∞Î•¥ÏßÄÎßå, ÌïôÏäµÏùÄ Ìï≠ÏÉÅ greedyÌïú ÎåÄÏÉÅÏ†ïÏ±Ö Í∏∞Ï§ÄÏúºÎ°ú Ïù¥Î£®Ïñ¥ÏßÑÎã§.</p>

<p><code class="language-plaintext highlighter-rouge">Ï§ëÏöîÎèÑ ÏÉòÌîåÎßÅ Î∂àÌïÑÏöî</code>: ÎåÄÏÉÅÏ†ïÏ±ÖÏù¥ Ìï≠ÏÉÅ greedyÌïòÎØÄÎ°ú, ÌñâÎèôÏ†ïÏ±ÖÍ≥ºÏùò Ï∞®Ïù¥Î•º Î≥¥Ï†ïÌï† ÌïÑÏöîÍ∞Ä ÏóÜÎã§. Îî∞ÎùºÏÑú Ï§ëÏöîÎèÑ ÎπÑÏú®(importance sampling ratio)ÏùÑ Í≥ÑÏÇ∞ÌïòÏßÄ ÏïäÎäîÎã§.</p>

<p>Q-learningÏùò ÌïµÏã¨ ÏóÖÎç∞Ïù¥Ìä∏ ÏãùÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§:</p>

<div style="overflow-x: auto;"> \[ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( R_t + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right) \] </div>

<p>Í∞ÑÎã®Ìûà ÎßêÌï¥ÏÑú, ÏßÄÍ∏à¬†Ï†êÏàò‚ÜêÏßÄÍ∏à¬†Ï†êÏàò+(ÏßÑÏßú¬†Ï†êÏàò¬†+¬†ÏòàÏ∏°¬†Ï†êÏàò‚àíÏßÄÍ∏à¬†Ï†êÏàò) Ïù¥Îã§.</p>

<ul>
  <li>
    <p>R_t: ÏßÄÍ∏à Î≥¥ÏÉÅ (ÏÉòÌîåÎßÅÎêú Í±∞!)</p>
  </li>
  <li>
    <p>Q(s‚Äô, a‚Äô): Îã§Ïùå ÏÉÅÌÉúÏóêÏÑú Í∞ÄÎä•Ìïú Î™®Îì† ÌñâÎèôÏóê ÎåÄÌï¥ Í∞ÄÏû• ÎÜíÏùÄ QÍ∞íÏùÑ ÏÑ†ÌÉù (max!)</p>
  </li>
</ul>

<p>Ïù¥ ÏãùÏóêÏÑú ÌïµÏã¨ÏùÄ Îã§Ïùå ÏÉÅÌÉú 
\(S_t+1\)
ÏóêÏÑú Í∞ÄÎä•Ìïú Î™®Îì† ÌñâÎèô Ï§ë Í∞ÄÏû• ÌÅ∞ Q Í∞íÏùÑ ÏÑ†ÌÉùÌïòÏó¨, Í∑∏Í≤ÉÏùÑ Î™©Ìëú Í∞í(target)ÏúºÎ°ú ÏÇ¨Ïö©ÌïúÎã§Îäî Ï†êÏù¥Îã§. Ï¶â, Ïã§Ï†ú ÏàòÌñâÌïú 
\(A_t+1\)
ÏôÄ Î¨¥Í¥ÄÌïòÍ≤å, greedyÌïú Í∞ÄÏπòÎ•º Í∏∞Ï§ÄÏúºÎ°ú ÌòÑÏû¨ Q Í∞íÏùÑ ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎäî Í≤ÉÏù¥Îã§.</p>

<p>Ïù¥Îü¨Ìïú Î∞©ÏãùÏùÄ ÌïôÏäµ ÏïàÏ†ïÏÑ±Í≥º ÏàòÎ†¥ Ïù¥Î°† Ï∏°Î©¥ÏóêÏÑú Ïú†Î¶¨ÌïòÎã§. ÌäπÌûà, ÏàòÎ†¥ Ï°∞Í±¥Ïù¥ Ïûò Ï†ïÏùòÎêòÏñ¥ ÏûàÏúºÎ©∞, Ï∂©Î∂ÑÌïú ÌÉêÌóòÍ≥º Ï†ÅÏ†àÌïú ÌïôÏäµÎ•† ÌïòÏóêÏÑú ÏµúÏ†ÅÏùò Q Ìï®ÏàòÎ°ú ÏàòÎ†¥Ìï®Ïù¥ Ï¶ùÎ™ÖÎêòÏñ¥ ÏûàÎã§. Îã§Îßå, ÌÉêÌóòÏùÑ ÏúÑÌïú Œµ-greedy Ï†ïÏ±Ö Îì± Î≥ÑÎèÑÏùò ÏàòÏßë Ï†ÑÎûµÏù¥ ÌïÑÏöîÌïòÎ©∞, Ï¥àÍ∏∞Ïóê ÏûòÎ™ªÎêú Q Í∞íÏù¥ Í≥†Ï∞©ÎêòÎäî Î¨∏Ï†úÎèÑ ÏûàÏùÑ Ïàò ÏûàÎã§. Ïù¥Î•º Î≥¥ÏôÑÌïòÍ∏∞ ÏúÑÌï¥ Double Q-learning, DQN, Prioritized Experience Replay Îì±Ïùò Îã§ÏñëÌïú ÌôïÏû• Í∏∞Î≤ïÏù¥ Ï°¥Ïû¨ÌïúÎã§.</p>

<h2 id="Ïã†Í≤ΩÎßùÍ≥º-q-learning">Ïã†Í≤ΩÎßùÍ≥º Q-learning</h2>

<p>MCÎ≤ïÏùÄ</p>

\[V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]\]

<p>ÏôÄ Í∞ôÏùÄ ÌòïÌÉúÎ°ú ÏÉÅÌÉú Í∞ÄÏπò Ìï®ÏàòÎ•º Í∞±Ïã†ÌïòÎäîÎç∞, Ïù¥Îïå Ï†ÑÏ≤¥ ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇòÏïº \(G_t\)Î•º Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Î∞òÎìúÏãú ÏóêÌîºÏÜåÎìúÍ∞Ä Ï¢ÖÎ£åÎêú Ïù¥ÌõÑÏóêÎßå ÏóÖÎç∞Ïù¥Ìä∏Í∞Ä Í∞ÄÎä•ÌñàÎã§. ÌïòÏßÄÎßå Temporal-Difference(TD) Î∞©ÏãùÏùÄ ÌòÑÏû¨ ÏÉÅÌÉúÏóêÏÑú Î∞õÏùÄ Î≥¥ÏÉÅÍ≥º Îã§Ïùå ÏÉÅÌÉúÏóêÏÑú ÏòàÏ∏°ÎêòÎäî Í∞ÄÏπòÏùò Ìï©ÏùÑ Ïù¥Ïö©Ìï¥ Ìïú Ïä§ÌÖù Îã®ÏúÑÎ°ú ÏóÖÎç∞Ïù¥Ìä∏Ìï† Ïàò ÏûàÎã§. Ïù¥Îïå ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ TD Ïò§Ï∞®Ïù¥Î©∞, Ïù¥Îäî</p>

<div style="overflow-x: auto;"> 
$$
\delta = r + \gamma V(s_{t+1}) - V(s_t)
$$
</div>

<p>ÏôÄ Í∞ôÏù¥ Ï†ïÏùòÎêúÎã§. Ïù¥Îü¨Ìïú ÏõêÎ¶¨Î•º ÌñâÎèô Í∞ÄÏπò Ìï®Ïàò \(Q(s, a)\)Ïóê Ï†ÅÏö©Ìïú Í≤ÉÏù¥ Q-learningÏù¥Îã§.</p>

<p>Q-learningÏóêÏÑúÎäî Îã®ÏàúÌûà Îã§Ïùå ÏÉÅÌÉúÏùò Í∞ÄÏπò \(V(s_{t+1})\)Î•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ ÏïÑÎãàÎùº, Îã§Ïùå ÏÉÅÌÉúÏóêÏÑú Í∞ÄÎä•Ìïú Î™®Îì† ÌñâÎèô Ï§ë Í∞ÄÏû• ÎÜíÏùÄ QÍ∞íÏùÑ Í∞ÄÏßÑ ÌñâÎèôÏùÑ Ï∑®ÌïúÎã§Í≥† Í∞ÄÏ†ïÌïòÍ≥† ÏóÖÎç∞Ïù¥Ìä∏Î•º ÏàòÌñâÌïúÎã§. Ï¶â,</p>

<div style="overflow-x: auto;">
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
$$
</div>

<p>ÌòïÌÉúÎ°ú Ïù¥Î£®Ïñ¥ÏßÑÎã§. Ïù¥Îïå Ï§ëÏöîÌïú Ï†êÏùÄ, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ïã§Ï†úÎ°ú \(\max_{a'}\)Ïóê Ìï¥ÎãπÌïòÎäî ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÏßÄ ÏïäÏïòÎçîÎùºÎèÑ, ÎßàÏπò Í∑∏ ÌñâÎèôÏùÑ ÌñàÎã§Í≥† Í∞ÄÏ†ïÌïòÍ≥† QÍ∞íÏùÑ Í∞±Ïã†ÌïúÎã§Îäî Í≤ÉÏù¥Îã§. Í∑∏ÎûòÏÑú Q-learningÏùÄ on-policy Î∞©ÏãùÏù¥ ÏïÑÎãàÎùº, Ïã§Ï†ú ÌñâÎèôÍ≥ºÎäî Î¨¥Í¥ÄÌïòÍ≤å ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Îî∞Î•¥Îäî Í≤ÉÏ≤òÎüº ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎäî off-policy ÌïôÏäµ Î∞©Î≤ïÏóê Ìï¥ÎãπÌïúÎã§.</p>

<p>Í≤∞Íµ≠ Q-learningÏùÄ TD Î∞©ÏãùÍ≥º ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú TD Ïò§Ï∞®Î•º Í∏∞Î∞òÏúºÎ°ú ÌïòÎ©∞, ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇòÍ∏∞ Ï†ÑÏóêÎèÑ Ïã§ÏãúÍ∞ÑÏúºÎ°ú Í∞íÏùÑ Í∞±Ïã†Ìï† Ïàò ÏûàÎã§Îäî Í≥µÌÜµÏ†êÏùÑ Í∞ÄÏßÄÏßÄÎßå, Í∞ÄÏπò Ï∂îÏ†ï ÏãúÏóê ÌòÑÏû¨Ïùò Ï†ïÏ±ÖÏù¥ ÏïÑÎãå ÏµúÏ†Å Ï†ïÏ±ÖÏùÑ Îî∞Î•¥Îäî Í≤ÉÏ≤òÎüº Í∞ÄÏ†ïÌïúÎã§Îäî Ï†êÏóêÏÑú Ï∞®Ïù¥Í∞Ä ÏûàÎã§.</p>

<h3 id="Í∏∞Ï°¥-dqn">Í∏∞Ï°¥ DQN</h3>

<p>ÏóÖÎç∞Ïù¥Ìä∏ ÌÉÄÍ≤ü:</p>

<div style="overflow-x: auto;">
$$
y = r + \gamma \max_{a'} Q_{\text{target}}(s', a')
$$
</div>

<ul>
  <li>Îã§Ïùå ÏÉÅÌÉú \(s'\)ÏóêÏÑú <strong>Í∞ÄÏû• ÌÅ∞ QÍ∞í</strong>ÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©</li>
  <li>ÏÑ†ÌÉùÍ≥º ÌèâÍ∞ÄÍ∞Ä <strong>Í∞ôÏùÄ ÎÑ§Ìä∏ÏõåÌÅ¨</strong>ÏóêÏÑú Ïù¥Î§ÑÏßê ‚Üí Í≥ºÎåÄÏ∂îÏ†ï Í∞ÄÎä•ÏÑ±</li>
</ul>

<hr />

<h3 id="double-dqn">Double DQN</h3>

<ol>
  <li>Îã§Ïùå ÌñâÎèô ÏÑ†ÌÉù:</li>
</ol>

<div style="overflow-x: auto;">
$$
a^* = \arg\max_{a'} Q_{\text{online}}(s', a')
$$
</div>

<ol>
  <li>Í∑∏ ÌñâÎèôÏùò Í∞í ÌèâÍ∞Ä:</li>
</ol>

<div style="overflow-x: auto;">
$$
y = r + \gamma Q_{\text{target}}(s', a^*)
$$
</div>

<ul>
  <li><strong>online ÎÑ§Ìä∏ÏõåÌÅ¨</strong>Î°ú ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÍ≥†,</li>
  <li><strong>target ÎÑ§Ìä∏ÏõåÌÅ¨</strong>Î°ú Í∑∏ Í∞íÏùÑ ÌèâÍ∞Ä</li>
  <li>‚Üí ÏÑ†ÌÉùÍ≥º ÌèâÍ∞ÄÎ•º <strong>Î∂ÑÎ¶¨</strong>Ìï¥ÏÑú Í≥ºÎåÄÏ∂îÏ†ï Î∞©ÏßÄ</li>
</ul>

<hr />

<p>Í∏∞Ï°¥ DQNÏù¥ Í≥ºÏû•Îêú QÍ∞íÏùÑ ÌïôÏäµÌïòÎäî Ïù¥Ïú†</p>

<p>Í∏∞Ï°¥ DQN (ÎòêÎäî Q-learning)ÏùÄ Îã§ÏùåÍ≥º Í∞ôÏùÄ Î∞©ÏãùÏúºÎ°ú QÍ∞íÏùÑ ÏóÖÎç∞Ïù¥Ìä∏:</p>

<div style="overflow-x: auto;">
$$
Q(s, a) \leftarrow r + \gamma \max_{a'} Q(s', a')
$$
</div>

<p>Ïó¨Í∏∞ÏÑú Î¨∏Ï†úÍ∞Ä Î∞úÏÉù.</p>

<ul>
  <li>\(Q(s', a')\) Í∞íÎì§ÏùÄ ÏïÑÏßÅ ÌïôÏäµÏù¥ Îçú Îêú, <strong>Î∂ÄÏ†ïÌôïÌïú Í∞í</strong></li>
  <li>Í∑∏Îü∞Îç∞ \(\max\) Ïó∞ÏÇ∞ÏùÑ Ïì∞Î©¥,</li>
  <li>Ïö¥ Ï¢ãÍ≤å <strong>Ïö∞Ïó∞Ìûà ÎÜíÏùÄ Í∞í</strong>Ïù¥ ÏÑ†ÌÉùÎê† Ïàò ÏûàÏùå.</li>
  <li>Í∑∏Îü¨Î©¥ Í∑∏ ÎÜíÏùÄ Í∞íÏùÑ <strong>ÏßÑÏßú Ï†ïÎãµÏù∏ Ï§Ñ ÏïåÍ≥† ÌïôÏäµ</strong>Ìï®.</li>
</ul>

<p><strong>Í≤∞Í≥ºÏ†ÅÏúºÎ°ú</strong>:</p>

<blockquote>
  <p>Q-networkÎäî Ïã§Ï†úÎ≥¥Îã§ <strong>Í≥ºÏû•Îêú QÍ∞í (Overestimated Q-value)</strong> ÏùÑ Í≥ÑÏÜç Î∞∞Ïö∞Í≤å Îê®</p>
</blockquote>

<p>Ïù¥ ÌòÑÏÉÅÏùÑ <strong>overestimation bias (Í≥ºÎåÄ Ï∂îÏ†ï Ìé∏Ìñ•)</strong> Ïù¥ÎùºÍ≥† Î∂ÄÎ¶ÖÎãàÎã§.</p>

<h3 id="prioritized-experience-replay-per">Prioritized Experience Replay (PER)</h3>

<h4 id="1-Í∞úÎÖê-ÏöîÏïΩ">1. Í∞úÎÖê ÏöîÏïΩ</h4>

<p>Prioritized Experience ReplayÎäî Ï§ëÏöîÎèÑÍ∞Ä ÎÜíÏùÄ Í≤ΩÌóòÏùÑ Îçî ÏûêÏ£º ÌïôÏäµÏóê ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏÉòÌîåÎßÅ ÌôïÎ•†ÏùÑ Ï°∞Ï†àÌïòÎäî Í∏∞Î≤ïÏù¥Îã§.<br />
Í∏∞Ï°¥ Experience ReplayÏóêÏÑúÎäî Î™®Îì† Í≤ΩÌóòÏùÑ ÎèôÏùºÌïú ÌôïÎ•†Î°ú ÏÉòÌîåÎßÅÌïòÏßÄÎßå, PERÏùÄ ÌïôÏäµÏóê Îçî Ïú†ÏùµÌïú Í≤ΩÌóòÏùÑ Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú ÌïôÏäµÌïúÎã§.</p>

<hr />

<h4 id="2-Í∏∞Ï°¥-experience-replayÏùò-ÌïúÍ≥Ñ">2. Í∏∞Ï°¥ Experience ReplayÏùò ÌïúÍ≥Ñ</h4>

<ul>
  <li>ÌôòÍ≤ΩÏóêÏÑú ÏàòÏßëÌïú Í≤ΩÌóò $(s, a, r, s‚Äô)$ÏùÑ Î≤ÑÌçºÏóê Ï†ÄÏû•ÌïúÎã§.</li>
  <li>ÌïôÏäµ Ïãú, Î≤ÑÌçºÏóêÏÑú <strong>Î¨¥ÏûëÏúÑÎ°ú</strong> ÏÉòÌîåÎßÅÌïúÎã§.</li>
  <li>Í∑∏Îü¨ÎÇò Î™®Îì† Í≤ΩÌóòÏù¥ ÌïôÏäµÏóê ÎòëÍ∞ôÏù¥ Ïú†ÏùµÌïú Í≤ÉÏùÄ ÏïÑÎãàÎã§.</li>
  <li>Ïñ¥Îñ§ Í≤ΩÌóòÏùÄ Îß§Ïö∞ Ï§ëÏöîÌïú Î∞òÎ©¥, Ïñ¥Îñ§ Í≤ΩÌóòÏùÄ Í±∞Ïùò Ïì∏Î™®ÏóÜÏùÑ Ïàò ÏûàÎã§.</li>
</ul>

<hr />

<h4 id="3-perÏùò-ÌïµÏã¨-ÏïÑÏù¥ÎîîÏñ¥">3. PERÏùò ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥</h4>

<ul>
  <li>Í≤ΩÌóòÎßàÎã§ <strong>Ïö∞ÏÑ†ÏàúÏúÑ(priority)</strong> Î•º Î∂ÄÏó¨ÌïòÍ≥†,</li>
  <li>Ïö∞ÏÑ†ÏàúÏúÑÍ∞Ä ÎÜíÏùÑÏàòÎ°ù <strong>ÏÉòÌîåÎßÅ ÌôïÎ•†ÏùÑ ÎÜíÏù∏Îã§</strong>.</li>
</ul>

<hr />

<h4 id="4-td-errorÎ•º-Ïù¥Ïö©Ìïú-Ïö∞ÏÑ†ÏàúÏúÑ-Ï†ïÏùò">4. TD-errorÎ•º Ïù¥Ïö©Ìïú Ïö∞ÏÑ†ÏàúÏúÑ Ï†ïÏùò</h4>

<p>Í≤ΩÌóòÏùò Ïö∞ÏÑ†ÏàúÏúÑÎäî ÏùºÎ∞òÏ†ÅÏúºÎ°ú TD-errorÎ•º Í∏∞Î∞òÏúºÎ°ú Í≥ÑÏÇ∞ÌïúÎã§.</p>

<div style="overflow-x: auto;">
$$
\delta = \left| r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right|
$$
</div>

<ul>
  <li>TD-errorÍ∞Ä ÌÅ¨Îã§Îäî Í≤ÉÏùÄ Q Ìï®ÏàòÍ∞Ä Í∑∏ Í≤ΩÌóòÏóê ÎåÄÌï¥ ÏûòÎ™ª ÏòàÏ∏°ÌñàÎã§Îäî ÏùòÎØ∏Ïù¥Îã§.</li>
  <li>Îî∞ÎùºÏÑú ÌïôÏäµÏùÑ ÌÜµÌï¥ Í∞úÏÑ†Ìï† Ïó¨ÏßÄÍ∞Ä ÌÅ¨ÎØÄÎ°ú, Îçî ÏûêÏ£º ÏÉòÌîåÎßÅÌï† Í∞ÄÏπòÍ∞Ä ÏûàÎã§.</li>
</ul>

<hr />

<h4 id="5-Ï§ëÏöîÎèÑ-Î≥¥Ï†ï-importance-sampling">5. Ï§ëÏöîÎèÑ Î≥¥Ï†ï (Importance Sampling)</h4>

<p>ÏÉòÌîåÎßÅ ÌôïÎ•†Ïù¥ Í∑†Îì±ÌïòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê ÌïôÏäµÏù¥ Ìé∏Ìñ•Îê† Ïàò ÏûàÎã§.<br />
Ïù¥Î•º Î≥¥Ï†ïÌïòÍ∏∞ ÏúÑÌï¥ Importance Sampling weightÎ•º Ï†ÅÏö©ÌïúÎã§.</p>

<div style="overflow-x: auto;">
$$
w_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^\beta
$$
</div>

<ul>
  <li>\(P(i)\): transition \(i\)Ïùò ÏÉòÌîåÎßÅ ÌôïÎ•†</li>
  <li>\(N\): Ï†ÑÏ≤¥ ÏÉòÌîå Ïàò</li>
  <li>\(\beta\): Î≥¥Ï†ï Í∞ïÎèÑ. Ï¥àÍ∏∞ÏóêÎäî ÏûëÍ≤å ÏãúÏûëÌïòÍ≥† ÌïôÏäµÏù¥ ÏßÑÌñâÎêòÎ©∞ Ï†êÏ∞® \(1\)Î°ú Ï¶ùÍ∞ÄÏãúÌÇ®Îã§.</li>
</ul>

<p>Ïù¥ weightÎäî lossÏóê Í≥±Ìï¥ÏÑú ÌïôÏäµ Ïãú Î∞òÏòÅÌïúÎã§.</p>

<hr />

<h4 id="6-perÏùò-Ï†ÑÏ≤¥-Íµ¨Ï°∞">6. PERÏùò Ï†ÑÏ≤¥ Íµ¨Ï°∞</h4>

<ol>
  <li>Í≤ΩÌóò \((s, a, r, s')\) Ï†ÄÏû• Ïãú, TD-error Í∏∞Î∞òÏúºÎ°ú priorityÎ•º Ï¥àÍ∏∞ÌôîÌïúÎã§.</li>
  <li>ÏÉòÌîåÎßÅ Ïãú, priorityÍ∞Ä ÎÜíÏùÄ transitionÏù¥ ÎΩëÌûê ÌôïÎ•†Ïù¥ Îçî ÎÜíÎèÑÎ°ù ÌïúÎã§.</li>
  <li>ÌïôÏäµ ÌõÑ TD-errorÎ•º Îã§Ïãú Í≥ÑÏÇ∞ÌïòÏó¨ priorityÎ•º Í∞±Ïã†ÌïúÎã§.</li>
  <li>Importance Sampling weightÎ•º ÌÜµÌï¥ Ìé∏Ìñ•ÏùÑ Î≥¥Ï†ïÌïúÎã§.</li>
</ol>

<hr />

<h4 id="8-ÏöîÏïΩ">8. ÏöîÏïΩ</h4>

<p>Prioritized Experience ReplayÎäî TD-errorÎ•º Í∏∞Î∞òÏúºÎ°ú Ï§ëÏöîÌïú Í≤ΩÌóòÏùÑ ÏÑ†Î≥ÑÏ†ÅÏúºÎ°ú ÌïôÏäµÌïòÏó¨ Q Ìï®ÏàòÏùò ÏàòÎ†¥ ÏÜçÎèÑÏôÄ Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏù∏Îã§.<br />
ÌïôÏäµ Ìé∏Ìñ•ÏùÑ Ï§ÑÏù¥Í∏∞ ÏúÑÌï¥ importance sampling Î≥¥Ï†ïÏù¥ ÌïÑÏöîÌïòÎ©∞, ÏùºÎ∞ò Experience ReplayÏóê ÎπÑÌï¥ Íµ¨ÌòÑ Î≥µÏû°ÎèÑÎäî Ï¶ùÍ∞ÄÌïòÏßÄÎßå ÏÑ±Îä• Ìñ•ÏÉÅÏóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎã§.</p>

<h3 id="ÎìÄÏñºÎßÅ-dqndueling-dqn">ÎìÄÏñºÎßÅ DQN(Dueling DQN)</h3>

<h4 id="1-ÌïµÏã¨-ÏïÑÏù¥ÎîîÏñ¥">1. ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥</h4>

<p>Í∏∞Ï°¥ DQNÏùÄ ÏÉÅÌÉú-ÌñâÎèôÏåç QÍ∞íÏùÑ Í≥ßÎ∞îÎ°ú ÏòàÏ∏°ÌïúÎã§:</p>

\[Q(s, a)\]

<p>Í∑∏Îü¨ÎÇò ÎìÄÏñºÎßÅ DQNÏùÄ QÍ∞íÏùÑ Îëê Í∞úÏùò Íµ¨ÏÑ±ÏöîÏÜåÎ°ú ÎÇòÎàÑÏñ¥ ÌëúÌòÑÌïúÎã§:</p>

<ul>
  <li>\(V(s)\): ÏÉÅÌÉú ÏûêÏ≤¥Ïùò Í∞ÄÏπò (Value)</li>
  <li>\(A(s, a)\): ÌäπÏ†ï ÌñâÎèôÏù¥ Í∑∏ ÏÉÅÌÉúÏóêÏÑú ÏñºÎßàÎÇò Îçî ÎÇòÏùÄÏßÄÎ•º ÎÇòÌÉÄÎÇ¥Îäî Ïù¥Ï†ê (Advantage)</li>
</ul>

<p>ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú QÍ∞íÏùÄ Îã§ÏùåÍ≥º Í∞ôÏù¥ Í≥ÑÏÇ∞ÎêúÎã§:</p>

<div style="overflow-x: auto;">
$$
Q(s, a) = V(s) + \left( A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a') \right)
$$
</div>

<p>Ïó¨Í∏∞ÏÑú ÌèâÍ∑†ÏùÑ ÎπºÎäî Ïù¥Ïú†Îäî Advantage Í∞íÎì§Ïù¥ Ï§ëÏã¨ÌôîÎêòÏñ¥Ïïº ÏïàÏ†ïÏ†ÅÏúºÎ°ú ÌïôÏäµÎêòÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§.</p>

<hr />

<h4 id="2-Ïôú-Î∂ÑÎ¶¨ÌïòÎäîÍ∞Ä">2. Ïôú Î∂ÑÎ¶¨ÌïòÎäîÍ∞Ä?</h4>

<p>ÎßéÏùÄ ÏÉÅÌô©ÏóêÏÑú Ïñ¥Îñ§ ÏÉÅÌÉúÎäî Î™ÖÎ∞±Ìûà ‚ÄúÏ¢ãÏùÄ ÏÉÅÌÉú‚ÄùÏù¥ÏßÄÎßå,<br />
Í∑∏ ÏÉÅÌÉú ÎÇ¥ÏóêÏÑúÏùò Í∞Å ÌñâÎèôÎì§ÏùÄ ÎπÑÏä∑Ìïú Í≤∞Í≥ºÎ•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏûàÎã§.</p>

<p>ÏòàÎ•º Îì§Ïñ¥, Í≥µÏ§ëÏóê Îñ† ÏûàÎäî Í≥µÏùÑ Î∞îÎùºÎ≥¥Îäî ÏÉÅÌÉúÏóêÏÑúÎäî<br />
ÏôºÏ™ΩÏúºÎ°ú ÏõÄÏßÅÏù¥Îì†, Ïò§Î•∏Ï™ΩÏúºÎ°ú ÏõÄÏßÅÏù¥Îì† ÌÅ∞ Ï∞®Ïù¥Í∞Ä ÏóÜÎã§.<br />
Ïù¥Îïå Ï§ëÏöîÌïú Í≤ÉÏùÄ ‚ÄúÏÉÅÌÉú ÏûêÏ≤¥Ïùò Í∞ÄÏπò‚ÄùÏù¥ÏßÄ, ‚ÄúÌñâÎèôÏùò Ï∞®Ïù¥‚ÄùÍ∞Ä ÏïÑÎãàÎã§.</p>

<p>DQNÏùÄ Ïù¥Îü¨Ìïú ÏÉÅÌô©ÏóêÏÑú QÍ∞íÏùÑ Ï†úÎåÄÎ°ú Î∂ÑÎ¶¨ÌïòÏßÄ Î™ªÌïòÍ≥†<br />
ÌïôÏäµ ÏÜçÎèÑÍ∞Ä ÎäêÎ†§ÏßÄÍ±∞ÎÇò Î∂àÏïàÏ†ïÌï¥ÏßÄÎäî Í≤ΩÌñ•Ïù¥ ÏûàÎã§.</p>

<hr />

<h4 id="3-ÎÑ§Ìä∏ÏõåÌÅ¨-Íµ¨Ï°∞">3. ÎÑ§Ìä∏ÏõåÌÅ¨ Íµ¨Ï°∞</h4>

<p>ÎìÄÏñºÎßÅ DQNÏùÄ Ï§ëÍ∞ÑÍπåÏßÄ Í≥µÏú†Îêú Ï∏µÏùÑ ÌÜµÍ≥ºÌïú ÌõÑ,<br />
ValueÏôÄ AdvantageÎ•º Í∞ÅÍ∞Å Ï∂úÎ†•ÌïòÎäî Îëê Í∞àÎûò(stream)Î°ú ÎÇòÎâúÎã§.</p>

<h3 id="ÎìÄÏñºÎßÅ-dqndueling-dqn-Í∞úÎÖê-ÏòàÏãúÎ°ú-Ïù¥Ìï¥ÌïòÍ∏∞">ÎìÄÏñºÎßÅ DQN(Dueling DQN) Í∞úÎÖê ÏòàÏãúÎ°ú Ïù¥Ìï¥ÌïòÍ∏∞</h3>

<p>ÎìÄÏñºÎßÅ DQNÏùÄ QÍ∞íÏùÑ Îã§ÏùåÍ≥º Í∞ôÏù¥ Îëê Î∂ÄÎ∂ÑÏúºÎ°ú ÎÇòÎàÑÏñ¥ Í≥ÑÏÇ∞ÌïúÎã§:</p>

<ul>
  <li><strong>ÏÉÅÌÉú Í∞ÄÏπò</strong>: \(V(s)\)</li>
  <li><strong>ÌñâÎèô Ïù¥Ï†ê(advantage)</strong>: \(A(s, a)\)</li>
</ul>

<p>ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú QÍ∞íÏùÄ Îã§ÏùåÍ≥º Í∞ôÏù¥ Í≥ÑÏÇ∞ÎêúÎã§:</p>

<div style="overflow-x: auto;">
$$
Q(s, a) = V(s) + \left( A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a') \right)
$$
</div>

<hr />

<h4 id="ÏòàÏãú-ÏûêÌåêÍ∏∞-ÏïûÏóê-ÏÑ†-ÏÉÅÌô©">ÏòàÏãú: ÏûêÌåêÍ∏∞ ÏïûÏóê ÏÑ† ÏÉÅÌô©</h4>

<p>ÏûêÌåêÍ∏∞ ÏïûÏóê ÏÑú ÏûàÎã§Í≥† Í∞ÄÏ†ïÌïòÏûê. ÏÑ†ÌÉù Í∞ÄÎä•Ìïú ÏùåÎ£åÎäî Îã§ÏùåÍ≥º Í∞ôÎã§:</p>

<ul>
  <li>ÏΩúÎùº</li>
  <li>ÏÇ¨Ïù¥Îã§</li>
  <li>Ïª§Ìîº</li>
</ul>

<h4 id="ÏÉÅÌÉú-Í∞ÄÏπò-vs">ÏÉÅÌÉú Í∞ÄÏπò \(V(s)\)</h4>

<ul>
  <li>ÏßÄÍ∏à ÏûêÌåêÍ∏∞ ÏïûÏóê ÏÑú ÏûàÎã§Îäî Í≤É ÏûêÏ≤¥Í∞Ä <strong>Ï¢ãÏùÄ ÏÉÅÌÉú</strong>Ïù¥Îã§.</li>
  <li>Ïñ¥Îñ§ ÏùåÎ£åÎ•º Í≥†Î•¥Îì† Í∞ÑÏóê Í∞àÏ¶ùÏùÑ Ìï¥ÏÜåÌï† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§.</li>
  <li>Îî∞ÎùºÏÑú \(V(s)\)Îäî ÎÜíÏùÄ Í∞íÏù¥ ÎêúÎã§ (Ïòà: 8.0)</li>
</ul>

<h4 id="ÌñâÎèô-Ïù¥Ï†ê-as-a">ÌñâÎèô Ïù¥Ï†ê \(A(s, a)\)</h4>

<ul>
  <li>ÏΩúÎùºÎ•º ÎàÑÎ•¥Îì†, ÏÇ¨Ïù¥Îã§Î•º ÎàÑÎ•¥Îì†, Ïª§ÌîºÎ•º ÎàÑÎ•¥Îì† <strong>ÌÅ∞ Ï∞®Ïù¥ ÏóÜÏùå</strong></li>
  <li>Ï¶â, <strong>ÌñâÎèôÏùò ÏÉÅÎåÄÏ†Å Ï∞®Ïù¥(advantage)</strong> Îäî ÏûëÎã§:
<br />
\(A(s, \text{ÏΩúÎùº}) = 0.1\)<br />
\(A(s, \text{ÏÇ¨Ïù¥Îã§}) = 0.0\)<br />
\(A(s, \text{Ïª§Ìîº}) = -0.1\)</li>
</ul>

<hr />

<h4 id="ÏöîÏïΩ">ÏöîÏïΩ</h4>

<ul>
  <li>Dueling DQNÏùÄ <strong>ÏÉÅÌÉúÍ∞Ä Ï¢ãÏùÄ Ïù¥Ïú†ÏôÄ ÌñâÎèôÏù¥ Ï¢ãÏùÄ Ïù¥Ïú†Î•º Î∂ÑÎ¶¨</strong>Ìï¥ÏÑú ÌïôÏäµÌïúÎã§.</li>
  <li>ÌäπÌûà <strong>ÌñâÎèô Í∞Ñ Ï∞®Ïù¥Í∞Ä ÌÅ¨ÏßÄ ÏïäÏùÄ ÏÉÅÌô©ÏóêÏÑú Îçî Ìö®Í≥ºÏ†Å</strong>Ïù¥Îã§.</li>
  <li>Ïù¥ Íµ¨Ï°∞Îäî ÏïàÏ†ïÏ†ÅÏù¥Í≥† Îπ†Î•∏ QÍ∞í ÌïôÏäµÏóê ÎèÑÏõÄÏùÑ Ï§ÄÎã§.</li>
</ul>

<h2 id="dqn-implemenatation">DQN implemenatation</h2>

<p>ANN + Q learningÏùÑ Ïù¥Ïö©ÌïòÏó¨ Ïã§Ï†úÎ°ú ÌïôÏäµÏùÑ ÏãúÌñâÌïòÏó¨ Í∞ÄÏû• ÎÜíÏùÄ ÌñâÎèôÎ≥¥ÏÉÅÏùÑ ÏñªÏñ¥Î≥¥Ïûê.</p>

<p>Î®ºÏ†Ä ÌôòÍ≤Ω Íµ¨ÏÑ±ÏùÑ Ìï¥ÏïºÌïúÎã§. Î¨∏Ï†úÏùò ÏòàÏãúÎ°ú Í≥µÌäÄÍ∏∞Í∏∞Í≤åÏûÑ(paddle game)ÏùÑ Í∞ïÌôîÌïôÏäµÎêú aiÎ°ú Í≤åÏûÑÏùÑ ÏãúÌñâÌï¥Î≥¥Ïûê.</p>

<p>Í∞ïÌôîÌïôÏäµÏùÑ ÌïòÍ∏∞ÏúÑÌï¥ÏÑúÎäî ÌôòÍ≤ΩÍµ¨ÏÑ±ÏùÑ Ìï¥ÏïºÌïòÍ≥† ÌôòÍ≤ΩÏùÑ Íµ¨ÌòÑÌï† ÎïåÎäî S, A, R, DÎ•º Í≥†Î†§Ìï¥ÏïºÌïúÎã§.</p>

<p><code class="language-plaintext highlighter-rouge">State</code> = [Í≥µÏùò xÏ¢åÌëú, Í≥µÏùò yÏ¢åÌëú, Í≥µÏùò xÎ≤°ÌÑ∞ ÏÜçÎèÑ, Í≥µÏùò yÎ≤°ÌÑ∞ ÏÜçÎèÑ, Ìå®Îì§Ïùò ÏúÑÏπò x]</p>

<p><code class="language-plaintext highlighter-rouge">Action</code> = [Ìå®Îì§ÏùÑ ÏôºÏ™ΩÏúºÎ°ú Ïù¥Îèô, Ï†ïÏßÄ, Ìå®Îì§ÏùÑ Ïò§Î•∏Ï™ΩÏúºÎ°ú Ïù¥Îèô]</p>

<p><code class="language-plaintext highlighter-rouge">Reward</code> = [Ìå®Îì§Ïóê ÎßûÏïÑÏÑú Í≥µÏù¥ ÌäïÍ∏¥Îã§. +1, Í≥µÏù¥ Îñ®Ïñ¥ÏßÑÎã§. -1]</p>

<p><code class="language-plaintext highlighter-rouge">Done</code> = [Ï¢ÖÎ£åÏ°∞Í±¥ : Í≥µÏù¥ Îñ®Ïñ¥ÏßÄÎäî Í≤ΩÏö∞]</p>

<hr />

<p><code class="language-plaintext highlighter-rouge">reset()</code> = [ ÌôòÍ≤ΩÏùÑ Ï¥àÍ∏∞ÌôîÌïòÍ≥†, ÌïôÏäµ ÏóêÌîºÏÜåÎìúÏùò ÏãúÏûë ÏÉÅÌÉúÎ•º Î∞òÌôòÌïòÎäî Ìï®ÏàòÏù¥Îã§.]</p>

<p><code class="language-plaintext highlighter-rouge">step(action)</code> = [Ìï®ÏàòÎäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏÑ†ÌÉùÌïú ÌñâÎèôÏóê Îî∞Îùº
ÌôòÍ≤ΩÏùÑ Ìïú Ïä§ÌÖù ÏïûÏúºÎ°ú ÏßÑÌñâÏãúÌÇ§Í≥†,
Í≥µÍ≥º Ìå®Îì§Ïùò Î¨ºÎ¶¨Ï†Å ÏõÄÏßÅÏûÑÏùÑ Í≥ÑÏÇ∞ÌïòÎ©∞,
Í∑∏Ïóê Îî∞Î•∏ Î≥¥ÏÉÅÍ≥º Í≤åÏûÑ Ï¢ÖÎ£å Ïó¨Î∂ÄÎ•º ÌåêÎã®Ìï¥Ï§ÄÎã§.]</p>

<p><code class="language-plaintext highlighter-rouge">render()</code> = [ÌôîÎ©¥ ÏãúÍ∞ÅÌôî]</p>

<p>ÏúÑÏùò Ï°∞Í±¥Îì§ÏùÑ Î∞îÌÉïÏúºÎ°ú Ìå®Îì§Í≤åÏûÑÏùò ÌôòÍ≤ΩÍµ¨ÏÑ± ÌÅ¥ÎûòÏä§Î•º Íµ¨ÌòÑÌï¥Î≥¥Ïûê.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pygame</span>

<span class="c1"># PaddleEnv ÌÅ¥ÎûòÏä§: Í≥µ ÌäÄÍ∏∞Í∏∞ ÌôòÍ≤ΩÏùÑ Íµ¨ÌòÑÌïú Í∞ïÌôîÌïôÏäµÏö© ÌôòÍ≤Ω ÌÅ¥ÎûòÏä§
</span><span class="k">class</span> <span class="nc">PaddleEnv</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">300</span><span class="p">):</span>
        <span class="c1"># ÌôîÎ©¥Ïùò ÎÑàÎπÑÏôÄ ÎÜíÏù¥ ÏÑ§Ï†ï
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span>
        <span class="c1"># Í≥µÏùò Î∞òÏßÄÎ¶Ñ
</span>        <span class="n">self</span><span class="p">.</span><span class="n">BR</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="c1"># Ìå®Îì§Ïùò Í∞ÄÎ°ú, ÏÑ∏Î°ú ÌÅ¨Í∏∞
</span>        <span class="n">self</span><span class="p">.</span><span class="n">PW</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">PH</span> <span class="o">=</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">10</span>
        <span class="c1"># ÌôîÎ©¥ Í∞±Ïã† ÌîÑÎ†àÏûÑ ÏÑ§Ï†ï
</span>        <span class="n">self</span><span class="p">.</span><span class="n">FPS</span> <span class="o">=</span> <span class="mi">60</span>
        <span class="c1"># Ìå®Îì§ ÏúÑÏπòÎäî ÌôîÎ©¥ Îß® ÏïÑÎûòÏ™Ω
</span>        <span class="n">self</span><span class="p">.</span><span class="n">paddle_y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">PH</span>
        <span class="c1"># ÌñâÎèô Í≥µÍ∞Ñ: ÏôºÏ™Ω(0), Ï†ïÏßÄ(1), Ïò§Î•∏Ï™Ω(2)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="c1"># ÏÉÅÌÉú Ï∞®Ïõê: Í≥µ ÏúÑÏπò(x, y), Í≥µ ÏÜçÎèÑ(vx, vy), Ìå®Îì§ ÏúÑÏπò(x)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state_dim</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="c1"># Ï¥àÍ∏∞Ìôî
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Í≥µÏùò Ï¥àÍ∏∞ ÏúÑÏπò: xÎäî Î¨¥ÏûëÏúÑ, yÎäî Í≥†Ï†ïÎêú ÏúÑÏ™Ω
</span>        <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">BR</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">BR</span><span class="p">),</span>
            <span class="n">self</span><span class="p">.</span><span class="n">BR</span> <span class="o">+</span> <span class="mi">10</span>
        <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Í≥µÏùò Ï¥àÍ∏∞ ÏÜçÎèÑ Î∞©Ìñ•: -30ÎèÑ ~ +30ÎèÑ Î≤îÏúÑÏóêÏÑú ÎûúÎç§ Í∞ÅÎèÑ
</span>        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">6</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">speed</span> <span class="o">=</span> <span class="mf">4.0</span>  <span class="c1"># Ï¥àÍ∏∞ ÏÜçÎèÑ ÌÅ¨Í∏∞
</span>        <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
            <span class="n">speed</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">),</span>  <span class="c1"># xÎ∞©Ìñ• ÏÜçÎèÑ
</span>            <span class="n">speed</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>   <span class="c1"># yÎ∞©Ìñ• ÏÜçÎèÑ
</span>        <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Ìå®Îì§ÏùÄ Ìï≠ÏÉÅ Í∞ÄÏö¥Îç∞ÏÑú ÏãúÏûë
</span>        <span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">PW</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="c1"># Í≤åÏûÑ ÏÉÅÌÉú: Ï¢ÖÎ£å ÏïÑÎãò
</span>        <span class="n">self</span><span class="p">.</span><span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="c1"># Ï†êÏàò Ï¥àÍ∏∞Ìôî
</span>        <span class="n">self</span><span class="p">.</span><span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_state</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_state</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># ÏÉÅÌÉú Î≤°ÌÑ∞ Î∞òÌôò (Ï†ïÍ∑úÌôîÎêú Í∞íÏúºÎ°ú)
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mf">10.0</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mf">10.0</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span>
        <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="c1"># Ìå®Îì§ Ïù¥Îèô: ÏôºÏ™Ω(0), Ï†ïÏßÄ(1), Ïò§Î•∏Ï™Ω(2)
</span>        <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span> <span class="o">-=</span> <span class="mi">5</span>
        <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span> <span class="o">+=</span> <span class="mi">5</span>
        <span class="c1"># Ìå®Îì§Ïù¥ ÌôîÎ©¥ Î∞ñÏúºÎ°ú ÎÇòÍ∞ÄÏßÄ ÏïäÎèÑÎ°ù Ï†úÌïú
</span>        <span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">PW</span><span class="p">)</span>

        <span class="c1"># Í≥µ ÏúÑÏπò ÏóÖÎç∞Ïù¥Ìä∏
</span>        <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span>

        <span class="c1"># Î≤Ω(Ï¢å/Ïö∞) Ï∂©Îèå Ï≤òÎ¶¨
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">BR</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">if</span> <span class="nf">abs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mf">2.0</span>

        <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">BR</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">BR</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">if</span> <span class="nf">abs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mf">2.0</span>

        <span class="c1"># Ï≤úÏû• Ï∂©Îèå Ï≤òÎ¶¨ (y Î∞©Ìñ• Î∞òÏ†Ñ)
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">BR</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># Î≥¥ÏÉÅ Ï¥àÍ∏∞Ìôî
</span>        <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Ìå®Îì§Í≥º Í≥µÏùò Ï∂©Îèå ÌåêÏ†ï
</span>        <span class="n">ball_x</span><span class="p">,</span> <span class="n">ball_y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span>
        <span class="n">paddle_left</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span>
        <span class="n">paddle_right</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">PW</span>
        <span class="n">paddle_top</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">paddle_y</span>
        <span class="n">paddle_bottom</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">paddle_y</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">PH</span>

        <span class="c1"># Ï∂©Îèå Ï°∞Í±¥ (Í≥µÏù¥ Ìå®Îì§ ÏúÑÏ™ΩÏóê ÎãøÏïòÏùÑ Îïå)
</span>        <span class="n">hit</span> <span class="o">=</span> <span class="p">(</span><span class="n">paddle_left</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">BR</span> <span class="o">&lt;=</span> <span class="n">ball_x</span> <span class="o">&lt;=</span> <span class="n">paddle_right</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">BR</span><span class="p">)</span> <span class="ow">and</span> \
              <span class="p">(</span><span class="n">paddle_top</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">BR</span> <span class="o">&lt;=</span> <span class="n">ball_y</span> <span class="o">&lt;=</span> <span class="n">paddle_bottom</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">hit</span><span class="p">:</span>
            <span class="c1"># Ï∂©Îèå Ïãú Î∞òÏÇ¨Í∞Å Í≥ÑÏÇ∞
</span>            <span class="n">paddle_center</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">paddle_x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">PW</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">ball_x</span> <span class="o">-</span> <span class="n">paddle_center</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">PW</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">max_bounce_angle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">radians</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>
            <span class="n">angle</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">*</span> <span class="n">max_bounce_angle</span>

            <span class="c1"># ÏÜçÎèÑ ÌÅ¨Í∏∞ Ïú†ÏßÄ + Î∞òÏÇ¨Í∞Å Ï†ÅÏö©
</span>            <span class="n">speed</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">speed</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ball_vel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nf">abs</span><span class="p">(</span><span class="n">speed</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">))</span>  <span class="c1"># Î∞òÎìúÏãú ÏúÑÎ°ú ÌäïÍπÄ
</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">score</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Î∞îÎã•Ïóê Îñ®Ïñ¥ÏßÑ Í≤ΩÏö∞ ‚Üí Í≤åÏûÑ Ï¢ÖÎ£å + Î≥¥ÏÉÅ -1
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">done</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_state</span><span class="p">(),</span> <span class="n">reward</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">done</span><span class="p">,</span> <span class="p">{}</span>



<span class="c1"># ÏàòÎèô Ï°∞Ïûë Î™®Îìú (‚Üê / ‚Üí ÌÇ§Î°ú ÌîåÎ†àÏù¥ Í∞ÄÎä•)
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">PaddleEnv</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="n">pygame</span><span class="p">.</span><span class="nf">init</span><span class="p">()</span>
    <span class="n">screen</span> <span class="o">=</span> <span class="n">pygame</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">set_mode</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">W</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">H</span><span class="p">))</span>
    <span class="n">pygame</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">set_caption</span><span class="p">(</span><span class="sh">"</span><span class="s">Paddle Game - Manual Play</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">clock</span> <span class="o">=</span> <span class="n">pygame</span><span class="p">.</span><span class="n">time</span><span class="p">.</span><span class="nc">Clock</span><span class="p">()</span>
    <span class="n">font</span> <span class="o">=</span> <span class="n">pygame</span><span class="p">.</span><span class="n">font</span><span class="p">.</span><span class="nc">SysFont</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>

    <span class="n">running</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">while</span> <span class="n">running</span><span class="p">:</span>
        <span class="n">clock</span><span class="p">.</span><span class="nf">tick</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">FPS</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">pygame</span><span class="p">.</span><span class="n">event</span><span class="p">.</span><span class="nf">get</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">e</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="n">pygame</span><span class="p">.</span><span class="n">QUIT</span><span class="p">:</span>
                <span class="n">running</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="c1"># ÌÇ§Î≥¥Îìú ÏûÖÎ†• Ï≤òÎ¶¨
</span>        <span class="n">keys</span> <span class="o">=</span> <span class="n">pygame</span><span class="p">.</span><span class="n">key</span><span class="p">.</span><span class="nf">get_pressed</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">keys</span><span class="p">[</span><span class="n">pygame</span><span class="p">.</span><span class="n">K_LEFT</span><span class="p">]:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">keys</span><span class="p">[</span><span class="n">pygame</span><span class="p">.</span><span class="n">K_RIGHT</span><span class="p">]:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># ÌôîÎ©¥ Í∑∏Î¶¨Í∏∞
</span>        <span class="n">screen</span><span class="p">.</span><span class="nf">fill</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">pygame</span><span class="p">.</span><span class="n">draw</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">screen</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span> <span class="n">env</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">env</span><span class="p">.</span><span class="n">BR</span><span class="p">)</span>
        <span class="n">pygame</span><span class="p">.</span><span class="n">draw</span><span class="p">.</span><span class="nf">rect</span><span class="p">(</span><span class="n">screen</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span>
                         <span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">paddle_x</span><span class="p">),</span> <span class="n">env</span><span class="p">.</span><span class="n">paddle_y</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">PW</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">PH</span><span class="p">))</span>
        <span class="n">screen</span><span class="p">.</span><span class="nf">blit</span><span class="p">(</span><span class="n">font</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Score: </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">score</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">)),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        <span class="n">pygame</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">flip</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Game Over | Final Score: </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">score</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">pygame</span><span class="p">.</span><span class="n">time</span><span class="p">.</span><span class="nf">wait</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
            <span class="n">running</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="n">pygame</span><span class="p">.</span><span class="nf">quit</span><span class="p">()</span>

</code></pre></div></div>

<h3 id="reset-Ìï®Ïàò-Íµ¨ÏÑ±">reset() Ìï®Ïàò Íµ¨ÏÑ±</h3>
<p>ÌôòÍ≤ΩÏùÑ Ï¥àÍ∏∞ÌôîÌïòÍ≥†, ÌïôÏäµ ÏóêÌîºÏÜåÎìúÏùò ÏãúÏûë ÏÉÅÌÉúÎ•º Î∞òÌôòÌïòÎäî Ìï®ÏàòÏù¥Îã§.</p>

<p><code class="language-plaintext highlighter-rouge">Í≥µ Ï¥àÍ∏∞ ÏúÑÏπò ÏÑ§Ï†ï</code> = [Í≥µÏùò xÏ¢åÌëúÎäî ÌôîÎ©¥ ÎÑàÎπÑ Î≤îÏúÑ ÎÇ¥ÏóêÏÑú Î¨¥ÏûëÏúÑÎ°ú ÏÑ§Ï†ïÎêòÍ≥†, yÏ¢åÌëúÎäî Ìï≠ÏÉÅ ÌôîÎ©¥ ÏÉÅÎã®ÏóêÏÑú ÏãúÏûëÌï®]</p>

<p><code class="language-plaintext highlighter-rouge">Í≥µ Ï¥àÍ∏∞ ÏÜçÎèÑ ÏÑ§Ï†ï</code> = [-30ÎèÑÏóêÏÑú +30ÎèÑ ÏÇ¨Ïù¥Ïùò ÎûúÎç§ Í∞ÅÎèÑÎ°ú Ï¥àÍ∏∞ ÏÜçÎèÑÎ•º ÏÑ§Ï†ïÌïòÎ©∞, Í≥µÏùÄ Ìï≠ÏÉÅ ÏïÑÎûò Î∞©Ìñ•ÏúºÎ°ú ÏõÄÏßÅÏù¥ÎèÑÎ°ù Ìï®]</p>

<p><code class="language-plaintext highlighter-rouge">Ìå®Îì§ Ï¥àÍ∏∞ ÏúÑÏπò ÏÑ§Ï†ï</code> = [Ìå®Îì§ÏùÄ Ìï≠ÏÉÅ ÌôîÎ©¥ ÌïòÎã® Ï§ëÏïôÏóê ÏúÑÏπòÌïòÎèÑÎ°ù Ï¥àÍ∏∞ÌôîÎê®]</p>

<p><code class="language-plaintext highlighter-rouge">Í≤åÏûÑ ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî</code> = [Ï†êÏàò(score)Îäî 0ÏúºÎ°ú, Í≤åÏûÑ Ï¢ÖÎ£å ÏÉÅÌÉú(done)Îäî FalseÎ°ú Ï¥àÍ∏∞ÌôîÎê®]</p>

<p><code class="language-plaintext highlighter-rouge">Ï¥àÍ∏∞ ÏÉÅÌÉú Î∞òÌôò</code> = [ÌòÑÏû¨ Í≥µÍ≥º Ìå®Îì§Ïùò ÏÉÅÌÉúÎ•º Î≤°ÌÑ∞ ÌòïÌÉúÎ°ú Ï†ïÎ¶¨Ìï¥ Î∞òÌôòÌï® ‚Üí <code class="language-plaintext highlighter-rouge">State</code> ÌòïÏãù]</p>

<hr />

<h3 id="stepaction-Ìï®Ïàò-Íµ¨ÏÑ±">step(action) Ìï®Ïàò Íµ¨ÏÑ±</h3>
<p>Ìï®ÏàòÎäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏÑ†ÌÉùÌïú ÌñâÎèôÏóê Îî∞Îùº
ÌôòÍ≤ΩÏùÑ Ìïú Ïä§ÌÖù ÏïûÏúºÎ°ú ÏßÑÌñâÏãúÌÇ§Í≥†,
Í≥µÍ≥º Ìå®Îì§Ïùò Î¨ºÎ¶¨Ï†Å ÏõÄÏßÅÏûÑÏùÑ Í≥ÑÏÇ∞ÌïòÎ©∞,
Í∑∏Ïóê Îî∞Î•∏ Î≥¥ÏÉÅÍ≥º Í≤åÏûÑ Ï¢ÖÎ£å Ïó¨Î∂ÄÎ•º ÌåêÎã®Ìï¥Ï§ÄÎã§.</p>

<p><code class="language-plaintext highlighter-rouge">Ìå®Îì§ Ïù¥Îèô Ï≤òÎ¶¨</code> = [ÏûÖÎ†•Îêú ÌñâÎèô(action)Ïóê Îî∞Îùº Ìå®Îì§ÏùÑ ÏôºÏ™Ω ÎòêÎäî Ïò§Î•∏Ï™ΩÏúºÎ°ú Ïù¥ÎèôÏãúÌÇ§Í≥†, ÌôîÎ©¥ Í≤ΩÍ≥ÑÎ•º ÎÑòÏßÄ ÏïäÎèÑÎ°ù Ï†úÌïúÌï®]</p>

<p><code class="language-plaintext highlighter-rouge">Í≥µ ÏúÑÏπò Í∞±Ïã†</code> = [ÌòÑÏû¨ Í≥µÏùò ÏÜçÎèÑ Î≤°ÌÑ∞Ïóê Îî∞Îùº Í≥µÏùò ÏúÑÏπòÎ•º ÏóÖÎç∞Ïù¥Ìä∏Ìï®]</p>

<p><code class="language-plaintext highlighter-rouge">Î≤Ω Ï∂©Îèå Ï≤òÎ¶¨</code> = [Í≥µÏù¥ Ï¢åÏö∞ Î≤ΩÏóê ÎãøÏúºÎ©¥ xÏ∂ï ÏÜçÎèÑÎ•º Î∞òÏ†ÑÏãúÌÇ§Í≥†, yÏÜçÎèÑÍ∞Ä ÎÑàÎ¨¥ ÏûëÏùÑ Í≤ΩÏö∞ÏóêÎäî ÎÇëÍπÄ Î∞©ÏßÄÎ•º ÏúÑÌï¥ Î≥¥Ï†ïÌï®]</p>

<p><code class="language-plaintext highlighter-rouge">Ï≤úÏû• Ï∂©Îèå Ï≤òÎ¶¨</code> = [Í≥µÏù¥ ÌôîÎ©¥Ïùò ÏÉÅÎã®(Ï≤úÏû•)Ïóê ÎãøÏúºÎ©¥ yÏ∂ï ÏÜçÎèÑÎ•º Î∞òÏ†ÑÏãúÌÇ¥]</p>

<p><code class="language-plaintext highlighter-rouge">Ìå®Îì§ Ï∂©Îèå Ï≤òÎ¶¨</code> = [Í≥µÏù¥ Ìå®Îì§Ïóê ÎãøÏúºÎ©¥ Î∞òÏÇ¨Í∞ÅÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ ÏúÑÏ™ΩÏúºÎ°ú ÌäÄÍ∏∞Í≥†, Î≥¥ÏÉÅ +1ÏùÑ Î∂ÄÏó¨ÌïòÎ©∞ Ï†êÏàòÎ•º 1Ï†ê Ï¶ùÍ∞ÄÏãúÌÇ¥]</p>

<p><code class="language-plaintext highlighter-rouge">Î∞îÎã• Ï∂©Îèå Ï≤òÎ¶¨</code> = [Í≥µÏù¥ Î∞îÎã•Ïóê ÎãøÏúºÎ©¥ Í≤åÏûÑÏù¥ Ï¢ÖÎ£åÎêòÍ≥†, Î≥¥ÏÉÅ -1Ïù¥ Î∂ÄÏó¨ÎêòÎ©∞ done = TrueÎ°ú ÏÑ§Ï†ïÎê®]</p>

<p><code class="language-plaintext highlighter-rouge">Í≤∞Í≥º Î∞òÌôò</code> = [Îã§Ïùå ÏÉÅÌÉú(State), Î≥¥ÏÉÅ(Reward), Ï¢ÖÎ£å Ïó¨Î∂Ä(Done), Ï∂îÍ∞Ä Ï†ïÎ≥¥({})Î•º Ìï®Íªò Î∞òÌôòÌï®]</p>

<hr />

<p>Ïù¥Ï†ú ÌôòÍ≤Ω Íµ¨ÏÑ±ÏùÑ ÎÅùÎÉàÏúºÎãà DQNÏùÑ Íµ¨ÌòÑÌï¥Î≥¥Ïûê. Î®ºÏ†Ä ÌïôÏäµÏóê ÏÇ¨Ïö©Ìï† ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞Î•º ÏÑ§Ï†ïÌï¥Î≥¥Ïûê.</p>

<h3 id="1-ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞-ÏÑ§Ï†ï">1. ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ----- ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞ -----
</span><span class="n">EPISODES</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">MEM_CAPACITY</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">EPS_START</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">EPS_END</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">EPS_DECAY</span> <span class="o">=</span> <span class="mf">0.995</span>
<span class="n">TARGET_UPDATE</span> <span class="o">=</span> <span class="mi">10</span>
</code></pre></div></div>

<p><strong>EPISODES</strong> = [Ï†ÑÏ≤¥ ÌïôÏäµ ÏóêÌîºÏÜåÎìú Ïàò, Ï¥ù Î™á Î≤à Í≤åÏûÑÏùÑ ÌîåÎ†àÏù¥ÌïòÎ©∞ ÌïôÏäµÌï†ÏßÄ ÏÑ§Ï†ïÌï®]</p>

<p><strong>GAMMA</strong> = [Î≥¥ÏÉÅ Ìï†Ïù∏Ïú®, ÎØ∏Îûò Î≥¥ÏÉÅÏùò Ï§ëÏöîÎèÑÎ•º Í≤∞Ï†ïÌï® (0Ïóê Í∞ÄÍπåÏö∏ÏàòÎ°ù Ï¶âÏãú Î≥¥ÏÉÅ Ï§ëÏãú)]</p>

<p><strong>LR</strong> = [ÌïôÏäµÎ•†, Í∞ÄÏ§ëÏπò ÏóÖÎç∞Ïù¥Ìä∏ ÏÜçÎèÑÎ•º Ï°∞Ï†àÌï®]</p>

<p><strong>BATCH_SIZE</strong> = [Q-ÎÑ§Ìä∏ÏõåÌÅ¨Î•º ÌïôÏäµÌï† Îïå ÏÇ¨Ïö©ÌïòÎäî ÎØ∏ÎãàÎ∞∞Ïπò ÌÅ¨Í∏∞]</p>

<p><strong>MEM_CAPACITY</strong> = [Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçºÏóê Ï†ÄÏû•Ìï† Ïàò ÏûàÎäî ÏµúÎåÄ transition Ïàò]</p>

<p><strong>EPS_START</strong> / <strong>EPS_END</strong> / <strong>EPS_DECAY</strong> = [Œµ-greedy ÌÉêÌóò Ï†ÑÎûµÏùò ÏãúÏûëÍ∞í, ÏµúÏÜåÍ∞í, Í∞êÏÜåÏú®]</p>

<p><strong>TARGET_UPDATE</strong> = [ÌÉÄÍ≤ü ÎÑ§Ìä∏ÏõåÌÅ¨Î•º Î™á ÏóêÌîºÏÜåÎìúÎßàÎã§ policy_netÏúºÎ°úÎ∂ÄÌÑ∞ ÎèôÍ∏∞ÌôîÌï†ÏßÄ ÏÑ§Ï†ï]</p>

<hr />

<h3 id="2-q-ÎÑ§Ìä∏ÏõåÌÅ¨-Ï†ïÏùò">2. Q-ÎÑ§Ìä∏ÏõåÌÅ¨ Ï†ïÏùò</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ----- Q-ÎÑ§Ìä∏ÏõåÌÅ¨ Ï†ïÏùò -----
</span>

<span class="k">class</span> <span class="nc">QNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>     <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</code></pre></div></div>

<hr />

<p><strong>ÏûÖÎ†• (state)</strong> = [Í≥µÏùò ÏúÑÏπò/ÏÜçÎèÑ, Ìå®Îì§ ÏúÑÏπò Îì± Ï¥ù 5Ï∞®Ïõê]</p>

<p><strong>Ï∂úÎ†• (QÍ∞í)</strong> = [Í∞Å ÌñâÎèô(ÏôºÏ™Ω, Ï†ïÏßÄ, Ïò§Î•∏Ï™Ω)Ïóê ÎåÄÌïú QÍ∞í (3Ï∞®Ïõê)]</p>

<p><strong>Íµ¨Ï°∞</strong> = [2Í∞úÏùò ÏùÄÎãâÏ∏µ (128‚Üí64) + ReLU ÌôúÏÑ±Ìôî ‚Üí Ï∂úÎ†•Ï∏µ]</p>

<p><strong>Ïó≠Ìï†</strong> = [ÏÉÅÌÉúÎ•º Î∞õÏïÑ Í∞Å ÌñâÎèôÏùò Í∞ÄÏπòÎ•º ÏòàÏ∏°ÌïòÎäî Ìï®Ïàò Í∑ºÏÇ¨Í∏∞ Ïó≠Ìï†]</p>

<h3 id="3-Î¶¨ÌîåÎ†àÏù¥-Î≤ÑÌçº-Íµ¨ÏÑ±">3. Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçº Íµ¨ÏÑ±</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ----- Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçº -----
</span><span class="n">Transition</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="nf">namedtuple</span><span class="p">(</span>
    <span class="sh">'</span><span class="s">Transition</span><span class="sh">'</span><span class="p">,</span> <span class="p">[</span><span class="sh">'</span><span class="s">s</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ns</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">done</span><span class="sh">'</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cap</span><span class="p">):</span> <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">cap</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>   <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">Transition</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">bsize</span><span class="p">):</span> <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">,</span> <span class="n">bsize</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">)</span>

</code></pre></div></div>
<hr />
<p>Í∞ïÌôîÌïôÏäµÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏Îäî ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÎ©∞ ÏÉÅÌÉú(state), ÌñâÎèô(action), Î≥¥ÏÉÅ(reward), Îã§Ïùå ÏÉÅÌÉú(next state)Î•º Î∞òÎ≥µÏ†ÅÏúºÎ°ú Í≤ΩÌóòÌïòÍ≤å ÎêúÎã§. Ïù¥Îü∞ Í≤ΩÌóòÏùÑ Îß§ timestepÎßàÎã§ Î∞îÎ°úÎ∞îÎ°ú ÏÇ¨Ïö©Ìï¥ÏÑú ÌïôÏäµÏóê Î∞òÏòÅÌïòÎäî Î∞©ÏãùÎèÑ ÏûàÏßÄÎßå, Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÌïôÏäµÏù¥ Îß§Ïö∞ Î∂àÏïàÏ†ïÌï¥Ïßà Ïàò ÏûàÎã§. Í∑∏ Ïù¥Ïú†Îäî ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏñªÎäî Í≤ΩÌóòÎì§Ïù¥ ÏÑúÎ°ú Í∞ïÌïòÍ≤å Ïó∞Í¥ÄÎêú ÏàúÏ∞®Ï†ÅÏù∏ Îç∞Ïù¥ÌÑ∞Îì§Ïù¥Í∏∞ ÎïåÎ¨∏Ïù¥Îã§. ÏòàÎ•º Îì§Ïñ¥ Í≥µÏù¥ ÌäÄÍ≥† Ìå®Îì§Ïù¥ ÏõÄÏßÅÏù¥Îäî Ïó∞ÏÜçÎêú Ïû•Î©¥Îì§ÏóêÏÑúÎäî ÎπÑÏä∑Ìïú ÏÉÅÌÉúÍ∞Ä Î∞òÎ≥µÎêòÍ∏∞ ÎïåÎ¨∏Ïóê, Ïù¥Í±∏ Í∑∏ÎåÄÎ°ú ÌïôÏäµÌïòÎ©¥ Î™®Îç∏Ïù¥ ÌäπÏ†ï ÏÉÅÌô©ÏóêÎßå Í≥ºÏ†ÅÌï©ÎêòÍ∏∞ ÏâΩÎã§.</p>

<p>Ïù¥Îü∞ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©ÎêòÎäî Í≤å Î∞îÎ°ú Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçºÎã§. Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçºÎäî Í≥ºÍ±∞Ïùò Í≤ΩÌóòÎì§ÏùÑ Ï†ÄÏû•Ìï¥ÎëêÍ≥†, ÌïôÏäµÌï† ÎïåÎßàÎã§ Í∑∏ Ï§ëÏóêÏÑú Î¨¥ÏûëÏúÑÎ°ú ÏùºÎ∂ÄÎ•º ÏÉòÌîåÎßÅÌï¥ÏÑú ÏÇ¨Ïö©ÌïòÎäî Î∞©ÏãùÏù¥Îã§. Ïù¥ Î∞©ÏãùÏùò ÌïµÏã¨ÏùÄ ÌïôÏäµÏóê ÏÇ¨Ïö©ÎêòÎäî Îç∞Ïù¥ÌÑ∞Îì§Ïù¥ Î¨¥ÏûëÏúÑÌôî(i.i.d. Í∞ÄÏ†ï) ÎêòÎèÑÎ°ù ÎßåÎìúÎäî Îç∞Ïóê ÏûàÎã§. Í∑∏Î†áÍ≤å ÌïòÎ©¥ Îç∞Ïù¥ÌÑ∞ Í∞ÑÏùò ÏÉÅÍ¥ÄÏÑ±ÏùÑ Ï§ÑÏùº Ïàò ÏûàÍ≥†, Îî•Îü¨Îãù Î™®Îç∏Ïù¥ ÏïàÏ†ïÏ†ÅÏúºÎ°ú ÌïôÏäµÎê† Ïàò ÏûàÎã§.</p>

<p>ÎòêÌïú Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçºÎäî Í≥ºÍ±∞Ïùò Í≤ΩÌóòÏùÑ Ïû¨ÌôúÏö©Ìï† Ïàò ÏûàÎã§Îäî Ï†êÏóêÏÑúÎèÑ Ìö®Ïú®Ï†ÅÏù¥Îã§. Ìïú Î≤àÏùò ÏóêÌîºÏÜåÎìúÏóêÏÑú ÏàòÏßëÌïú Îç∞Ïù¥ÌÑ∞Í∞Ä Î∞îÎ°ú ÏÇ¨ÎùºÏßÄÏßÄ ÏïäÍ≥†, Ïó¨Îü¨ Î≤à ÏÉòÌîåÎßÅÎêòÏñ¥ QÍ∞í Í∞±Ïã†Ïóê ÌôúÏö©ÎêòÍ∏∞ ÎïåÎ¨∏Ïóê ÌïôÏäµ Ìö®Ïú®ÎèÑ Ïò¨ÎùºÍ∞ÑÎã§. ÌäπÌûà Î≥¥ÏÉÅÏù¥ ÎìúÎ¨ºÍ≤å Î∞úÏÉùÌïòÎäî ÌôòÍ≤ΩÏóêÏÑúÎäî Ïú†Ïö©Ìïú Í≤ΩÌóòÏù¥ Ï†ÄÏû•ÎêòÏñ¥ ÏûàÎã§Î©¥, Í∑∏Í±∏ Î∞òÎ≥µÌï¥ÏÑú ÌïôÏäµÏóê ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏñ¥ÏÑú Ìõ®Ïî¨ Îπ†Î•¥Í≤å ÌïôÏäµÏù¥ ÏßÑÌñâÎêúÎã§.</p>

<p>Í≤∞Íµ≠ Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçºÎäî DQNÏóêÏÑú Í∞ÄÏû• ÌïµÏã¨Ï†ÅÏù∏ ÏïàÏ†ïÌôî Í∏∞Î≤ï Ï§ë ÌïòÎÇòÎ°ú, Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨Î•º Í∑†ÏùºÌïòÍ≤å ÎßåÎì§Í≥†, ÌïôÏäµÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÏùºÎ∞òÌôîÏãúÌÇ¨ Ïàò ÏûàÎèÑÎ°ù ÎèÑÏôÄÏ£ºÎäî Ï§ëÏöîÌïú Íµ¨ÏÑ±ÏöîÏÜåÎã§.</p>

<p><strong>Transition</strong> = [ÌïòÎÇòÏùò Í≤ΩÌóòÏùÑ (s, a, r, s‚Ä≤, done) ÌäúÌîåÎ°ú Ï†ÄÏû•Ìï®]</p>

<p><strong>ReplayBuffer</strong> = [dequeÎ°ú Íµ¨ÌòÑÎêú Í≥†Ï†ï ÌÅ¨Í∏∞ ÌÅê Íµ¨Ï°∞]</p>

<p><strong>Ïó≠Ìï†</strong> = [Í≤ΩÌóòÏùÑ Î™®ÏïÑ Î¨¥ÏûëÏúÑÎ°ú ÏÉòÌîåÎßÅÌïòÏó¨ i.i.d. ÏÉòÌîåÎ°ú ÌïôÏäµ Í∞ÄÎä•ÌïòÍ≤å Ìï®]</p>

<p><strong>Ïù¥Î°†Ï†Å Ìö®Í≥º</strong> = [ÏÉòÌîå Ìö®Ïú®ÏÑ± Ìñ•ÏÉÅ + ÌïôÏäµ ÏïàÏ†ïÌôî (non-correlated ÏÉòÌîå)]</p>

<h3 id="4-Ï£ºÏöî-Ï¥àÍ∏∞Ìôî-Íµ¨ÏÑ±">4. Ï£ºÏöî Ï¥àÍ∏∞Ìôî Íµ¨ÏÑ±</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">PaddleEnv</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">policy_net</span> <span class="o">=</span> <span class="nc">QNet</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_net</span> <span class="o">=</span> <span class="nc">QNet</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_net</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
    <span class="n">memory</span> <span class="o">=</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="n">MEM_CAPACITY</span><span class="p">)</span>

    <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_START</span>
    <span class="n">rewards_log</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<hr />

<p>Í∞ïÌôîÌïôÏäµ ÌïôÏäµ Î£®ÌîÑÍ∞Ä ÏãúÏûëÎêòÍ∏∞ Ï†ÑÏóê, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌïôÏäµÌï† Ïàò ÏûàÎèÑÎ°ù Ïó¨Îü¨ Íµ¨ÏÑ±ÏöîÏÜåÎì§ÏùÑ Î®ºÏ†Ä Ï¥àÍ∏∞ÌôîÌï¥ÏïºÌïúÎã§.</p>

<p>Î®ºÏ†Ä PaddleEnv()Î•º ÌÜµÌï¥ ÌôòÍ≤ΩÏùÑ ÏÉùÏÑ±ÌïòÍ≥†, ÌïôÏäµÏóê ÏÇ¨Ïö©Ìï† Q-networkÎ•º Îëê Í∞ú ÎßåÎì†Îã§.
ÌïòÎÇòÎäî Ïã§Ï†ú ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï† Îïå ÏÇ¨Ïö©ÎêòÎäî policy_netÏù¥Í≥†,
Îã§Î•∏ ÌïòÎÇòÎäî target QÍ∞íÏùÑ Í≥ÑÏÇ∞Ìï† Îïå ÏÇ¨Ïö©ÎêòÎäî target_netÏù¥Îã§.</p>

<div style="overflow-x: auto;">
$$
Q(s, a) \leftarrow Q(s, a) + \alpha \cdot \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$
</div>

<ul>
  <li><strong>policy_net</strong>ÏùÄ ÌïôÏäµÌïòÎäî Î™®Îç∏</li>
  <li><strong>target_net</strong>ÏùÄ ÌïôÏäµÏùÑ ÏúÑÌïú Í≥†Ï†ïÎêú ÌÉÄÍπÉ ÏÉùÏÑ±Í∏∞</li>
  <li>\(Q(s, a)\)  ‚Üí policy_netÏù¥ ÏòàÏ∏°Ìïú Í∞í</li>
  <li>\(r + \gamma \max_{a'} Q(s', a')\) ‚Üí target_net ÏúºÎ°ú TDÌÉÄÍπÉÍ∞íÏù¥Îã§. ÌòÑÏû¨Ïùò QÍ∞íÏùÑ ÌÉÄÍπÉ Î∞©Ìñ•ÏúºÎ°ú Ï°∞Í∏àÏî© Ïù¥ÎèôÏãúÏºúÏïº ÌïúÎã§.</li>
  <li><strong>TD ÌÉÄÍπÉ(Temporal Difference target)</strong>ÏùÄ Í∞ïÌôîÌïôÏäµÏóêÏÑú ÌòÑÏû¨ ÏÉÅÌÉúÏùò QÍ∞íÏùÑ Ïñ¥ÎñªÍ≤å Í∞±Ïã†Ìï†ÏßÄ Í≤∞Ï†ïÌïòÎäî Í∏∞Ï§ÄÍ∞í.</li>
</ul>

<p>Ï¥àÍ∏∞ÏóêÎäî Îëê ÎÑ§Ìä∏ÏõåÌÅ¨Ïùò Í∞ÄÏ§ëÏπòÍ∞Ä ÎèôÏùºÌïòÍ≤å ÏÑ§Ï†ïÎêòÎ©∞, Ïù¥ÌõÑ ÌïôÏäµÏù¥ ÏßÑÌñâÎêòÎ©¥ policy_netÎßå ÌïôÏäµÎêòÍ≥†,
target_netÏùÄ ÏùºÏ†ï Ï£ºÍ∏∞ÎßàÎã§ policy_netÏùò Í∞ÄÏ§ëÏπòÎ•º Î≥µÏÇ¨Î∞õÍ≤å ÎêúÎã§.
Ïù¥Î†áÍ≤å Ìï®ÏúºÎ°úÏç® Q-learningÏóêÏÑú Î∞úÏÉùÌï† Ïàò ÏûàÎäî Î∂àÏïàÏ†ïÌïú ÌïôÏäµ Î¨∏Ï†úÎ•º ÏôÑÌôîÌïòÍ≥†,
TD ÌÉÄÍπÉÏùò Î∂ÑÏÇ∞ÏùÑ Ï§ÑÏó¨ ÏïàÏ†ïÏ†ÅÏù∏ ÌïôÏäµÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïúÎã§.</p>

<p>ÎòêÌïú, policy_netÏùò ÌååÎùºÎØ∏ÌÑ∞Î•º ÏóÖÎç∞Ïù¥Ìä∏ÌïòÍ∏∞ ÏúÑÌïú ÏòµÌã∞ÎßàÏù¥Ï†ÄÎ°úÎäî AdamÏùÑ ÏÇ¨Ïö©ÌïòÎ©∞,
Í≤ΩÌóòÏùÑ Ï†ÄÏû•ÌïòÍ≥† ÏÉòÌîåÎßÅÌïòÍ∏∞ ÏúÑÌïú ReplayBufferÎèÑ Ìï®Íªò Ï¥àÍ∏∞ÌôîÎêúÎã§.
ÌÉêÌóò-ÌôúÏö© Í∑†ÌòïÏùÑ Ï°∞Ï†àÌïòÎäî Œµ(Ïó°Ïã§Î°†)ÎèÑ Ï¥àÍ∏∞Í∞íÏúºÎ°ú ÏÑ§Ï†ïÌïòÍ≥†,
ÏóêÌîºÏÜåÎìúÎ≥Ñ Ï¥ù Î≥¥ÏÉÅÏùÑ Í∏∞Î°ùÌï† rewards_logÎèÑ Îπà Î¶¨Ïä§Ìä∏Î°ú Ï§ÄÎπÑÌï¥ÎëîÎã§.</p>

<p><strong>env</strong> = [ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏÉÅÌò∏ÏûëÏö©Ìï† Í∞ïÌôîÌïôÏäµ ÌôòÍ≤Ω Í∞ùÏ≤¥ (PaddleEnv)]</p>

<p><strong>policy_net</strong> = [ÌòÑÏû¨ Ï†ïÏ±ÖÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÎäî Q-network]</p>

<p><strong>target_net</strong> = [TD ÌÉÄÍπÉÏùÑ ÏïàÏ†ïÏ†ÅÏúºÎ°ú Í≥ÑÏÇ∞ÌïòÍ∏∞ ÏúÑÌïú Q-network (policy_netÏùò Î≥µÏÇ¨Î≥∏)]</p>

<p><strong>target_net.copy()</strong> = [ÏùºÏ†ï Ï£ºÍ∏∞ÎßàÎã§ policy_netÏùò Í∞ÄÏ§ëÏπòÎ•º Î≥µÏÇ¨ÌïòÏó¨ ÏïàÏ†ïÏÑ± ÌôïÎ≥¥]</p>

<p><strong>optimizer</strong> = [policy_netÏùò ÌååÎùºÎØ∏ÌÑ∞Î•º ÌïôÏäµÏãúÌÇ§Í∏∞ ÏúÑÌïú ÏòµÌã∞ÎßàÏù¥Ï†Ä (Adam ÏÇ¨Ïö©)]</p>

<p><strong>memory</strong> = [transitionÎì§ÏùÑ Ï†ÄÏû•ÌïòÎäî Í≤ΩÌóò Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçº]</p>

<p><strong>eps</strong> = [ÌÉêÌóòÎ•†(Œµ-greedy) Ï¥àÍ∏∞Í∞í, ÌïôÏäµÏù¥ ÏßÑÌñâÎêòÎ©¥ Ï†êÏ†ê Í∞êÏÜåÌï®]</p>

<p><strong>rewards_log</strong> = [ÏóêÌîºÏÜåÎìúÎ≥Ñ ÎàÑÏ†Å Î≥¥ÏÉÅÏùÑ Í∏∞Î°ùÌïòÎäî Î¶¨Ïä§Ìä∏]</p>

<h3 id="5-ÌñâÎèô-ÏÑ†ÌÉù-Î∞è-ÌïôÏäµ-Ìï®Ïàò">5. ÌñâÎèô ÏÑ†ÌÉù Î∞è ÌïôÏäµ Ìï®Ïàò</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="nf">randrange</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">return</span> <span class="nf">policy_net</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>

</code></pre></div></div>

<p>(1) ÌñâÎèô ÏÑ†ÌÉù (choose_action)</p>

<p>Œµ-greedy = [Î¨¥ÏûëÏúÑ ÌÉêÌóòÍ≥º ÏµúÏ†Å ÌñâÎèô ÏÑ†ÌÉùÏùÑ ÏÑûÏñ¥ ÏÇ¨Ïö©Ìï®]</p>

<ul>
  <li>random &lt; Œµ ‚Üí ÎûúÎç§ ÌñâÎèô (ÌÉêÌóò)</li>
  <li>random ‚â• Œµ ‚Üí argmax(Q) ÌñâÎèô ÏÑ†ÌÉù (ÌôúÏö©)</li>
</ul>

<hr />

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">train_step</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">memory</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nc">Transition</span><span class="p">(</span><span class="o">*</span><span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">))</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">s</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">a</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">r</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">ns</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">ns</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">done</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">q_sa</span> <span class="o">=</span> <span class="nf">policy_net</span><span class="p">(</span><span class="n">s</span><span class="p">).</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">q_ns</span> <span class="o">=</span> <span class="nf">target_net</span><span class="p">(</span><span class="n">ns</span><span class="p">).</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">q_ns</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">d</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()(</span><span class="n">q_sa</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<p>(2) ÌïôÏäµ step (train_step)</p>

<p>ÏûÖÎ†• = [Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçºÏóêÏÑú ÏÉòÌîåÎßÅÎêú batch]</p>

<p>Q(s, a) = [ÌòÑÏû¨ Ï†ïÏ±Ö ÎÑ§Ìä∏ÏõåÌÅ¨Í∞Ä ÏòàÏ∏°Ìïú QÍ∞í]</p>

<p>target = [r + Œ≥ * max_a‚Ä≤ Q_target(s‚Ä≤, a‚Ä≤)] ‚Üí DQNÏùò ÌïµÏã¨ ÏóÖÎç∞Ïù¥Ìä∏ ÏàòÏãù Í∏∞Î∞ò</p>

<p>loss = [ÏòàÏ∏°Í∞íÍ≥º ÌÉÄÍπÉÍ∞íÏùò Ï∞®Ïù¥ (MSE)]</p>

<p>Ïó≠Ìï† = [TD Ïò§Ï∞®Î•º Í∏∞Î∞òÏúºÎ°ú Q-functionÏùÑ ÏóÖÎç∞Ïù¥Ìä∏Ìï®]</p>

<h4 id="Í≤ΩÌóò-Í∞úÏàò-ÌôïÏù∏"><strong>Í≤ΩÌóò Í∞úÏàò ÌôïÏù∏</strong></h4>
<hr />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
    <span class="k">return</span>
</code></pre></div></div>
<ul>
  <li>Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçºÏóê ÏÉòÌîåÏù¥ Ï∂©Î∂ÑÌûà ÏåìÏù¥ÏßÄ ÏïäÏïòÏúºÎ©¥ ÌïôÏäµÌïòÏßÄ ÏïäÏùå</li>
</ul>

<h4 id="ÎØ∏ÎãàÎ∞∞Ïπò-ÏÉòÌîåÎßÅ-Î∞è-Î∂ÑÎ¶¨"><strong>ÎØ∏ÎãàÎ∞∞Ïπò ÏÉòÌîåÎßÅ Î∞è Î∂ÑÎ¶¨</strong></h4>
<hr />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch</span> <span class="o">=</span> <span class="n">memory</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="nc">Transition</span><span class="p">(</span><span class="o">*</span><span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">))</span>
</code></pre></div></div>
<ul>
  <li>ÏÉòÌîåÏùÑ BATCH_SIZEÎßåÌÅº Î¨¥ÏûëÏúÑÎ°ú ÏÑ†ÌÉùÌïòÍ≥†, Í∞ÅÍ∞Å s, a, r, s‚Ä≤, doneÏúºÎ°ú Î∂ÑÎ¶¨</li>
</ul>

<h4 id="ÌÖêÏÑúÎ°ú-Î≥ÄÌôò-Î∞è-Ïû•ÏπòÎ°ú-Ïù¥Îèô"><strong>ÌÖêÏÑúÎ°ú Î≥ÄÌôò Î∞è Ïû•ÏπòÎ°ú Ïù¥Îèô</strong></h4>
<hr />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">s</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">a</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">r</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">ns</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">ns</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">done</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>Í∞Å Î∞∞Ïó¥ÏùÑ PyTorch ÌÖêÏÑúÎ°ú Î≥ÄÌôò ÌõÑ GPU ÎòêÎäî CPUÎ°ú Ïù¥Îèô</li>
</ul>

<h4 id="ÌòÑÏû¨-qÍ∞í-ÏòàÏ∏°-ÏòàÏ∏°Í∞í"><strong>ÌòÑÏû¨ QÍ∞í ÏòàÏ∏° (ÏòàÏ∏°Í∞í)</strong></h4>
<hr />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q_sa</span> <span class="o">=</span> <span class="nf">policy_net</span><span class="p">(</span><span class="n">s</span><span class="p">).</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
</code></pre></div></div>
<ul>
  <li>policy_netÏúºÎ°ú ÏÉÅÌÉúÏóê ÎåÄÌïú Î™®Îì† ÌñâÎèôÏùò QÍ∞í ÏòàÏ∏°</li>
  <li>gatherÎ°ú Ïã§Ï†ú Ï∑®Ìïú ÌñâÎèôÏùò QÍ∞íÎßå Ï∂îÏ∂ú</li>
</ul>

<h4 id="td-ÌÉÄÍπÉ-Í≥ÑÏÇ∞-Ï†ïÎãµÍ∞í"><strong>TD ÌÉÄÍπÉ Í≥ÑÏÇ∞ (Ï†ïÎãµÍ∞í)</strong></h4>
<hr />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">q_ns</span> <span class="o">=</span> <span class="nf">target_net</span><span class="p">(</span><span class="n">ns</span><span class="p">).</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">q_ns</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">d</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>target_netÏúºÎ°ú Îã§Ïùå ÏÉÅÌÉúÏùò ÏµúÎåÄ QÍ∞í Í≥ÑÏÇ∞</li>
  <li>TD ÌÉÄÍπÉ Ï†ïÏùò:
  TD ÌÉÄÍπÉ = r + Œ≥ max Q(s‚Äô, a‚Äô)</li>
  <li>ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇ¨Îã§Î©¥ Œ≥ Q(s‚Ä≤, a‚Ä≤)Îäî Î¨¥ÏãúÎê®</li>
</ul>

<h4 id="ÏÜêÏã§-Í≥ÑÏÇ∞"><strong>ÏÜêÏã§ Í≥ÑÏÇ∞</strong></h4>
<hr />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()(</span><span class="n">q_sa</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>ÏòàÏ∏°Í∞íÍ≥º ÌÉÄÍπÉÍ∞í Í∞ÑÏùò MSE ÏÜêÏã§ Í≥ÑÏÇ∞</li>
</ul>

<h4 id="Ïó≠Ï†ÑÌåå-Î∞è-Í∞ÄÏ§ëÏπò-ÏóÖÎç∞Ïù¥Ìä∏"><strong>Ïó≠Ï†ÑÌåå Î∞è Í∞ÄÏ§ëÏπò ÏóÖÎç∞Ïù¥Ìä∏</strong></h4>
<hr />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>
<ul>
  <li>Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ‚Üí Ïó≠Ï†ÑÌåå ‚Üí Í∞ÄÏ§ëÏπò ÏóÖÎç∞Ïù¥Ìä∏ ÏàòÌñâ</li>
</ul>

<h3 id="6-Ï†ÑÏ≤¥-ÌïôÏäµ-Î£®ÌîÑ">6. Ï†ÑÏ≤¥ ÌïôÏäµ Î£®ÌîÑ</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPISODES</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">total_r</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">memory</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">total_r</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="nf">train_step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="n">eps</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">EPS_END</span><span class="p">,</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">EPS_DECAY</span><span class="p">)</span>
    <span class="n">rewards_log</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_r</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ep</span> <span class="o">%</span> <span class="n">TARGET_UPDATE</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">target_net</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>

    <span class="nf">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">EP </span><span class="si">{</span><span class="n">ep</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s"> | Score </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">score</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s"> | TotalR </span><span class="si">{</span><span class="n">total_r</span><span class="si">:</span><span class="mf">4.1</span><span class="n">f</span><span class="si">}</span><span class="s"> | Œµ </span><span class="si">{</span><span class="n">eps</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<p>env.reset() = [ÏÉàÎ°úÏö¥ ÏóêÌîºÏÜåÎìú ÏãúÏûë]</p>

<p>choose_action() = [Œµ-greedyÎ°ú ÌñâÎèô ÏÑ†ÌÉù]</p>

<p>env.step() = [ÌôòÍ≤ΩÏúºÎ°úÎ∂ÄÌÑ∞ Îã§Ïùå ÏÉÅÌÉú, Î≥¥ÏÉÅ, done ÏñªÏùå]</p>

<p>memory.push() = [transition Ï†ÄÏû•]</p>

<p>train_step() = [Q-network ÏóÖÎç∞Ïù¥Ìä∏]</p>

<p>eps Í∞êÏÜå = [ÌÉêÌóòÎ•† Í∞êÏÜå ‚Üí Ï†êÏ†ê ÏµúÏ†Å Ï†ïÏ±ÖÏúºÎ°ú ÏàòÎ†¥]</p>

<p>target_net ÏóÖÎç∞Ïù¥Ìä∏ = [Îß§ TARGET_UPDATE Ï£ºÍ∏∞Î°ú ÎèôÍ∏∞Ìôî]</p>

<p>Î™®Îç∏ Ï†ÄÏû• = [ÌïôÏäµ ÏôÑÎ£åÎêú policy_netÏùÑ .pthÎ°ú Ï†ÄÏû•]</p>

<h4 id="ÌôòÍ≤Ω-Ï¥àÍ∏∞Ìôî-Î∞è-Î≥ÄÏàò-Ï§ÄÎπÑ"><strong>ÌôòÍ≤Ω Ï¥àÍ∏∞Ìôî Î∞è Î≥ÄÏàò Ï§ÄÎπÑ</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">total_r</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>
<ul>
  <li>ÏÉàÎ°úÏö¥ ÏóêÌîºÏÜåÎìú ÏãúÏûë Ïãú ÌôòÍ≤ΩÏùÑ Ï¥àÍ∏∞ÌôîÌïòÍ≥†, ÎàÑÏ†Å Î≥¥ÏÉÅ Ï¥àÍ∏∞Ìôî</li>
</ul>

<h4 id="ÌñâÎèô-ÏÑ†ÌÉù-Î∞è-ÌôòÍ≤ΩÍ≥º-ÏÉÅÌò∏ÏûëÏö©"><strong>ÌñâÎèô ÏÑ†ÌÉù Î∞è ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">action</span> <span class="o">=</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
<span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>Œµ-greedy Î∞©ÏãùÏúºÎ°ú ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÍ≥†, Í∑∏Ïóê Îî∞Îùº ÌôòÍ≤ΩÏùÑ Ìïú Ïä§ÌÖù ÏßÑÌñâ</li>
</ul>

<h4 id="Í≤ΩÌóò-Ï†ÄÏû•"><strong>Í≤ΩÌóò Ï†ÄÏû•</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">memory</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>ÌòÑÏû¨ transitionÏùÑ Î¶¨ÌîåÎ†àÏù¥ Î≤ÑÌçºÏóê Ï†ÄÏû•Ìï®</li>
</ul>

<h4 id="ÏÉÅÌÉú-ÏóÖÎç∞Ïù¥Ìä∏-Î∞è-ÌïôÏäµ-ÏßÑÌñâ"><strong>ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ Î∞è ÌïôÏäµ ÏßÑÌñâ</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
<span class="n">total_r</span> <span class="o">+=</span> <span class="n">reward</span>
<span class="nf">train_step</span><span class="p">()</span>
</code></pre></div></div>
<ul>
  <li>Îã§Ïùå ÏÉÅÌÉúÎ°ú Ïù¥ÎèôÌïòÍ≥†, Î≥¥ÏÉÅÏùÑ ÎàÑÏ†Å</li>
  <li>train_step()ÏùÑ Ìò∏Ï∂úÌïòÏó¨ policy_netÏùÑ Ìïú Î≤à ÌïôÏäµ</li>
</ul>

<h4 id="ÏóêÌîºÏÜåÎìú-Ï¢ÖÎ£å-Ï≤òÎ¶¨"><strong>ÏóêÌîºÏÜåÎìú Ï¢ÖÎ£å Ï≤òÎ¶¨</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">done</span><span class="p">:</span>
    <span class="k">break</span>
</code></pre></div></div>
<ul>
  <li>ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Í≥µÏùÑ ÎÜìÏ≥êÏÑú ÏóêÌîºÏÜåÎìúÍ∞Ä ÎÅùÎÇòÎ©¥ while Î£®ÌîÑÎ•º Ï¢ÖÎ£å</li>
</ul>

<h4 id="ÌÉêÌóòÎ•†-Í∞êÏÜå-Î∞è-Î≥¥ÏÉÅ-Í∏∞Î°ù"><strong>ÌÉêÌóòÎ•† Í∞êÏÜå Î∞è Î≥¥ÏÉÅ Í∏∞Î°ù</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eps</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">EPS_END</span><span class="p">,</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">EPS_DECAY</span><span class="p">)</span>
<span class="n">rewards_log</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_r</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>Œµ Í∞í Í∞êÏÜå: Ï†êÏ†ê Î¨¥ÏûëÏúÑ ÌñâÎèô ÎåÄÏã† ÏòàÏ∏° Í∏∞Î∞ò ÌñâÎèôÏùÑ ÎäòÎ¶º</li>
  <li>Ï¥ù Î≥¥ÏÉÅ Í∏∞Î°ù (ÏóêÌîºÏÜåÎìúÎ≥Ñ ÌïôÏäµ Í≤ΩÌñ• ÌôïÏù∏Ïö©)</li>
</ul>

<h4 id="ÌÉÄÍπÉ-ÎÑ§Ìä∏ÏõåÌÅ¨-ÎèôÍ∏∞Ìôî"><strong>ÌÉÄÍπÉ ÎÑ§Ìä∏ÏõåÌÅ¨ ÎèôÍ∏∞Ìôî</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">ep</span> <span class="o">%</span> <span class="n">TARGET_UPDATE</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">target_net</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>
</code></pre></div></div>
<ul>
  <li>ÏùºÏ†ï Ï£ºÍ∏∞ÎßàÎã§ target_netÏùÑ policy_netÏúºÎ°úÎ∂ÄÌÑ∞ Î≥µÏÇ¨ÌïòÏó¨ ÏïàÏ†ïÏÑ± ÌôïÎ≥¥</li>
</ul>

<h4 id="Î°úÍ∑∏-Ï∂úÎ†•"><strong>Î°úÍ∑∏ Ï∂úÎ†•</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">EP </span><span class="si">{</span><span class="n">ep</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s"> | Score </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">score</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s"> | TotalR </span><span class="si">{</span><span class="n">total_r</span><span class="si">:</span><span class="mf">4.1</span><span class="n">f</span><span class="si">}</span><span class="s"> | Œµ </span><span class="si">{</span><span class="n">eps</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>ÌïôÏäµ Ï§ëÍ∞Ñ Ï§ëÍ∞Ñ ÌòÑÏû¨ ÏóêÌîºÏÜåÎìú Í≤∞Í≥ºÎ•º Ï∂úÎ†•ÌïòÏó¨ ÏßÑÌñâÏÉÅÌô© ÌôïÏù∏</li>
</ul>

<h3 id="7-ÌïôÏäµ-Í≤∞Í≥º-ÏãúÍ∞ÅÌôî">7. ÌïôÏäµ Í≤∞Í≥º ÏãúÍ∞ÅÌôî</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># play_trained_dqn.py
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pygame</span>
<span class="kn">from</span> <span class="n">paddle_env</span> <span class="kn">import</span> <span class="n">PaddleEnv</span>
<span class="kn">from</span> <span class="n">train_dqn</span> <span class="kn">import</span> <span class="n">QNet</span>  <span class="c1"># QNet ÌÅ¥ÎûòÏä§ Ïû¨ÏÇ¨Ïö©
</span>
<span class="c1"># Î™®Îç∏ Î°úÎìú
</span><span class="n">env</span> <span class="o">=</span> <span class="nc">PaddleEnv</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">QNet</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">paddle_dqn_model.pth</span><span class="sh">"</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># ÌñâÎèô ÏÑ†ÌÉù Ìï®Ïàò (Œµ = 0 ‚Üí Ìï≠ÏÉÅ argmax)
</span>

<span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">return</span> <span class="nf">model</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">argmax</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>


<span class="c1"># Í≤åÏûÑ Î£®ÌîÑ
</span><span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">pygame</span><span class="p">.</span><span class="nf">init</span><span class="p">()</span>
<span class="n">screen</span> <span class="o">=</span> <span class="n">pygame</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">set_mode</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">W</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">H</span><span class="p">))</span>
<span class="n">clock</span> <span class="o">=</span> <span class="n">pygame</span><span class="p">.</span><span class="n">time</span><span class="p">.</span><span class="nc">Clock</span><span class="p">()</span>
<span class="n">font</span> <span class="o">=</span> <span class="n">pygame</span><span class="p">.</span><span class="n">font</span><span class="p">.</span><span class="nc">SysFont</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">clock</span><span class="p">.</span><span class="nf">tick</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">FPS</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">pygame</span><span class="p">.</span><span class="n">event</span><span class="p">.</span><span class="nf">get</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">event</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="n">pygame</span><span class="p">.</span><span class="n">QUIT</span><span class="p">:</span>
            <span class="n">pygame</span><span class="p">.</span><span class="nf">quit</span><span class="p">()</span>
            <span class="nf">exit</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="n">screen</span><span class="p">.</span><span class="nf">fill</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">pygame</span><span class="p">.</span><span class="n">draw</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">screen</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span>
                       <span class="n">env</span><span class="p">.</span><span class="n">ball_pos</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">env</span><span class="p">.</span><span class="n">BR</span><span class="p">)</span>
    <span class="n">pygame</span><span class="p">.</span><span class="n">draw</span><span class="p">.</span><span class="nf">rect</span><span class="p">(</span><span class="n">screen</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span>
                     <span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">paddle_x</span><span class="p">),</span> <span class="n">env</span><span class="p">.</span><span class="n">paddle_y</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">PW</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">PH</span><span class="p">))</span>
    <span class="n">screen</span><span class="p">.</span><span class="nf">blit</span><span class="p">(</span><span class="n">font</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Score: </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">score</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">)),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">pygame</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">flip</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">üéÆ ÏµúÏ¢Ö Ï†êÏàò: </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">score</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">pygame</span><span class="p">.</span><span class="n">time</span><span class="p">.</span><span class="nf">wait</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
        <span class="k">break</span>

</code></pre></div></div>

<h2 id="double-dqn-implementation">Double DQN implementation</h2>

<p>Í∏∞Î≥∏ DQN:</p>

<div style="overflow-x: auto;">
$$
\text{TD Target}_{\text{DQN}} = r + \gamma \cdot \max_{a'} Q_{\text{target}}(s', a')
$$
</div>

<p>Double DQN:</p>

<div style="overflow-x: auto;">
$$
\text{TD Target}_{\text{DoubleDQN}} = r + \gamma \cdot Q_{\text{target}}(s', \arg\max_{a'} Q_{\text{policy}}(s', a'))
$$
</div>

<p>‚Üí ÏÑ†ÌÉùÏùÄ <code class="language-plaintext highlighter-rouge">policy_net</code>, ÌèâÍ∞ÄÎäî <code class="language-plaintext highlighter-rouge">target_net</code></p>

<p><code class="language-plaintext highlighter-rouge">Double DQNÏù¥ QÍ∞í Í≥ºÎåÄÏ∂îÏ†ïÏùÑ Ï§ÑÏù¥Îäî Ïù¥Ïú†</code></p>

<p>Í∏∞Î≥∏ DQNÏùÄ Îã§Ïùå ÏÉÅÌÉúÏóêÏÑúÏùò ÌÉÄÍπÉ QÍ∞íÏùÑ Í≥ÑÏÇ∞Ìï† Îïå, max Ïó∞ÏÇ∞ÏùÑ ÌÜµÌï¥ target ÎÑ§Ìä∏ÏõåÌÅ¨ÏóêÏÑú Í∞ÄÏû• ÌÅ∞ QÍ∞íÏùÑ ÏßÅÏ†ë ÏÑ†ÌÉùÌïòÍ≥† Í∑∏ Í∞íÏùÑ ÌÉÄÍπÉÏúºÎ°ú ÏÇ¨Ïö©ÌïúÎã§.</p>

<div style="overflow-x: auto;">
$$
\text{TD Target}_{\text{DQN}} = r + \gamma \cdot \max_{a'} Q_{\text{target}}(s', a')
$$
</div>

<p>Ïù¥ Î∞©ÏãùÏùÄ QÍ∞íÏù¥ Ï†ïÌôïÌï† Í≤ΩÏö∞Ïóî Î¨∏Ï†úÍ∞Ä ÏóÜÏßÄÎßå, Ïã§Ï†úÎ°úÎäî QÍ∞íÏù¥ ÌïôÏäµ Ï¥àÍ∏∞Ïóê Îß§Ïö∞ Î∂ÄÏ†ïÌôïÌï† Ïàò ÏûàÎã§. Ïù¥Îïå max Ïó∞ÏÇ∞ÏùÄ Ïö∞Ïó∞Ìûà ÎÜíÍ≤å Ï∂îÏ†ïÎêú QÍ∞íÏùÑ ÏÑ†ÌÉùÌï† Í∞ÄÎä•ÏÑ±Ïù¥ Îß§Ïö∞ ÎÜíÎã§. Í∑∏Î†áÍ≤å ÎêòÎ©¥, ÏûòÎ™ªÎêú Í∞íÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌïôÏäµÏù¥ Î∞òÎ≥µÎêòÎ©¥ÏÑú QÍ∞íÏù¥ Ï†êÏ†ê Ïã§Ï†úÎ≥¥Îã§ Î∂ÄÌíÄÎ†§ÏßÄÎäî Í≥ºÎåÄÏ∂îÏ†ï(overestimation)Ïù¥ Î∞úÏÉùÌïúÎã§.</p>

<p>Double DQNÏùÄ Ïù¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ ‚ÄòÌñâÎèô ÏÑ†ÌÉù‚ÄôÍ≥º ‚ÄòQÍ∞í ÌèâÍ∞Ä‚ÄôÎ•º Îã§Î•∏ ÎÑ§Ìä∏ÏõåÌÅ¨Î°ú ÎÇòÎààÎã§. ÌñâÎèôÏùÄ policy_netÏúºÎ°ú ÏÑ†ÌÉùÌïòÍ≥†, Í∑∏ ÌñâÎèôÏùò QÍ∞íÏùÄ target_netÏúºÎ°ú ÌèâÍ∞ÄÌïòÎäî Í≤ÉÏù¥Îã§. Ïù¥Îïå ÌÉÄÍπÉÏùÄ Îã§ÏùåÍ≥º Í∞ôÏù¥ Í≥ÑÏÇ∞ÎêúÎã§.</p>

<div style="overflow-x: auto;">
$$
\text{TD Target}_{\text{DoubleDQN}} = r + \gamma \cdot Q_{\text{target}}(s', \arg\max_{a'} Q_{\text{policy}}(s', a'))
$$
</div>

<p>Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÏÑ†ÌÉùÏùÄ ÏµúÏã† ÎÑ§Ìä∏ÏõåÌÅ¨(policy_net)Í∞Ä Îã¥ÎãπÌïòÏßÄÎßå, ÌèâÍ∞Ä Í∏∞Ï§ÄÏùÄ ÏïàÏ†ïÏ†ÅÏù∏ target_netÏùÑ ÏÇ¨Ïö©ÌïòÍ≤å ÎêòÏñ¥, QÍ∞íÏùò ÏÑ†ÌÉùÏù¥ noiseÏóê ÏùòÌï¥ Ïò§ÏóºÎêòÎäî Í≤ÉÏùÑ Ï§ÑÏùº Ïàò ÏûàÎã§.</p>

<p>Double DQNÏù¥ QÍ∞íÏùò Í≥ºÎåÄÏ∂îÏ†ïÏùÑ Ïñ¥ÎñªÍ≤å Ï§ÑÏù¥ÎäîÏßÄÎ•º ÏÑ§Î™ÖÌïòÍ∏∞ ÏúÑÌï¥, ÏòàÏãúÎ•º Îì§Ïñ¥ ÏÑ§Î™ÖÌï† Ïàò ÏûàÎã§.</p>

<p>Ïñ¥Îñ§ ÏÉÅÌÉú s‚Ä≤ÏóêÏÑú ÏÑ∏ Í∞ÄÏßÄ Í∞ÄÎä•Ìïú ÌñâÎèôÏóê ÎåÄÌï¥ Ïã§Ï†ú QÍ∞íÍ≥º policy_net, target_netÏù¥ Í∞ÅÍ∞Å Ï∂îÏ†ïÌïú Í∞íÏù¥ Îã§ÏùåÍ≥º Í∞ôÎã§Í≥† ÌïòÏûê.</p>

<ul>
  <li>ÌñâÎèô A: Ïã§Ï†ú Q = 5.0, policy_net = 5.0, target_net = 5.0</li>
  <li>ÌñâÎèô B: Ïã§Ï†ú Q = 4.5, policy_net = 6.5 (noiseÎ°ú Í≥ºÎåÄ), target_net = 4.5</li>
  <li>ÌñâÎèô C: Ïã§Ï†ú Q = 3.0, policy_net = 3.0, target_net = 3.0</li>
</ul>

<p>Í∏∞Î≥∏ DQNÏùò Í≤ΩÏö∞, ÌÉÄÍπÉ Í∞íÏùÑ Í≥ÑÏÇ∞Ìï† Îïå target_netÏóêÏÑú QÍ∞íÏù¥ Í∞ÄÏû• ÌÅ∞ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïòÍ≥†, Ìï¥Îãπ Í∞íÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©ÌïúÎã§.<br />
ÏúÑÏùò ÏòàÏãúÏóêÏÑúÎäî target_netÏóêÏÑú Í∞ÄÏû• ÎÜíÏùÄ QÍ∞íÏùÄ ÌñâÎèô AÏùò 5.0Ïù¥ÎØÄÎ°ú Îã§ÏùåÍ≥º Í∞ôÏù¥ ÌÉÄÍπÉÏùÑ Í≥ÑÏÇ∞ÌïòÍ≤å ÎêúÎã§.</p>

<p><code class="language-plaintext highlighter-rouge">target_DQN = r + Œ≥ * 5.0</code></p>

<p>Ïù¥ Î∞©ÏãùÏùÄ ÏïàÏ†ÑÌï¥ Î≥¥Ïùº Ïàò ÏûàÏßÄÎßå, ÏÑ†ÌÉùÍ≥º ÌèâÍ∞ÄÍ∞Ä Î™®Îëê target_netÏóê ÏùòÌï¥ Ïù¥Î£®Ïñ¥ÏßÄÎØÄÎ°ú,<br />
target_netÏù¥ Î∂ÄÏ†ïÌôïÌïòÍ±∞ÎÇò Ïû°Ïùå(noise)Ïù¥ Ìè¨Ìï®Îêú ÏÉÅÌÉúÎùºÎ©¥ ÏûòÎ™ªÎêú QÍ∞íÏù¥ ÏÑ†ÌÉùÎêòÏñ¥ ÌïôÏäµÏù¥ Ïò§Ï∞®Î•º Ï∂ïÏ†ÅÌï† Ïàò ÏûàÎã§.</p>

<p>Î∞òÎ©¥, Double DQNÏùÄ QÍ∞íÏù¥ Í∞ÄÏû• ÌÅ¥ Í≤ÉÏúºÎ°ú ‚ÄúÏòàÏ∏°ÎêòÎäî‚Äù ÌñâÎèôÏùÑ policy_netÏù¥ ÏÑ†ÌÉùÌïòÍ≥†,<br />
Í∑∏ ÌñâÎèôÏùò Ïã§Ï†ú QÍ∞íÏùÄ ÏïàÏ†ïÎêú target_netÏúºÎ°úÎ∂ÄÌÑ∞ ÌèâÍ∞ÄÎ∞õÎäîÎã§.</p>

<p>ÏúÑ ÏòàÏãúÏóêÏÑú policy_netÏùÄ ÌñâÎèô BÏùò QÍ∞íÏùÑ 6.5Î°ú Í∞ÄÏû• ÎÜíÍ≤å Ï∂îÏ†ïÌïòÎØÄÎ°ú Ïù¥ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïúÎã§.<br />
ÌïòÏßÄÎßå ÌèâÍ∞Ä Îã®Í≥ÑÏóêÏÑúÎäî target_netÏùò QÍ∞í 4.5Í∞Ä ÏÇ¨Ïö©ÎêòÏñ¥ ÌÉÄÍπÉÏùÑ Í≥ÑÏÇ∞ÌïúÎã§.</p>

<p><code class="language-plaintext highlighter-rouge">target_DoubleDQN = r + Œ≥ * 4.5</code></p>

<p>Ïù¥Ï≤òÎüº, ÌñâÎèô ÏÑ†ÌÉùÏùÄ ÏïÑÏßÅ ÌïôÏäµ Ï§ëÏù∏ policy_netÏù¥ Îã¥ÎãπÌïòÍ≥†,<br />
Í∑∏ ÏÑ†ÌÉùÏóê ÎåÄÌïú QÍ∞í ÌèâÍ∞ÄÎäî ÏÉÅÎåÄÏ†ÅÏúºÎ°ú ÏïàÏ†ïÎêú target_netÏúºÎ°úÎ∂ÄÌÑ∞ Î∞õÏïÑÏò§Í∏∞ ÎïåÎ¨∏Ïóê,<br />
ÏùºÏãúÏ†ÅÏúºÎ°ú Í≥ºÎåÄÌïòÍ≤å Ï∂îÏ†ïÎêú Í∞íÏù¥ Ïã§Ï†ú ÌïôÏäµÏóê Î∞òÏòÅÎêòÏßÄ ÏïäÍ≤å ÎêúÎã§.</p>

<p>Í≤∞Í≥ºÏ†ÅÏúºÎ°ú Double DQNÏùÄ QÍ∞íÏù¥ noiseÏóê ÏùòÌï¥ Î∂ÄÌíÄÎ†§ÏßÑ Í≤ΩÏö∞ÏóêÎèÑ ÏïàÏ†ïÏ†ÅÏù∏ ÌèâÍ∞ÄÎ•º ÌÜµÌï¥ ÌïôÏäµÏùÑ ÏàòÌñâÌïòÍ≤å ÎêòÏñ¥,<br />
Í∏∞Î≥∏ DQNÎ≥¥Îã§ Îçî Î≥¥ÏàòÏ†ÅÏù¥Í≥† ÏïàÏ†ïÏ†ÅÏù∏ ÌïôÏäµÏù¥ Í∞ÄÎä•ÌïòÎ©∞, Í≥ºÎåÄÏ∂îÏ†ï Î¨∏Ï†úÎ•º Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÏôÑÌôîÌï† Ïàò ÏûàÎã§.</p>

<h2 id="lstmÍ≥º-r2d2Íµ¨Ï°∞">LSTMÍ≥º R2D2Íµ¨Ï°∞</h2>

<h3 id="lstmÏù¥ÎûÄ">LSTMÏù¥ÎûÄ?</h3>

<p><strong>LSTM(Long Short-Term Memory)</strong>ÏùÄ Í∏∞Ï°¥ ÏàúÌôòÏã†Í≤ΩÎßù(RNN)Ïùò Îã®Ï†êÏùÑ Î≥¥ÏôÑÌïú Íµ¨Ï°∞Ïù¥Îã§.
ÏùºÎ∞òÏ†ÅÏù∏ RNNÏùÄ Îç∞Ïù¥ÌÑ∞Î•º ÏàúÏÑúÎåÄÎ°ú Ï≤òÎ¶¨ÌïòÎ©¥ÏÑú Í≥ºÍ±∞ Ï†ïÎ≥¥Î•º Îã§Ïùå Îã®Í≥ÑÎ°ú Ï†ÑÎã¨ÌïòÎäî Íµ¨Ï°∞Ïù¥ÏßÄÎßå,
ÏãúÍ∞ÑÏù¥ Í∏∏Ïñ¥ÏßàÏàòÎ°ù Ïò§ÎûòÎêú Ï†ïÎ≥¥Î•º ÏûäÏñ¥Î≤ÑÎ¶¨Îäî Í≤ΩÌñ•Ïù¥ Í∞ïÌï¥ÏßÄÎ©∞, Ïù¥Î°ú Ïù∏Ìï¥ ÌïôÏäµÏù¥ Î∂àÏïàÏ†ïÌï¥ÏßÄÎäî Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌïúÎã§.
Ïù¥Î•º <strong>Ïû•Í∏∞ ÏùòÏ°¥ÏÑ± Î¨∏Ï†ú(long-term dependency)</strong>ÎùºÍ≥† ÌïúÎã§.</p>

<p>LSTMÏùÄ Ïù¥Îü¨Ìïú Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ ÎÇ¥Î∂ÄÏóê <strong>Í∏∞Ïñµ ÏÖÄ(cell state)</strong>Í≥º Í≤åÏù¥Ìä∏(gate) Íµ¨Ï°∞Î•º ÎèÑÏûÖÌïòÏòÄÎã§.
Í≤åÏù¥Ìä∏Îäî ÏûÖÎ†• Í≤åÏù¥Ìä∏, ÎßùÍ∞Å Í≤åÏù¥Ìä∏, Ï∂úÎ†• Í≤åÏù¥Ìä∏Î°ú Íµ¨ÏÑ±ÎêòÎ©∞, Í∞ÅÍ∞ÅÏùò Í≤åÏù¥Ìä∏Îäî ÌòÑÏû¨ ÏûÖÎ†•Í≥º Ïù¥Ï†Ñ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú
Ïñ¥Îñ§ Ï†ïÎ≥¥Î•º Í∏∞ÏñµÌï†ÏßÄ, Ïñ¥Îñ§ Ï†ïÎ≥¥Î•º ÏûäÏùÑÏßÄ, Ïñ¥Îñ§ Ï†ïÎ≥¥Î•º Ï∂úÎ†•Ìï†ÏßÄÎ•º Ï°∞Ï†àÌïúÎã§.
Ïù¥Îü¨Ìïú Íµ¨Ï°∞ ÎçïÎ∂ÑÏóê LSTMÏùÄ Í∏¥ Î¨∏Îß• Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌïú Î¨∏Ï†úÏóêÏÑúÎèÑ ÏïàÏ†ïÏ†ÅÏù∏ ÌïôÏäµÏù¥ Í∞ÄÎä•ÌïòÎã§.</p>

<p>ÏòàÎ•º Îì§Ïñ¥, Í≥µÏù¥ Ïñ¥ÎîîÏóêÏÑú ÌäÄÏñ¥ÎÇòÏôÄ Ï†ÅÏù¥ Ïñ¥ÎñªÍ≤å ÏõÄÏßÅÏù¥ÎäîÏßÄÎ•º ÌåêÎã®Ìï¥Ïïº ÌïòÎäî Í≤åÏûÑÏóêÏÑúÎäî
Ìïú ÏàúÍ∞ÑÏùò ÌôîÎ©¥ Ï†ïÎ≥¥Îßå Í∞ÄÏßÄÍ≥† ÌñâÎèôÏùÑ Ï†ïÌïòÍ∏∞ Ïñ¥Î†µÎã§.
Ïù¥Îïå LSTMÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥, Í≥ºÍ±∞Ïùò ÏõÄÏßÅÏûÑÏùÑ Í∏∞ÏñµÌïòÎ©¥ÏÑú ÌòÑÏû¨Ïùò ÏÉÅÌô©ÏùÑ Ìï¥ÏÑùÌïòÍ≥†, Îçî ÎÇòÏùÄ ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌï† Ïàò ÏûàÎã§.</p>

<div align="center">
  <img src="https://cdn.dida.do/new-project-3-1-1024x607-1024x585.webp" alt="bandit" width="100%" />
</div>

<p>Í≥ºÍ±∞Ïùò Ï†ïÎ≥¥Î•º ÏñºÎßàÎÇò Ïú†ÏßÄÌï†ÏßÄ,<br />
ÏÉàÎ°úÏö¥ Ï†ïÎ≥¥Î•º ÏñºÎßàÎÇò Î∞õÏïÑÎì§ÏùºÏßÄ,<br />
ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Ïñ¥Îñ§ Ï†ïÎ≥¥Î•º Ï∂úÎ†•Ìï†ÏßÄÎ•º Í≤∞Ï†ïÌïòÎäî Í≤åÏù¥Ìä∏ Í∏∞Î∞ò Î©îÏª§ÎãàÏ¶òÏù¥Îã§.</p>

<h4 id="-1-ÏûÖÎ†•-Íµ¨ÏÑ±">üü¢ 1. ÏûÖÎ†• Íµ¨ÏÑ±</h4>
<ul>
  <li>\(x_t\): ÌòÑÏû¨ ÏãúÏ†êÏùò ÏûÖÎ†• (Ïòà: ÌòÑÏû¨ Í¥ÄÏ∞∞)</li>
  <li>\(h_{t-1}\): Ïù¥Ï†Ñ ÏãúÏ†êÏùò Ï∂úÎ†• (hidden state)</li>
  <li>\(C_{t-1}\): Ïù¥Ï†Ñ ÏãúÏ†êÏùò ÏÖÄ ÏÉÅÌÉú (cell state)
‚Üí Ïù¥ ÏÑ∏ Í∞ÄÏßÄÍ∞Ä Î™®Îëê LSTM ÏÖÄ ÎÇ¥Î∂Ä Í≥ÑÏÇ∞Ïóê ÏÇ¨Ïö©ÎêúÎã§.</li>
</ul>

<hr />

<h4 id="-2-forget-gate-ÏûäÏùÑ-Ï†ïÎ≥¥-Í≤∞Ï†ï">üîµ 2. Forget Gate (ÏûäÏùÑ Ï†ïÎ≥¥ Í≤∞Ï†ï)</h4>

<ul>
  <li>
    <p>Í≥ÑÏÇ∞Ïãù:<br />
\(f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\)</p>
  </li>
  <li>
    <p>Ïó≠Ìï†:<br />
Í≥ºÍ±∞ Í∏∞Ïñµ \(C_{t-1}\) ÏóêÏÑú ÏñºÎßàÎÇò ÏßÄÏö∏ÏßÄÎ•º Í≤∞Ï†ïÌïúÎã§.</p>
  </li>
  <li>
    <p>Ìï¥ÏÑù:<br />
\(\\sigma\) Î•º Í±∞Ï≥ê Í≥±ÏÖà(√ó)ÏùÑ ÌïòÍ≤å ÎêòÎ©¥,<br />
Í∞íÏù¥ 0Ïóê Í∞ÄÍπåÏö∞Î©¥ Í∏∞ÏñµÏùÑ ÏßÄÏö∞Í≥†, 1Ïóê Í∞ÄÍπåÏö∞Î©¥ Ïú†ÏßÄÌïúÎã§Îäî ÎúªÏù¥Îã§.</p>
  </li>
</ul>

<hr />

<h4 id="-3-input-gate--ÌõÑÎ≥¥-Í∏∞Ïñµ-ÏÉùÏÑ±">üü° 3. Input Gate + ÌõÑÎ≥¥ Í∏∞Ïñµ ÏÉùÏÑ±</h4>

<ul>
  <li>
    <p>ÏûÖÎ†• Í≤åÏù¥Ìä∏:<br />
\(i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)\)</p>
  </li>
  <li>
    <p>ÌõÑÎ≥¥ ÏÖÄ ÏÉÅÌÉú ÏÉùÏÑ±:<br />
\(\tilde{C}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)\)</p>
  </li>
  <li>
    <p>Ìï¥ÏÑù:<br />
Ïù¥ ÎëòÏùÑ Í≥±Ìï¥ÏÑú ÌòÑÏû¨ ÏãúÏ†êÏóê ÏñºÎßàÎÇò ÏÉàÎ°úÏö¥ Ï†ïÎ≥¥Î•º Ï†ÄÏû•Ìï†ÏßÄÎ•º Í≤∞Ï†ïÌïúÎã§.</p>
  </li>
</ul>

<hr />

<h4 id="-4-ÏÖÄ-ÏÉÅÌÉú-ÏóÖÎç∞Ïù¥Ìä∏">üü† 4. ÏÖÄ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏</h4>

<ul>
  <li>
    <p>ÏóÖÎç∞Ïù¥Ìä∏ Ïãù:<br />
\(C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t\)</p>
  </li>
  <li>
    <p>Ìï¥ÏÑù:<br />
Í≥ºÍ±∞ Í∏∞Ïñµ Ï§ë ÏùºÎ∂ÄÎäî ÏûäÍ≥† (forget),<br />
ÏÉàÎ°úÏö¥ Ï†ïÎ≥¥Î•º ÏùºÎ∂Ä Î∞õÏïÑÎì§Ïó¨ÏÑú (input),<br />
ÏÉàÎ°úÏö¥ ÏÖÄ ÏÉÅÌÉú \(C_t\) Î•º ÎßåÎì†Îã§.</p>
  </li>
</ul>

<hr />

<h4 id="-5-output-gate-Ï∂úÎ†•-Í≤∞Ï†ï">üî¥ 5. Output Gate (Ï∂úÎ†• Í≤∞Ï†ï)</h4>

<ul>
  <li>
    <p>Ï∂úÎ†• Í≤åÏù¥Ìä∏:<br />
\(o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\)</p>
  </li>
  <li>
    <p>ÏµúÏ¢Ö Ï∂úÎ†•(hidden state):<br />
\(h_t = o_t \cdot \tanh(C_t)\)</p>
  </li>
  <li>
    <p>Ìï¥ÏÑù:<br />
ÏÖÄ ÏÉÅÌÉúÎ•º Ï†ÑÏ≤¥ Í∑∏ÎåÄÎ°ú ÎÇ¥Î≥¥ÎÇ¥Îäî Í≤ÉÏù¥ ÏïÑÎãàÎùº,<br />
output gateÎ•º ÌÜµÌï¥ Ï∂úÎ†•Ìï† Ï†ïÎ≥¥Ïùò ÏñëÏùÑ Ï°∞Ï†àÌïúÎã§.</p>
  </li>
</ul>

<h3 id="r2d2ÎûÄ-Î¨¥ÏóáÏù∏Í∞Ä">R2D2ÎûÄ Î¨¥ÏóáÏù∏Í∞Ä?</h3>

<p><strong>R2D2</strong> Îäî DeepMindÏóêÏÑú Ï†úÏïàÌïú Í∞ïÌôîÌïôÏäµ ÏïåÍ≥†Î¶¨Ï¶òÏúºÎ°ú, Í∏∞Ï°¥Ïùò DQN(Deep Q-Network)Ïóê LSTMÏùÑ Í≤∞Ìï©ÌïòÍ≥†, Ïù¥Î•º Í≤ΩÌóò Ïû¨ÏÉù(replay) Í≥º Î∂ÑÏÇ∞ ÌïôÏäµ Íµ¨Ï°∞ÏôÄ Ìï®Íªò ÏÇ¨Ïö©ÌïòÎäî Î∞©ÏãùÏù¥Îã§.</p>

<p>Ïù¥ ÏïåÍ≥†Î¶¨Ï¶òÏùò ÌïµÏã¨ÏùÄ <strong>‚ÄúÍ∏∞Ïñµ‚Äù</strong> Í≥º <strong>‚ÄúÏàúÏ∞®ÏÑ±‚Äù</strong> ÏùÑ Îã§Î£®Îäî Îç∞ ÏûàÎã§.
Í∏∞Ï°¥ DQNÏùÄ ÌïòÎÇòÏùò ÏÉÅÌÉúÎßåÏùÑ Î≥¥Í≥† Ï¶âÍ∞ÅÏ†ÅÏù∏ ÌñâÎèôÏùÑ Í≤∞Ï†ïÌïòÍ∏∞ ÎïåÎ¨∏Ïóê, ÌôòÍ≤ΩÏùò ÏãúÍ∞ÑÏ†Å Îß•ÎùΩ(Í≥ºÍ±∞ Ï†ïÎ≥¥)ÏùÑ Î∞òÏòÅÌïòÎäî Îç∞ ÌïúÍ≥ÑÍ∞Ä ÏûàÎã§.</p>

<p>ÏòàÎ•º Îì§Ïñ¥, ÏïûÏóêÏÑú Ïñ¥Îñ§ ÌñâÎèôÏùÑ ÌñàÎäîÏßÄÎ•º Í≥†Î†§ÌïòÏßÄ ÏïäÍ≥† Îã®ÏàúÌûà ÌòÑÏû¨ ÏÉÅÌÉúÎßå Î≥¥Í≥† ÌåêÎã®ÌïúÎã§Î©¥, ÏãúÍ≥ÑÏó¥ ÌôòÍ≤ΩÏóêÏÑúÎäî Ï§ëÏöîÌïú Ï†ÑÎûµÏùÑ ÎÜìÏπòÍ∏∞ ÏâΩÎã§.</p>

<p>Ïù¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ R2D2Îäî <strong>ÏàúÌôò Ïã†Í≤ΩÎßù(LSTM)</strong>ÏùÑ Q-network ÎÇ¥Î∂ÄÏóê ÌÜµÌï©ÌïúÎã§.
LSTMÏùÄ Í≥ºÍ±∞Ïùò Ï†ïÎ≥¥Î•º hidden stateÎ°ú Ï†ÑÎã¨Î∞õÏïÑ, ÌòÑÏû¨ ÏûÖÎ†•Í≥º Ìï®Íªò Ï≤òÎ¶¨ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê ÏãúÍ∞Ñ ÌùêÎ¶ÑÏóê Îî∞Î•∏ Ï†ïÎ≥¥Ïùò Ïó∞ÏÜçÏÑ±ÏùÑ Ïú†ÏßÄÌï† Ïàò ÏûàÎã§.</p>

<h4 id="1-lstm-Í∏∞Î∞ò-q-network">1. LSTM Í∏∞Î∞ò Q-Network</h4>

<p>Í∏∞Î≥∏ DQNÏùÄ Ïó¨Îü¨ Í∞úÏùò fully connected layerÎ•º Ïù¥Ïö©ÌïòÏßÄÎßå, R2D2Îäî Ï§ëÍ∞ÑÏóê LSTMÏùÑ ÏÇΩÏûÖÌïúÎã§.<br />
Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÏóêÏù¥Ï†ÑÌä∏Îäî Í≥ºÍ±∞ Í¥ÄÏ∞∞Í≥º ÌñâÎèôÏùò Ïó∞ÏÜç ÌùêÎ¶ÑÏùÑ Í∏∞Î∞òÏúºÎ°ú QÍ∞íÏùÑ Ï∂îÎ°†Ìï† Ïàò ÏûàÎã§.<br />
ÏòàÎ•º Îì§Ïñ¥, Í≥µÏù¥ Ïñ¥ÎîîÎ°ú ÌäÄÏóàÎäîÏßÄ, ÏÉÅÎåÄÍ∞Ä ÏßÅÏ†ÑÏóê Ïñ¥Îñ§ ÌñâÎèôÏùÑ ÌñàÎäîÏßÄÎ•º LSTMÏù¥ Í∏∞ÏñµÌïòÍ≥†, Í∑∏Í±∏ Î∞îÌÉïÏúºÎ°ú ÏßÄÍ∏à Ïñ¥Îñ§ ÌñâÎèôÏùÑ Ìï†ÏßÄ ÌåêÎã®ÌïúÎã§.</p>

<hr />

<h4 id="2-recurrent-replay-buffer">2. Recurrent Replay Buffer</h4>

<p>Í∏∞Ï°¥ DQNÏùÄ transition Îã®ÏúÑÎ°ú (s, a, r, s‚Äô, done) Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÏßÄÎßå,<br />
R2D2Îäî Ï†ÑÏ≤¥ ÏãúÌÄÄÏä§ ÎòêÎäî Í≥†Ï†ï Í∏∏Ïù¥Ïùò Ïó∞ÏÜçÎêú transition Î¨∂ÏùåÏùÑ Ï†ÄÏû•ÌïúÎã§.<br />
Ïù¥Ïú†Îäî LSTMÏù¥ Ïù¥Ï†Ñ hidden stateÎ•º Ïú†ÏßÄÌï¥Ïïº ÏùòÎØ∏ ÏûàÎäî Ï∂úÎ†•ÏùÑ ÎÇ¥Í∏∞ ÎïåÎ¨∏Ïù¥Îã§.</p>

<hr />

<h4 id="3-burn-in-Îã®Í≥Ñ">3. Burn-in Îã®Í≥Ñ</h4>

<p>LSTMÏùò ÌïôÏäµÏóêÎäî Ìïú Í∞ÄÏßÄ Ïñ¥Î†§ÏõÄÏù¥ ÏûàÎäîÎç∞, Î∞îÎ°ú ÌïôÏäµ Ï§ëÍ∞ÑÏóê ÏûÑÏùòÏùò ÏãúÏ†êÏóêÏÑú ÏãúÏûëÌïòÎ©¥ Í∑∏ ÏãúÏ†êÏùò hidden stateÍ∞Ä ÏóÜÎã§Îäî Í≤ÉÏù¥Îã§.<br />
Ïù¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥, R2D2Îäî ÌïôÏäµÌï† Îïå ÏãúÌÄÄÏä§Ïùò ÏïûÎ∂ÄÎ∂ÑÏùÑ <strong>‚Äúburn-in Íµ¨Í∞Ñ‚Äù</strong>ÏúºÎ°ú ÏßÄÏ†ïÌïòÍ≥†,<br />
Ïù¥ Íµ¨Í∞ÑÏùÑ Ïã§Ï†úÎ°ú ÌïôÏäµÌïòÏßÄ ÏïäÍ≥†, Ïò§ÏßÅ hidden stateÎ•º Î≥µÏõêÌïòÎäî Ïö©ÎèÑÎ°úÎßå ÏÇ¨Ïö©ÌïúÎã§.</p>

<p>ÏòàÎ•º Îì§Ïñ¥, Ï†ÑÏ≤¥ ÏãúÌÄÄÏä§Í∞Ä 20 timestepÏù¥ÎùºÍ≥† Í∞ÄÏ†ïÌïòÏûê.<br />
Ïù¥ Ï§ë 0~5 timestepÏùÄ LSTMÏùò ÏÉÅÌÉúÎ•º Îã§Ïãú Ïû¨ÌòÑÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÍ≥†,<br />
6~19 timestepÎ∂ÄÌÑ∞Îäî Ïã§Ï†úÎ°ú lossÎ•º Í≥ÑÏÇ∞ÌïòÎ©∞ Q-learning ÏóÖÎç∞Ïù¥Ìä∏Î•º ÏßÑÌñâÌïúÎã§.<br />
Ïù¥Î†áÍ≤å ÌïòÎ©¥ LSTMÏùÄ ÎßàÏπò Ïã§Ï†ú ÏóêÌîºÏÜåÎìúÏùò ÌùêÎ¶ÑÏùÑ Í≥ÑÏÜç Ïù¥Ïñ¥ÎÇòÍ∞ÄÎäî Í≤ÉÏ≤òÎüº ÏÉÅÌÉúÎ•º Ïú†ÏßÄÌï† Ïàò ÏûàÎã§.</p>

<hr />

<h4 id="4-n-step-q-learning">4. n-step Q-learning</h4>

<p>R2D2Îäî ÏùºÎ∞òÏ†ÅÏù∏ Q-learningÏù¥ ÏïÑÎãå n-step TD targetÏùÑ ÏÇ¨Ïö©ÌïúÎã§.<br />
Ï¶â, Î∞îÎ°ú Îã§Ïùå ÏãúÏ†êÏùò Î≥¥ÏÉÅÏù¥ ÏïÑÎãàÎùº, ÏùºÏ†ï ÏãúÍ∞Ñ Îí§ÍπåÏßÄÏùò ÎàÑÏ†Å Î≥¥ÏÉÅÏùÑ Ïù¥Ïö©Ìï¥ ÌÉÄÍπÉÏùÑ Í≥ÑÏÇ∞ÌïúÎã§.<br />
Ïù¥ Î∞©ÏãùÏùÄ ÏãúÍ∞ÑÏ†ÅÏúºÎ°ú Îçî Î®º Í≤∞Í≥ºÍπåÏßÄ Í≥†Î†§ÌïòÎäî Îç∞ Ìö®Í≥ºÏ†ÅÏù¥Îã§.</p>

<hr />

<h4 id="5-prioritized-replay--importance-sampling">5. Prioritized Replay + Importance Sampling</h4>

<p>Ï§ëÏöîÌïú ÏãúÌÄÄÏä§(Ïòà: TD-errorÍ∞Ä ÌÅ∞ Í≤É)Î•º Ïö∞ÏÑ† ÏÉòÌîåÎßÅÌïòÍ≥†,<br />
Í∑∏Ïóê Îî∞Î•∏ Î≥¥Ï†ï Í≥ÑÏàòÎ°ú Í∞ÄÏ§ëÏπòÎ•º Ï°∞Ï†àÌïòÎäî Í∏∞Î≤ïÎèÑ Ìï®Íªò ÏÇ¨Ïö©ÎêúÎã§.</p>

<hr />

<h4 id="6-Î∂ÑÏÇ∞-Í∞ïÌôîÌïôÏäµ-Íµ¨Ï°∞-distributed-learning">6. Î∂ÑÏÇ∞ Í∞ïÌôîÌïôÏäµ Íµ¨Ï°∞ (Distributed Learning)</h4>

<p>R2D2Îäî Îã®Ïùº ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌôòÍ≤ΩÏùÑ ÌÉêÌóòÌïòÍ≥† ÌïôÏäµÌïòÎäî Íµ¨Ï°∞Í∞Ä ÏïÑÎãàÎã§.<br />
ÎåÄÏã†, <strong>Ïó¨Îü¨ Í∞úÏùò actor(ÏóêÏù¥Ï†ÑÌä∏)</strong>Í∞Ä Î≥ëÎ†¨Î°ú ÌôòÍ≤ΩÏùÑ ÌîåÎ†àÏù¥ÌïòÎ©∞ Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßëÌïòÍ≥†,<br />
<strong>ÌïòÎÇòÏùò Ï§ëÏïô learner(ÌïôÏäµÍ∏∞)</strong>Í∞Ä Ïù¥ Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú ÌïôÏäµÌïúÎã§.</p>

<p>Ïù¥ Íµ¨Ï°∞Ïùò Ïû•Ï†êÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§:</p>
<ul>
  <li>ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Í∞Ä Îã§ÏñëÌï¥ÏßÑÎã§ (ÌÉêÌóòÏù¥ ÌíçÎ∂ÄÌï®)</li>
  <li>ÌïôÏäµ ÏÜçÎèÑÍ∞Ä Îß§Ïö∞ Îπ®ÎùºÏßÑÎã§ (Î≥ëÎ†¨ Ï≤òÎ¶¨)</li>
  <li>Îã§ÏñëÌïú Ï†ïÏ±Ö(epsilon Ï∞®Ïù¥ Îì±)ÏùÑ ÎèôÏãúÏóê Ïã§ÌóòÌï† Ïàò ÏûàÎã§</li>
</ul>

<hr />

<h4 id="ÏòàÏãú-Ìå®Îì§-Í≤åÏûÑÏóêÏÑúÏùò-r2d2-Ï†ÅÏö©">ÏòàÏãú: Ìå®Îì§ Í≤åÏûÑÏóêÏÑúÏùò R2D2 Ï†ÅÏö©</h4>

<p>Ìå®Îì§ Í≤åÏûÑÏóêÏÑú Í≥µÏù¥ Í≥ÑÏÜç ÏõÄÏßÅÏù¥Í≥†, ÏÉÅÎåÄÎ∞©ÎèÑ Í≥ÑÏÜç Î∞òÏùëÌïòÍ∏∞ ÎïåÎ¨∏Ïóê,<br />
ÏóêÏù¥Ï†ÑÌä∏Îäî Í≥ºÍ±∞Ïóê Í≥µÏù¥ ÌäÄÏóàÎçò Î∞©Ìñ•, Ïù¥Ï†ÑÏóê ÏûêÏã†Ïùò Ìå®Îì§Ïù¥ Ïñ¥Îñ§ ÏúÑÏπòÏóê ÏûàÏóàÎäîÏßÄÎ•º Í∏∞ÏñµÌï† Ïàò ÏûàÏñ¥Ïïº ÌïúÎã§.<br />
Ïù¥Îü∞ Ï†ïÎ≥¥Î•º Îã®Í∏∞Ï†ÅÏù∏ Í¥ÄÏ∞∞ÎßåÏúºÎ°ú ÌåêÎã®ÌïòÍ∏∞Îäî Ïñ¥Î†µÎã§.<br />
R2D2Îäî LSTMÏùÑ ÌÜµÌï¥ Ïù¥Îü∞ Ï†ïÎ≥¥Î•º ÏãúÍ∞Ñ ÏàúÏÑúÎåÄÎ°ú ÏåìÏïÑÎëêÍ≥†,<br />
ÌòÑÏû¨Ïùò ÏÉÅÌô©ÏùÑ Ïù¥Ìï¥Ìï† Îïå Ïù¥ Í∏∞ÏñµÎì§ÏùÑ Ìï®Íªò Í≥†Î†§Ìï¥ÏÑú Ï†ÑÎûµÏ†ÅÏù∏ ÏõÄÏßÅÏûÑÏùÑ ÏÑ†ÌÉùÌïòÍ≤å ÎßåÎì†Îã§.<br />
ÏòàÎ•º Îì§Ïñ¥, ‚ÄúÏßÄÍ∏àÏùÄ ÏôºÏ™ΩÏúºÎ°ú ÏõÄÏßÅÏù¥Îäî Í≤å Ï¢ãÍ≤†Ïñ¥. ÏôúÎÉêÌïòÎ©¥ 3ÌÉÄÏûÑ Ï†ÑÎ∂ÄÌÑ∞ Í≥µÏù¥ Ï†êÏ†ê ÏôºÏ™ΩÏúºÎ°ú Ïò§Í≥† ÏûàÏóàÍ±∞Îì†.‚Äù<br />
Ïù¥Îü∞ Ï∂îÎ°†Ïù¥ Í∞ÄÎä•Ìïú Í≤ÉÏù¥Îã§.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">collections</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="n">paddle_env</span> <span class="kn">import</span> <span class="n">PaddleEnv</span>

<span class="c1"># ----- ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞ -----
</span><span class="n">EPISODES</span> <span class="o">=</span> <span class="mi">1100</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">MEM_CAPACITY</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">EPS_START</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">EPS_END</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">EPS_DECAY</span> <span class="o">=</span> <span class="mf">0.98</span>
<span class="n">TARGET_UPDATE</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">BURN_IN</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Ï¥àÍ∏∞ hidden state Î≥µÏõêÏö©
</span>
<span class="c1"># -------------------------------------------------------------
# [1] LSTM Í∏∞Î∞ò Q-Network Íµ¨ÌòÑ
# Í∏∞Ï°¥ DQNÏùÄ Îã®ÏàúÌïú MLPÎ•º ÏÇ¨Ïö©ÌïòÏßÄÎßå, R2D2Îäî Í≥ºÍ±∞ Ï†ïÎ≥¥Î•º Í∏∞ÏñµÌïòÍ∏∞ ÏúÑÌï¥ LSTMÏùÑ ÏÇ¨Ïö©ÌïúÎã§.
# Ïù¥ LSTMÏùÄ Í≥ºÍ±∞ timestepÏúºÎ°úÎ∂ÄÌÑ∞ Î∞õÏùÄ hidden stateÎ•º Ïú†ÏßÄÌïòÎ©∞ ÌòÑÏû¨ QÍ∞íÏùÑ ÏòàÏ∏°ÌïúÎã§.
# -------------------------------------------------------------
</span><span class="k">class</span> <span class="nc">R2D2Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>              <span class="c1"># [B, T, D] -&gt; [B, T, H]
</span>        <span class="n">x</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>         <span class="c1"># [B, T, H]
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">out</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">hidden</span>               <span class="c1"># Q-values: [B, T, A]
</span>
<span class="c1"># -------------------------------------------------------------
# [2] Recurrent Replay Buffer Íµ¨ÌòÑ
# ÏùºÎ∞ò DQNÏùÄ Îã®Ïùº transitionÏùÑ Ï†ÄÏû•ÌïòÏßÄÎßå, R2D2Îäî LSTMÏùò Î¨∏Îß• Ïú†ÏßÄ ÌäπÏÑ± ÎïåÎ¨∏Ïóê Ï†ÑÏ≤¥ ÏãúÌÄÄÏä§Î•º Ï†ÄÏû•ÌïúÎã§.
# Ïù¥Î•º ÏúÑÌï¥ ÏóêÌîºÏÜåÎìú Îã®ÏúÑÎ°ú bufferÏóê Ï†ÄÏû•ÌïúÎã§.
# -------------------------------------------------------------
</span><span class="k">class</span> <span class="nc">RecurrentReplayBuffer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">capacity</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>  <span class="c1"># ÏãúÌÄÄÏä§ Îã®ÏúÑ Ï†ÄÏû•
</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">bsize</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">,</span> <span class="n">bsize</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">)</span>

<span class="c1"># ----- ÌïôÏäµ Î£®ÌîÑ ÏãúÏûë -----
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">PaddleEnv</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">policy_net</span> <span class="o">=</span> <span class="nc">R2D2Net</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_net</span> <span class="o">=</span> <span class="nc">R2D2Net</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_net</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
    <span class="n">memory</span> <span class="o">=</span> <span class="nc">RecurrentReplayBuffer</span><span class="p">(</span><span class="n">MEM_CAPACITY</span><span class="p">)</span>

    <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_START</span>
    <span class="n">rewards_log</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># ---------------------------------------------------------
</span>    <span class="c1"># [3] ÌñâÎèô ÏÑ†ÌÉù Ìï®Ïàò (epsilon-greedy + LSTM hidden Ïú†ÏßÄ)
</span>    <span class="c1"># ÏóêÏù¥Ï†ÑÌä∏Îäî epsilon ÌôïÎ•†Î°ú Î¨¥ÏûëÏúÑ ÏÑ†ÌÉùÏùÑ ÌïòÎ©∞, Í∑∏ Ïô∏ÏóêÎäî
</span>    <span class="c1"># LSTM Í∏∞Î∞ò Q-networkÏùò Ï∂úÎ†•ÏùÑ ÏÇ¨Ïö©Ìï¥ ÏµúÏ†Å ÌñâÎèôÏùÑ ÏÑ†ÌÉùÌïúÎã§.
</span>    <span class="c1"># ---------------------------------------------------------
</span>    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="nf">randrange</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">),</span> <span class="n">hidden</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># [1, 1, D]
</span>            <span class="n">q</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="nf">policy_net</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">argmax</span><span class="p">().</span><span class="nf">item</span><span class="p">(),</span> <span class="n">hidden</span>

    <span class="c1"># ---------------------------------------------------------
</span>    <span class="c1"># [4] ÌïôÏäµ Ìï®Ïàò (train_step)
</span>    <span class="c1"># ÌïµÏã¨ Î∂ÄÎ∂Ñ: burn-inÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ LSTMÏùò hidden stateÎ•º Î≥µÏõêÌïòÍ≥†,
</span>    <span class="c1"># Ïù¥ÌõÑ Íµ¨Í∞ÑÏóê ÎåÄÌï¥ Q-learning ÏóÖÎç∞Ïù¥Ìä∏Î•º ÏàòÌñâÌïúÎã§.
</span>    <span class="c1"># ÌòÑÏû¨Îäî 1-step Q-learningÎßå Ìè¨Ìï®Îê® (n-step ÎØ∏Ìè¨Ìï®)
</span>    <span class="c1"># ---------------------------------------------------------
</span>    <span class="k">def</span> <span class="nf">train_step</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">memory</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
            <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">episode</span><span class="p">)</span>
            <span class="n">T</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>

            <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">ns</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">next_states</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">actions</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">dones</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">hidden</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">if</span> <span class="n">T</span> <span class="o">&gt;</span> <span class="n">BURN_IN</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="nf">policy_net</span><span class="p">(</span><span class="n">s</span><span class="p">[:,</span> <span class="p">:</span><span class="n">BURN_IN</span><span class="p">],</span> <span class="n">hidden</span><span class="p">)</span>  <span class="c1"># burn-in: LSTM ÏÉÅÌÉú Î≥µÏõêÎßå ÏàòÌñâ
</span>
                <span class="n">q_s</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">policy_net</span><span class="p">(</span><span class="n">s</span><span class="p">[:,</span> <span class="n">BURN_IN</span><span class="p">:],</span> <span class="n">hidden</span><span class="p">)</span>
                <span class="n">q_s</span> <span class="o">=</span> <span class="n">q_s</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">a</span><span class="p">[:,</span> <span class="n">BURN_IN</span><span class="p">:]).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

                <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
                    <span class="n">q_ns_policy</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">policy_net</span><span class="p">(</span><span class="n">ns</span><span class="p">[:,</span> <span class="n">BURN_IN</span><span class="p">:])</span>
                    <span class="n">best_actions</span> <span class="o">=</span> <span class="n">q_ns_policy</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="n">q_ns_target</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">target_net</span><span class="p">(</span><span class="n">ns</span><span class="p">[:,</span> <span class="n">BURN_IN</span><span class="p">:])</span>
                    <span class="n">q_ns</span> <span class="o">=</span> <span class="n">q_ns_target</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">best_actions</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                    <span class="n">target</span> <span class="o">=</span> <span class="n">r</span><span class="p">[:,</span> <span class="n">BURN_IN</span><span class="p">:]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">q_ns</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">d</span><span class="p">[:,</span> <span class="n">BURN_IN</span><span class="p">:])</span>  <span class="c1"># 1-step target Í≥ÑÏÇ∞
</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()(</span><span class="n">q_s</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1"># ---------------------------------------------------------
</span>    <span class="c1"># [5] ÏóêÌîºÏÜåÎìú Î∞òÎ≥µ (ÌÉêÌóò, ÌïôÏäµ, Î≤ÑÌçº Ï†ÄÏû•)
</span>    <span class="c1"># ÌïòÎÇòÏùò ÏóêÌîºÏÜåÎìúÎ•º ÌîåÎ†àÏù¥ÌïòÍ≥†, ÏãúÌÄÄÏä§ Ï†ÑÏ≤¥Î•º replay bufferÏóê Ï†ÄÏû•
</span>    <span class="c1"># Ïù¥ÌõÑ train_step()ÏùÑ Ìò∏Ï∂úÌïòÏó¨ Q-network ÏóÖÎç∞Ïù¥Ìä∏ ÏàòÌñâ
</span>    <span class="c1"># ---------------------------------------------------------
</span>    <span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPISODES</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">total_r</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">episode_buffer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">episode_buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="n">done</span><span class="p">)))</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">total_r</span> <span class="o">+=</span> <span class="n">reward</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">memory</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="n">episode_buffer</span><span class="p">)</span>
        <span class="nf">train_step</span><span class="p">()</span>

        <span class="n">eps</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">EPS_END</span><span class="p">,</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">EPS_DECAY</span><span class="p">)</span>
        <span class="n">rewards_log</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_r</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ep</span> <span class="o">%</span> <span class="n">TARGET_UPDATE</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">target_net</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">EP </span><span class="si">{</span><span class="n">ep</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s"> | Score </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">score</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s"> | TotalR </span><span class="si">{</span><span class="n">total_r</span><span class="si">:</span><span class="mf">5.1</span><span class="n">f</span><span class="si">}</span><span class="s"> | Œµ </span><span class="si">{</span><span class="n">eps</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># ---------------------------------------------------------
</span>    <span class="c1"># [6] Î™®Îç∏ Ï†ÄÏû•
</span>    <span class="c1"># ---------------------------------------------------------
</span>    <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sh">"</span><span class="s">paddle_dqn_model.pth</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: paddle_dqn_model.pth</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Î™®Îç∏ÏùÑ Ï†ÄÏû•ÌïòÏó¨ Ïã§ÌñâÌï¥Î≥∏ Í≤∞Í≥º</p>

<div align="center">
  <img src="/images/lstm.gif" alt="bandit" width="60%" />
</div>

<p>ÌïôÏäµÏù¥ ÏïÑÏ£º Ïûò Ïù¥Î£®Ïñ¥Ï†∏ Í≤åÏûÑÏùÑ Ï¶êÍ∏∞Îäî aiÎ•º ÌôïÏù∏Ìï† Ïàò ÏûàÎã§.</p>

						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
			<div class="section section--grey">
				<div class="container">
					<div class="row">
						<div class="col-md-7">
							<div class="comments-area">
  <h3 class="comments-title">Comments</h3>
  <div id="comments-list" class="comments-list">
    <!-- Comments will be populated here -->
  </div>
  
  <div class="comment-respond">
    <h4 class="comment-reply-title">Leave a Comment</h4>
    <form id="comment-form" class="comment-form">
      <div class="form-group">
        <label for="name">Name</label>
        <input type="text" id="name" name="name" required class="form-control">
      </div>
      <div class="form-group">
        <label for="email">Email</label>
        <input type="email" id="email" name="email" required class="form-control">
      </div>
      <div class="form-group">
        <label for="comment">Comment</label>
        <textarea id="comment" name="comment" required class="form-control" rows="5"></textarea>
      </div>
      <div class="form-group admin-field">
        <label for="password">Admin Password</label>
        <input type="password" id="password" name="password" class="form-control" placeholder="For comment moderation">
      </div>
      <div class="form-submit">
        <button type="submit" class="btn btn--dark btn--rounded">Submit Comment</button>
      </div>
    </form>
  </div>
</div>

<script>
  (function() {
    // Simple storage for comments using localStorage
    const COMMENTS_STORAGE_KEY = 'page_comments_/page/reinforce/';
    
    // Load comments from storage
    function loadComments() {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      const commentsList = document.getElementById('comments-list');
      commentsList.innerHTML = '';
      
      if (comments.length === 0) {
        commentsList.innerHTML = '<div class="no-comments">No comments yet. Be the first to comment!</div>';
        return;
      }
      
      comments.forEach((comment, index) => {
        const commentDiv = document.createElement('div');
        commentDiv.className = 'comment';
        commentDiv.dataset.id = index;
        
        const commentHTML = `
          <div class="comment-meta">
            <div class="comment-author">
              <strong>${comment.name}</strong>
            </div>
            <div class="comment-metadata">
              <span>${new Date(comment.date).toLocaleDateString()} ${new Date(comment.date).toLocaleTimeString()}</span>
            </div>
          </div>
          <div class="comment-content">
            <p>${comment.text}</p>
          </div>
          <div class="comment-actions">
            <button class="btn-link reply-btn" data-id="${index}">
              <i class="icon icon--arrow-right"></i> Reply
            </button>
            <button class="btn-link delete-btn" data-id="${index}">
              <i class="icon icon--cross"></i> Delete
            </button>
          </div>
          <div class="reply-form-wrapper" id="reply-form-${index}" style="display: none;">
            <form class="reply-form" data-parent="${index}">
              <div class="form-group">
                <label for="reply-name-${index}">Name</label>
                <input type="text" id="reply-name-${index}" name="name" required class="form-control">
              </div>
              <div class="form-group">
                <label for="reply-comment-${index}">Reply</label>
                <textarea id="reply-comment-${index}" name="comment" required class="form-control" rows="3"></textarea>
              </div>
              <div class="form-submit">
                <button type="submit" class="btn btn--dark btn--rounded btn--sm">Submit Reply</button>
              </div>
            </form>
          </div>
          <div class="children" id="replies-${index}">
            ${renderReplies(comment.replies || [])}
          </div>
        `;
        
        commentDiv.innerHTML = commentHTML;
        commentsList.appendChild(commentDiv);
      });
      
      // Add event listeners to reply buttons
      document.querySelectorAll('.reply-btn').forEach(button => {
        button.addEventListener('click', function() {
          const commentId = this.dataset.id;
          const replyForm = document.getElementById(`reply-form-${commentId}`);
          replyForm.style.display = replyForm.style.display === 'none' ? 'block' : 'none';
        });
      });
      
      // Add event listeners to delete buttons
      document.querySelectorAll('.delete-btn').forEach(button => {
        button.addEventListener('click', function() {
          const commentId = this.dataset.id;
          const password = prompt('Enter admin password to delete:');
          
          if (password === 'admin123') { // Simple password for demo
            deleteComment(parseInt(commentId));
          } else {
            alert('Incorrect password');
          }
        });
      });
      
      // Add event listeners to reply forms
      document.querySelectorAll('.reply-form').forEach(form => {
        form.addEventListener('submit', function(e) {
          e.preventDefault();
          const parentId = parseInt(this.dataset.parent);
          const replyName = this.querySelector('[name="name"]').value;
          const replyText = this.querySelector('[name="comment"]').value;
          
          addReply(parentId, replyName, replyText);
          this.reset();
          document.getElementById(`reply-form-${parentId}`).style.display = 'none';
        });
      });
    }
    
    // Render replies
    function renderReplies(replies) {
      if (!replies || replies.length === 0) return '';
      
      let html = '';
      replies.forEach((reply, replyIndex) => {
        html += `
          <div class="comment child-comment" data-id="${replyIndex}">
            <div class="comment-meta">
              <div class="comment-author">
                <strong>${reply.name}</strong>
              </div>
              <div class="comment-metadata">
                <span>${new Date(reply.date).toLocaleDateString()} ${new Date(reply.date).toLocaleTimeString()}</span>
              </div>
            </div>
            <div class="comment-content">
              <p>${reply.text}</p>
            </div>
            <div class="comment-actions">
              <button class="btn-link delete-reply-btn" data-parent="${replyIndex}">
                <i class="icon icon--cross"></i> Delete
              </button>
            </div>
          </div>
        `;
      });
      return html;
    }
    
    // Add a new comment
    function addComment(name, email, text) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      comments.push({
        name: name,
        email: email,
        text: text,
        date: new Date().toISOString(),
        replies: []
      });
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Add a reply to a comment
    function addReply(parentId, name, text) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      if (!comments[parentId].replies) {
        comments[parentId].replies = [];
      }
      
      comments[parentId].replies.push({
        name: name,
        text: text,
        date: new Date().toISOString()
      });
      
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Delete a comment
    function deleteComment(commentId) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      comments.splice(commentId, 1);
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Event listener for comment form
    document.getElementById('comment-form').addEventListener('submit', function(e) {
      e.preventDefault();
      
      const name = document.getElementById('name').value;
      const email = document.getElementById('email').value;
      const comment = document.getElementById('comment').value;
      
      addComment(name, email, comment);
      this.reset();
    });
    
    // Initial load
    document.addEventListener('DOMContentLoaded', loadComments);
  })();
</script>

<style>
  /* Comments styling that matches Doks theme */
  .comments-area {
    margin-top: 2.5rem;
    font-family: 'Noto Sans', sans-serif;
  }
  
  .comments-title {
    margin-bottom: 1.5rem;
    font-size: 1.75em;
    font-weight: 600;
    color: #333;
  }
  
  .comment {
    margin-bottom: 1.5rem;
    padding: 1.25rem;
    background-color: #fff;
    border-radius: 4px;
    box-shadow: 0 1px 4px rgba(0,0,0,.04);
    border-left: 4px solid #253951;
    transition: all .2s ease;
  }
  
  .comment:hover {
    box-shadow: 0 1px 10px rgba(0,0,0,.08);
  }
  
  .comment-meta {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 0.75rem;
  }
  
  .comment-author {
    font-weight: 600;
  }
  
  .comment-metadata {
    font-size: 0.75em;
    color: #8a8a8a;
  }
  
  .comment-content {
    line-height: 1.6;
    color: #333;
  }
  
  .comment-content p {
    margin-bottom: 0.5rem;
  }
  
  .comment-actions {
    margin-top: 0.75rem;
    padding-top: 0.5rem;
    border-top: 1px solid #f5f5f5;
  }
  
  .btn-link {
    background: none;
    border: none;
    padding: 0;
    color: #253951;
    cursor: pointer;
    font-size: 0.85em;
    text-decoration: none;
    margin-right: 1rem;
    transition: color .2s ease;
  }
  
  .btn-link:hover {
    color: #0056b3;
    text-decoration: underline;
  }
  
  .reply-form-wrapper {
    margin-top: 1rem;
    margin-bottom: 1rem;
    padding: 1rem;
    background-color: #f9f9f9;
    border-radius: 4px;
  }
  
  .children {
    margin-top: 1rem;
    margin-left: 1.5rem;
    padding-left: 1rem;
    border-left: 1px solid #eaeaea;
  }
  
  .child-comment {
    margin-bottom: 1rem;
    border-left: 3px solid #6c757d;
  }
  
  .no-comments {
    padding: 1rem;
    background-color: #f5f5f5;
    border-radius: 4px;
    font-style: italic;
    color: #666;
  }
  
  /* Form styling */
  .comment-respond {
    margin-top: 2rem;
    padding: 1.5rem;
    background-color: #f9f9f9;
    border-radius: 4px;
  }
  
  .comment-reply-title {
    margin-bottom: 1.25rem;
    font-size: 1.25em;
    font-weight: 600;
    color: #333;
  }
  
  .comment-form .form-group {
    margin-bottom: 1rem;
  }
  
  .comment-form label {
    display: block;
    margin-bottom: 0.5rem;
    font-weight: 600;
    color: #333;
  }
  
  .comment-form .form-control {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid #e0e0e0;
    border-radius: 4px;
    background-color: #fff;
    font-family: inherit;
    font-size: 0.95em;
    transition: border-color .2s ease;
  }
  
  .comment-form .form-control:focus {
    border-color: #253951;
    outline: none;
  }
  
  .admin-field {
    opacity: 0.8;
  }
  
  .form-submit {
    margin-top: 1.5rem;
  }
  
  /* Match the Doks buttons */
  .btn {
    display: inline-block;
    font-weight: 500;
    text-align: center;
    white-space: nowrap;
    vertical-align: middle;
    user-select: none;
    padding: 0.75rem 1.25rem;
    font-size: 1rem;
    line-height: 1.5;
    border-radius: 4px;
    transition: all .2s ease-in-out;
    text-decoration: none;
    cursor: pointer;
  }
  
  .btn--dark {
    background-color: #253951;
    border-color: #253951;
    color: #fff;
  }
  
  .btn--dark:hover {
    background-color: #1a2a3c;
    border-color: #1a2a3c;
  }
  
  .btn--rounded {
    border-radius: 100px;
  }
  
  .btn--sm {
    padding: 0.5rem 1rem;
    font-size: 0.875rem;
  }
</style>

						</div><!-- /.col -->
					</div><!-- /.row -->
				</div><!-- /.container -->
			</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
				<div class="micro-nav">
	<div class="container">
		<div class="row">
			<div class="col-xs-12">
				<a href="/" class="micro-nav__back">
					<i class="icon icon--arrow-left"></i>
					Back to homepage
				</a><!-- /.micro-nav__back -->
			</div><!-- /.col -->
		</div><!-- /.row -->
	</div><!-- /.container -->
</div><!-- /.micro-nav -->

			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
						<a href="/" class="site-footer__logo">InchanBaek Note</a>
					
					
						<hr>
						<p class="site-footer__copyright">Copyright &copy; 2017. - InchanBaek Note <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
					<div class="col-sm-6 align-right">
						<ul class="social-list">
							
								<li>
									<a href="https://www.linkedin.com/in/inchan-baek-728197265/" target="_blank" class="social-list__item social-list__item--linkedin">
										<i class="icon icon--linkedin"></i>
									</a>
								</li>
							
								<li>
									<a href="https://www.instagram.com/in_chanchan/" target="_blank" class="social-list__item social-list__item--instagram">
										<i class="icon icon--instagram"></i>
									</a>
								</li>
							
								<li>
									<a href="https://www.youtube.com/@icb6048" target="_blank" class="social-list__item social-list__item--youtube">
										<i class="icon icon--youtube"></i>
									</a>
								</li>
							
						</ul><!-- /.social-list -->
					</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>





		</div><!-- /.js-footer-area -->
	</body>
</html>