<!DOCTYPE html>
<html >
	<head>
		

<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<meta name="google-site-verification" content="googlef9130e9561130734.html" />
<title>Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology</title>

	<meta name="description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">


	<meta name="keywords" content="강화학습, 머신러닝, 인공지능, 인찬백, InchanBaek, 리워드, 에이전트, 액션, MDP, 마르코프 결정 과정, Q-러닝, reinforcement learning, machine learning, AI, reward, agent, action, Markov decision process, Q-learning, deep reinforcement learning">

<meta name="author" content="Inchan Baek">
<!-- Open Graph / Facebook -->
<meta property="og:type" content="website">
<meta property="og:url" content="http://localhost:4000/page/reinforce/">
<meta property="og:title" content="Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology">
<meta property="og:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">
<!-- Twitter -->
<meta property="twitter:card" content="summary">
<meta property="twitter:url" content="http://localhost:4000/page/reinforce/">
<meta property="twitter:title" content="Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology">
<meta property="twitter:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">
<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="canonical" href="http://localhost:4000/page/reinforce/">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning from Scratch</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Reinforcement Learning from Scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques." />
<meta property="og:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques." />
<link rel="canonical" href="http://localhost:4000/page/reinforce/" />
<meta property="og:url" content="http://localhost:4000/page/reinforce/" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reinforcement Learning from Scratch" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.","headline":"Reinforcement Learning from Scratch","url":"http://localhost:4000/page/reinforce/"}</script>
<!-- End Jekyll SEO tag -->

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/moonspam/NanumSquare@1.0/nanumsquare.css">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<link rel="stylesheet" href="/doks-theme/assets/css/custom.css">
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      securityLevel: 'loose'
    });
    
    // Find all pre blocks with mermaid class and render them
    document.querySelectorAll('pre code.language-mermaid').forEach(function(element) {
      // Get the mermaid code
      const mermaidCode = element.textContent;
      
      // Create a new div for the mermaid diagram
      const newDiv = document.createElement('div');
      newDiv.className = 'mermaid';
      newDiv.textContent = mermaidCode;
      
      // Replace the pre element with the new div
      const preElement = element.parentElement;
      preElement.parentElement.replaceChild(newDiv, preElement);
    });
    
    // Trigger mermaid render
    mermaid.init();
  });
</script>

	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
		


	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">InchanBaek Note</a>
					
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Reinforcement Learning from Scratch</h1>
								
								
									<p class="hero-subheader__desc">A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.</p>
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="content">
							<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h2 id="bandit-problem">Bandit Problem</h2>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>

<ul>
  <li>
    <p>Supervised Learning : When the input and output data are given, it is a method of modeling the relationship between input data and output data.</p>
  </li>
  <li>
    <p>Unsupervised Learning : When the input data is given, it is a method of finding the characteristics of the input data.</p>
  </li>
  <li>
    <p><strong>Reinforcement Learning</strong> : <strong>Agent</strong> is an entity that interacts with the <strong>environment</strong> and receives information about the environment to choose <strong>actions</strong> that <strong>maximize rewards</strong>.</p>
  </li>
</ul>

<h3 id="what-is-bandit-problem">What is Bandit problem?</h3>

<p>Bandit == Slot machine</p>

<p>Each slot machine has a different probability.</p>

<p>At first, we don’t know which slot machine is the best.</p>

<p>We need to find the good machine by actually playing.</p>

<p>The goal is to get as much reward as possible within a limited number of plays.</p>

<div align="center">
  <div class="mermaid">
    graph LR
    A[Agent] --&gt;|action| B[Environment]
    B --&gt;|reward| A
  </div>
</div>

<p><strong>The agent as a player selects actions in a given environment, and the environment provides rewards to the agent.</strong></p>

<p><strong>Goal</strong>: <strong>Select actions that maximize rewards</strong> -&gt; <strong>Get as many coins as possible</strong> -&gt; <strong>Find the best slot machine</strong></p>

<h3 id="value-and-action-value">Value and Action Value</h3>

<ul>
  <li><strong>Value</strong>: Expected reward that can be obtained in a specific state</li>
</ul>

\[E[R_t]\]

<ul>
  <li><strong>Action Value</strong>: Expected reward obtained as a result of an action</li>
</ul>

\[Q(A) = E[R_t | A]\]

<p>(E = Expectation, Q = Quality, A = Action, R = Reward)</p>

<p>Let’s calculate the expected rewards for slot machines a and b.</p>

<p>Below is a table for slot machine a.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Slot machine a</th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Coins obtainable</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">10</td>
    </tr>
    <tr>
      <td style="text-align: center">Reward probability</td>
      <td style="text-align: center">0.70</td>
      <td style="text-align: center">0.15</td>
      <td style="text-align: center">0.12</td>
      <td style="text-align: center">0.03</td>
    </tr>
  </tbody>
</table>

<p>Here’s a table for slot machine b.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Slot machine b</th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Coins obtainable</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">10</td>
    </tr>
    <tr>
      <td style="text-align: center">Reward probability</td>
      <td style="text-align: center">0.50</td>
      <td style="text-align: center">0.40</td>
      <td style="text-align: center">0.09</td>
      <td style="text-align: center">0.01</td>
    </tr>
  </tbody>
</table>

<p>The expected values for the two machines are:</p>

<ul>
  <li>Slot machine a: (0.7 * 0 + 0.15 * 1 + 0.12 * 5 + 0.03 * 10) = 1.05</li>
  <li>Slot machine b: (0.5 * 0 + 0.4 * 1 + 0.09 * 5 + 0.01 * 10) = 0.95</li>
</ul>

<p><strong>Slot machine a is better than slot machine b.</strong></p>

<h3 id="value-estimation">Value Estimation</h3>

<p>Let’s say the rewards obtained during n plays are R1, R2, …, Rn.
Then the action value estimate Qn can be calculated as follows:</p>

\[Q_n = \frac{R_1 + R_2 + ... + R_n}{n}\]

<p>However, if we estimate the value this way after n plays, the computational and memory load becomes large.
We can calculate the nth value estimate using the (n-1)th value estimate.</p>

\[Q_{n-1} = \frac{R_1 + R_2 + ... + R_{n-1}}{n-1}\]

<p>If we multiply both sides of this equation by (n-1):</p>

\[(n - 1)Q_{n-1} = R_1 + R_2 + ... + R_{n-1}\]

<p>Now we can calculate the nth value estimate:</p>

\[Q_n = \frac{1}{n} (R_1 + R_2 + ... + R_{n-1} + R_n)\]

\[=\frac{1}{n} (n - 1)Q_{n-1} + \frac{1}{n} R_n\]

\[= Q_{n - 1} + \frac{1}{n} (R_n - Q_{n - 1})\]

<h3 id="players-policy">Player’s Policy</h3>

<p>If we completely trust uncertain estimates, we might miss the best action. Therefore, the agent needs to reduce uncertainty and increase the reliability of estimation.</p>

<ul>
  <li><strong>Policy</strong>: The strategy that determines the actions an agent selects when interacting with the environment</li>
</ul>

<p>There are two policies that can be used to reduce uncertainty:</p>

<ol>
  <li><strong>Exploration</strong>: Selecting uncertain actions to gain information about the environment</li>
  <li><strong>Exploitation</strong>: Selecting the best action based on information available so far</li>
</ol>

<p><strong>Ultimately, reinforcement learning algorithms are about finding the right ‘balance between exploitation and exploration’!!!!</strong></p>

<h3 id="epsilon-greedy-policy">Epsilon-Greedy Policy</h3>
<p>This is one of the algorithms used to balance exploration and exploitation.
For example, if \(\epsilon\) = 0.1, it selects a random action with 10% probability and selects the best action with 90% probability.</p>

<h3 id="solving-the-bandit-problem">Solving the Bandit Problem</h3>

<ul>
  <li><strong>Action Value Estimation</strong>: Estimate the action value and select the best action.</li>
  <li><strong>Policy</strong>: Use the <strong>epsilon-greedy policy</strong> to balance <strong>exploration</strong> and <strong>exploitation</strong>.</li>
</ul>

<p>Let’s implement the above content in code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arms</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.38991635</span><span class="p">,</span> <span class="mf">0.5937864</span><span class="p">,</span>  <span class="mf">0.55356798</span><span class="p">,</span> <span class="mf">0.46228943</span><span class="p">,</span> <span class="mf">0.48251845</span><span class="p">,</span> <span class="mf">0.47595196</span><span class="p">,</span> <span class="mf">0.53560295</span><span class="p">,</span> <span class="mf">0.43374032</span><span class="p">,</span> <span class="mf">0.55913105</span><span class="p">,</span> <span class="mf">0.57484477</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
        <span class="n">rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">rate</span> <span class="o">&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span> <span class="p">:</span> 
            <span class="k">return</span> <span class="mi">0</span>


<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epslion</span><span class="p">,</span> <span class="n">action_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epslion</span> <span class="o">=</span> <span class="n">epslion</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Ns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Ns</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">])</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">Ns</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">epslion</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">steps</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="nc">Bandit</span><span class="p">()</span>

<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">()</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">bandit</span><span class="p">.</span><span class="nf">play</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
    <span class="n">rates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">act</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Actions</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Action</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<div align="center">
  <img src="/images/bandit.png" alt="bandit" width="100%" />
</div>

<p>After about 10,000 plays, it still doesn’t know that selecting the slot machine at index 1 as an action is optimal.
Let’s try with more steps.</p>

<div align="center">
  <img src="/images/bandit2.png" alt="bandit" width="100%" />
</div>

<p>After about 30,000 plays, it learns that selecting the slot machine at index 1 as an action is optimal.
It took an additional 20,000 plays to recognize a probability difference of about 2%.</p>

<h3 id="non-stationary-problem">Non-stationary Problem</h3>

<p>The bandit problem we’ve covered so far belongs to the category of <strong>stationary problems</strong>. A stationary problem is one where the probability distribution of rewards <strong>does not change</strong>. In the code above, you can see that the probabilities are fixed in the variable called rates.</p>

<p>However, in reality, the probability distribution of rewards often changes. This is called a <strong>non-stationary problem</strong>. How should we handle this?</p>

<p>First, in stationary problems, we updated the action value estimate with the following equation:</p>

\[Q_n = Q_{n - 1} + \frac{1}{n} (R_n - Q_{n - 1})\]

<p>But in <strong>non-stationary problems</strong>, we update the action value estimate with the following equation:</p>

\[Q_n = Q_{n - 1} + \alpha (R_n - Q_{n - 1})\]

<p>This method <strong>reduces the weight of rewards obtained long ago</strong> and <strong>increases the weight of recently obtained rewards</strong>. Here, \(\alpha\) is called the <strong>learning rate</strong>.</p>

<div style="overflow-x: auto;">

$$
= Q_{n - 1} + \alpha (R_n - Q_{n - 1})
$$
$$
= (1 - \alpha) Q_{n - 1} + \alpha R_n
$$
$$
= \alpha R_n + (1 - \alpha) {(\alpha R_{n - 1} + (1 - \alpha) Q_{n - 2})}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 Q_{n - 2}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} + (1 - \alpha)^3 Q_{n - 3}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} +
$$
$$
(1 - \alpha)^3 \alpha R_{n - 3} + ... + (1 - \alpha)^{n - 1} \alpha R_1 + (1 - \alpha)^n Q_0
$$

</div>

<p>\(Q_0\) is the initial value. Depending on the value we set, bias can occur in the learning results. However, when using sample averages, the bias disappears.</p>

<p>This method is called <strong>exponential moving average</strong> or <strong>exponentially weighted moving average</strong>.</p>

<ul>
  <li><strong>Exponential Weighted Moving Average</strong>: A method that gives <strong>more weight to recently obtained rewards</strong> and <strong>less weight to rewards obtained long ago</strong></li>
</ul>

<p>Let’s implement this in Python code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arms</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.38991635</span><span class="p">,</span> <span class="mf">0.7837864</span><span class="p">,</span>  <span class="mf">0.55356798</span><span class="p">,</span> <span class="mf">0.46228943</span><span class="p">,</span> <span class="mf">0.48251845</span><span class="p">,</span> <span class="mf">0.47595196</span><span class="p">,</span> <span class="mf">0.53560295</span><span class="p">,</span> <span class="mf">0.43374032</span><span class="p">,</span> <span class="mf">0.55913105</span><span class="p">,</span> <span class="mf">0.57484477</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
        <span class="n">rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">+=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">rate</span> <span class="o">&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span> <span class="p">:</span> 
            <span class="k">return</span> <span class="mi">0</span>

<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epslion</span><span class="p">,</span> <span class="n">action_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epslion</span> <span class="o">=</span> <span class="n">epslion</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">epslion</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> 

<span class="n">steps</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="nc">Bandit</span><span class="p">()</span>

<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">()</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">bandit</span><span class="p">.</span><span class="nf">play</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
    <span class="n">rates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">act</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Actions</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Action</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/bandit3.png" alt="bandit" width="100%" />
</div>

<p>When we set the fixed value \(\alpha\) = 0.8, we can see that the results converge faster than when using sample averages.</p>

<h3 id="summary">Summary</h3>

<ul>
  <li><strong>Bandit Problem</strong>: A fundamental problem in reinforcement learning where the goal is to find a method that maximizes rewards among multiple slot machines</li>
  <li><strong>Action Value</strong>: The expected reward obtained as a result of an action</li>
  <li><strong>Policy</strong>: The strategy that determines the actions an agent selects when interacting with the environment</li>
  <li><strong>Epsilon-Greedy Policy</strong>: One of the algorithms used to balance <strong>exploration and exploitation</strong></li>
  <li><strong>Non-stationary Problem</strong>: A problem where the probability distribution of rewards changes</li>
  <li><strong>Exponential Weighted Moving Average</strong>: A method that gives <strong>more weight to recently obtained rewards</strong> and <strong>less weight to rewards obtained long ago</strong></li>
</ul>

<h2 id="markov-decision-process">Markov Decision Process</h2>

<p>Let’s examine problems where the state of the environment changes according to an agent’s actions.</p>

<h3 id="what-is-a-markov-decision-process">What is a Markov Decision Process?</h3>

<ul>
  <li>
    <p><strong>Markov Decision Process (MDP)</strong>: A method of modeling an environment where the agent interacts with the environment, and the environment’s state satisfies the Markov property</p>
  </li>
  <li>
    <p><strong>Markov Property</strong>: The property where the <strong>future state depends only on the current state</strong></p>
  </li>
</ul>

<p>MDPs require the concept of time. At a specific time, the agent takes an action, and as a result, transitions to a new state. The time unit in this case is called a time step.</p>

<div align="center">
  <div class="mermaid">
    graph LR
    A[Agent] --&gt;|action| B[Environment]
    B --&gt;|reward, state| A
  </div>
</div>

<ul>
  <li><strong>State Transition</strong>: How does the state transition?</li>
  <li><strong>Reward</strong>: How is the reward given?</li>
  <li><strong>Policy</strong>: How does the agent determine its actions?</li>
</ul>

<p>The above three elements must be expressed in formulas.</p>

<p>If the state transition is <strong>deterministic</strong>, the next state s’ depends only on the current state s and action a.</p>

<p><strong>State transition function</strong> =&gt; 
\(s' = f(s, a)\)</p>

<p>If the state transition is <strong>probabilistic</strong>, the next state s’ depends only on the current state s and action a.</p>

<p><strong>State transition probability</strong> =&gt;
\(P(s' | s, a)\)</p>

<h3 id="reward-function">Reward Function</h3>

<p>The <strong>reward function</strong> returns the reward for state s and action a. It returns the reward received when the agent takes action a in state s and moves to the next state s’.</p>

<p><strong>Reward function</strong> =&gt;
\(r(s, a, s')\)</p>

<h3 id="agents-policy">Agent’s Policy</h3>

<p>The agent’s <strong>policy</strong> refers to how the agent determines its actions. The agent determines its actions based solely on the <strong>‘current state’</strong>.
This is because <strong>‘all the information needed about the environment is contained in the current state’</strong>.</p>

<p>A policy that the agent decides probabilistically can be expressed as follows:</p>

<p><strong>Policy</strong> =&gt;
\(\pi(a | s) = P(a | s)\)</p>

<h3 id="goal-of-mdp">Goal of MDP</h3>

<p>The goal of MDP is to find a policy that maximizes rewards. The agent behaves according to the policy 
\(\pi(a | s)\)
The next state is determined according to that action and the state transition probability \(P(s' | s, a)\). And the agent receives rewards according to the reward function \(r(s, a, s')\).</p>

<h3 id="return">Return</h3>

<p>The state at time t is \(S_t\), according to the policy \(\pi\), the action is \(A_t\), the reward is \(R_t\), and this leads to a flow that transitions to the new state \(S_{t+1}\). The return at this time can be defined as follows:</p>

\[G_t = R_t + rR_{t+1} + r^2R_{t+2} + ... = \sum_{k=0}^{\infty} r^k R_{t+k}\]

<p>As time passes, the reward decreases exponentially due to \(\gamma\).</p>

<h3 id="state-value-function">State Value Function</h3>

<p>The agent’s goal is to maximize returns. Even if an agent starts in the same state, the returns can vary for each episode. To respond to such stochastic behavior, we use the expectation, i.e., the expected return, as an indicator.</p>

<p>The state value function is a function that represents the expected value of rewards that can be received in the future, starting from a specific state in reinforcement learning. It is generally represented as \(V(s)\), where \(s\) represents the state. The state value function is calculated according to policy \(\pi\), and is defined by the following formula:</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s, \pi \right]
$$
</div>

\[= \mathbb{E}_\pi \left[G_t \mid S_t = s \right]\]

<p>Where:</p>
<ul>
  <li>\(\mathbb{E}_\pi\): Expected value according to policy \(\pi\)</li>
  <li>\(\gamma\): Discount rate (0 ≤ \(\gamma\) &lt; 1)</li>
  <li>\(R_{t+1}\): Reward at time \(t+1\)</li>
  <li>\(S_0 = s\): Initial state</li>
</ul>

<p>In other words, the state value function is used to predict the total rewards that will be received in the long term, starting from a specific state, when following a given policy. This plays an important role in evaluating the quality of a policy or finding the optimal policy.</p>

<h3 id="optimal-policy-and-optimal-value-function">Optimal Policy and Optimal Value Function</h3>

<p>In reinforcement learning, the optimal policy \(\pi^*\) is a policy that maximizes the expected reward in all states. If the agent follows the optimal policy, it can obtain the maximum possible reward.</p>

<p>The optimal value function \(V^*(s)\) is the sum of expected rewards that can be obtained when starting from state \(s\) and following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
V^*(s) = \max_{\pi} V^{\pi}(s) = \max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s \right]
$$
</div>

<p>Similarly, the optimal action-value function \(Q^*(s,a)\) is the sum of expected rewards that can be obtained when taking action \(a\) in state \(s\) and thereafter following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
Q^*(s,a) = \max_{\pi} Q^{\pi}(s,a) = \max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right]
$$
</div>

<p>The optimal policy and optimal value function can be defined through the Bellman Optimality Equation:</p>

<div style="overflow-x: auto;">
$$
V^*(s) = \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]
$$
</div>

<div style="overflow-x: auto;">
$$
Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')
$$
</div>

<p>The goal of reinforcement learning is to find such an optimal policy or optimal value function.</p>

<h2 id="bellman-equation">Bellman equation</h2>

<p>First, Summary of the Above.</p>

<ul>
  <li><strong>❓ What is an MDP?</strong></li>
</ul>

<p>An MDP is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under the control of an agent.</p>

<p>It consists of:</p>

<ul>
  <li>A set of <strong>states (S)</strong></li>
  <li>A set of <strong>actions (A)</strong></li>
  <li>A <strong>transition probability function (P)</strong></li>
  <li>A <strong>reward function (R)</strong></li>
  <li>A <strong>discount factor (γ)</strong></li>
</ul>

<p>So, MDP is the <strong>foundation of reinforcement learning</strong>, where an agent learns to choose actions that maximize cumulative reward over time.</p>

<ul>
  <li><strong>❓Why is important Bellman equation in MDP?</strong></li>
</ul>

<p>The <strong>Bellman equation</strong> is important in Markov Decision Processes (MDPs) because it provides a <strong>recursive decomposition of the value function</strong>, which represents the expected return starting from a given state. It serves as the <strong>foundation for many reinforcement learning algorithms</strong>, enabling <strong>efficient computation of optimal policies</strong> by breaking down complex problems into smaller subproblems.</p>

<p>🔑 <strong>Bellman Equation – Easy Explanation (with Keywords)</strong></p>
<ul>
  <li>
    <p><strong>The Bellman equation expresses</strong>
“<strong>What kind of future reward can I expect if I act well in this state?</strong>”</p>
  </li>
  <li>
    <p><strong>It uses recursion to break down a complex problem into smaller subproblems.</strong></p>
  </li>
  <li>
    <p><strong>This allows us to efficiently and systematically optimize the overall policy.</strong></p>
  </li>
  <li>
    <p><strong>Many reinforcement learning algorithms like Q-learning and Value Iteration</strong>
are based on the Bellman equation.</p>
  </li>
</ul>

<h3 id="derivation-of-bellman-equation">Derivation of Bellman Equation.</h3>

<p>First, let’s define ‘Return at time t’ as the sum of rewards from time ‘t’</p>

<div style="overflow-x: auto;">
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$
</div>

<p>Second, What is ‘Return at time t + 1’?</p>
<div style="overflow-x: auto;">
$$
G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
</div>

<p>So, we can rearrange the equation as above two equatios.</p>

<div style="overflow-x: auto;">
$$
G_t = R_t + \gamma G_{t+1}
$$
</div>

<p>We know the relation between \(G_t\) and \(G_{t+1}\).</p>

<p>Based on the state-vlaue function \(V_\pi(s)\) we obtained earlier, we can derived the following conclusion.</p>

<div style="overflow-x: auto;">
$$
V_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
    = \mathbb{E}_\pi[R_t + \gamma G_{t+1} | S_t = s]
    = \mathbb{E}_\pi[R_t | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
$$
</div>

<p>(Since Linearity of Expectation 👉 \(\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]\))</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        s((s)) --&gt; A((A))
        s((s)) --&gt; B((B))
        s((s)) --&gt; C((C))
        A --&gt; A1((A1))
        A --&gt; A2((A2))
        B --&gt; B1((B1))
        B --&gt; B2((B2))
        C --&gt; C1((C1))
        C --&gt; C2((C2))
        style s fill:#ffffff,stroke:#000000,stroke-width:2px
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
        style A1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style A2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C2 fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>we define \(\pi(a | s)\) as the probability of taking action \(a\) in state \(s\).</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        s((s)) --&gt; A((A))
        s((s)) --&gt; B((B))
        s((s)) --&gt; C((C))
        style s fill:#ffffff,stroke:#000000,stroke-width:2px
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>so 
\(\pi(a_1 | s) = A\), 
\(\pi(a_2 | s) = B\), 
\(\pi(a_3 | s) = C\)</p>

<p>and we choose the action along with the policy \(\pi\). we move \(s\) to \(s'\) with the probability 
\(P(s' | s, a)\). (P is the transition probability function)</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        A((A)) --&gt; A1((A1))
        A --&gt; A2((A2))
        B((B)) --&gt; B1((B1))
        B --&gt; B2((B2))
        C((C)) --&gt; C1((C1))
        C --&gt; C2((C2))
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
        style A1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style A2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C2 fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>According to above graph,</p>

\[A_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[A_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

\[B_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[B_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

\[C_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[C_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

<p>Let’s generalize the above equation.</p>

<div style="overflow-x: auto;">

$$
\mathbb{E}_\pi[R_t | S_t = s] = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s')
$$

</div>

<div style="overflow-x: auto;">

$$
V_\pi(s) = \mathbb{E}_\pi[R_t | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
$$


$$
= \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s') + \gamma \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) V_\pi(s')
$$

$$
= \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right]
$$

</div>

<p>This is the <strong>Bellman euqation</strong> for the state value function.</p>

<h3 id="state-value-function-and-action-value-functionq-function">State Value Function and Action Value Function(Q-function)</h3>

<p>The state value function \(V_\pi(s)\) is the expected return starting from state \(s\) and following policy \(\pi\). It can be expressed as:</p>

<div style="overflow-x: auto;">
$$
V_\pi(s) = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right]
= \mathbb{E}_\pi \left[ G_t | S_t = s \right]
$$
</div>

<p>The Q-function represents the expected return when taking action a in state s at time t, and thereafter following policy π.</p>

<div style="overflow-x: auto;">
$$
q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]
$$
</div>

<p>In short,</p>

<ul>
  <li>
    <p>policy 
\(\pi\)
determines how to act in a given state \(s\)</p>
  </li>
  <li>
    <p>Value function \(V_\pi(s)\) evalutes how good it is to be in a specific state under policy \(\pi\)</p>
  </li>
  <li>
    <p>Action value function(Q-function) \(q_\pi(s, a)\) evalutes how good it is to take a specific action in a given state under policy \(\pi\)</p>
  </li>
</ul>

<div style="overflow-x: auto;">
$$
q_\pi(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right] = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' | s') q_\pi(s', a') \right]
$$
</div>

<h3 id="optimal-action-value-function">optimal Action Value Function</h3>

<p>The optimal action value function \(q^*(s, a)\) is the maximum expected return when taking action \(a\) in state \(s\) and thereafter following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
q^*(s, a) = \max_{\pi} q_\pi(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t = s, A_t = a \right] = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \max_{a'} q^*(s', a') \right]
$$
</div>

<h3 id="optimal-policy">optimal Policy</h3>

<p>We assume that the optimal action value function \(q^*(s, a)\) is known. Then the optimal policy at state \(s\) is defined as follows.</p>

<div style="overflow-x: auto;">
$$
\mu^*(s) = \arg \max_a q^*(s, a)
$$
</div>

<h2 id="dynamic-programming">Dynamic Programming</h2>

<p>Dynamic programming is a method used to solve complex problems by breaking them down into simpler subproblems. It is particularly useful in reinforcement learning for solving Markov Decision Processes (MDPs).</p>

<h3 id="3x4-grid-world">3x4 grid world</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---+---+---+---+
| S |   |   | G |
+---+---+---+---+
|   | # |   | B |
+---+---+---+---+
|   |   |   |   |
+---+---+---+---+
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">S</code>: Start position</li>
  <li><code class="language-plaintext highlighter-rouge">G</code>: Goal position</li>
  <li><code class="language-plaintext highlighter-rouge">#</code>: Obstacle or blocked cell</li>
  <li><code class="language-plaintext highlighter-rouge">B</code>: Bomb location with a reward of <code class="language-plaintext highlighter-rouge">-1.0</code></li>
  <li>Blank cells are navigable spaces.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">common.gridworld_render</span> <span class="k">as</span> <span class="n">render_helper</span>


<span class="k">class</span> <span class="nc">GridWorld</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># 행동 공간(가능한 행동들)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>  <span class="c1"># 행동의 의미
</span>            <span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">UP</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">:</span> <span class="sh">"</span><span class="s">DOWN</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="sh">"</span><span class="s">LEFT</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">3</span><span class="p">:</span> <span class="sh">"</span><span class="s">RIGHT</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>  <span class="c1"># 보상 맵(각 좌표의 보상 값)
</span>            <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
             <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
             <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>    <span class="c1"># 목표 상태(좌표)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>    <span class="c1"># 벽 상태(좌표)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>   <span class="c1"># 시작 상태(좌표)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>   <span class="c1"># 에이전트 초기 상태(좌표)
</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>

    <span class="k">def</span> <span class="nf">states</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
                <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="c1"># 이동 위치 계산
</span>        <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># 이동한 위치가 그리드 월드의 테두리 밖이나 벽인가?
</span>        <span class="k">if</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">elif</span> <span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

        <span class="k">return</span> <span class="n">next_state</span>  <span class="c1"># 다음 상태 반환
</span>
    <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>

    <span class="k">def</span> <span class="nf">render_v</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_q</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="class-gridworld">Class: <code class="language-plaintext highlighter-rouge">GridWorld</code></h3>
<p>This class represents a simple grid-based environment for reinforcement learning. It defines the grid’s structure, the agent’s movement, and the rewards associated with each state.</p>

<hr />

<h4 id="__init__-method"><strong><code class="language-plaintext highlighter-rouge">__init__</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># up, down, left, right
</span>    <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">up</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="sh">'</span><span class="s">down</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">2</span><span class="p">:</span> <span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="sh">'</span><span class="s">right</span><span class="sh">'</span>
    <span class="p">}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">])</span>
    <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
</code></pre></div></div>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">action_space</code></strong>: Defines the possible actions the agent can take:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">0</code>: Move up</li>
      <li><code class="language-plaintext highlighter-rouge">1</code>: Move down</li>
      <li><code class="language-plaintext highlighter-rouge">2</code>: Move left</li>
      <li><code class="language-plaintext highlighter-rouge">3</code>: Move right</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">action_meaning</code></strong>: Maps action indices to human-readable directions.</p>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">reward_map</code></strong>: A 2D NumPy array representing the grid. Each cell contains:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">0</code>: Neutral reward.</li>
      <li><code class="language-plaintext highlighter-rouge">1.0</code>: Positive reward (goal state).</li>
      <li><code class="language-plaintext highlighter-rouge">-1.0</code>: Negative reward (bomb state).</li>
      <li><code class="language-plaintext highlighter-rouge">None</code>: Represents an obstacle (wall).</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">start_state</code></strong>: The agent’s starting position <code class="language-plaintext highlighter-rouge">(2, 0)</code> (row 2, column 0).</p>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">wall_state</code></strong>: The position of the wall <code class="language-plaintext highlighter-rouge">(1, 1)</code> (row 1, column 1), which the agent cannot pass through.</p>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">goal_state</code></strong>: The position of the goal <code class="language-plaintext highlighter-rouge">(0, 3)</code> (row 0, column 3).</p>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">agent_state</code></strong>: Tracks the agent’s current position, initialized to the start state.</li>
</ol>

<hr />

<h4 id="properties"><strong>Properties</strong></h4>
<p>These properties provide useful information about the grid.</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">height</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the number of rows in the grid.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">width</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the number of columns in the grid.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">shape</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the grid’s dimensions as a tuple <code class="language-plaintext highlighter-rouge">(rows, columns)</code>.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">actions</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the list of possible actions.</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="state-method"><strong><code class="language-plaintext highlighter-rouge">state</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
            <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>A generator that iterates over all possible states (grid cells) in the environment.</li>
  <li>Each state is represented as a tuple <code class="language-plaintext highlighter-rouge">(row, column)</code>.</li>
</ul>

<hr />

<h4 id="next_state-method"><strong><code class="language-plaintext highlighter-rouge">next_state</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">:</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">if</span> <span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">return</span> <span class="n">next_state</span>
</code></pre></div></div>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">action_move_map</code></strong>: Maps actions to their corresponding movements:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">(-1, 0)</code>: Move up (decrease row index).</li>
      <li><code class="language-plaintext highlighter-rouge">(1, 0)</code>: Move down (increase row index).</li>
      <li><code class="language-plaintext highlighter-rouge">(0, -1)</code>: Move left (decrease column index).</li>
      <li><code class="language-plaintext highlighter-rouge">(0, 1)</code>: Move right (increase column index).</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">next_state</code></strong>: Calculates the agent’s next position based on the current state and action.</p>
  </li>
  <li><strong>Boundary Check</strong>:
    <ul>
      <li>If the next state is outside the grid’s boundaries, the agent stays in the current state.</li>
    </ul>
  </li>
  <li><strong>Wall Check</strong>:
    <ul>
      <li>If the next state is a wall, the agent stays in the current state.</li>
    </ul>
  </li>
  <li><strong>Returns</strong>: The valid next state after applying the action.</li>
</ol>

<hr />

<h4 id="reward-method"><strong><code class="language-plaintext highlighter-rouge">reward</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
</code></pre></div></div>

<ol>
  <li><strong>Inputs</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">state</code>: The current state.</li>
      <li><code class="language-plaintext highlighter-rouge">action</code>: The action taken.</li>
      <li><code class="language-plaintext highlighter-rouge">next_state</code>: The resulting state after the action.</li>
    </ul>
  </li>
  <li><strong>Returns</strong>: The reward associated with the <code class="language-plaintext highlighter-rouge">next_state</code>, as defined in the <code class="language-plaintext highlighter-rouge">reward_map</code>.</li>
</ol>

<hr />

<h3 id="summary-1">Summary</h3>
<p>The <code class="language-plaintext highlighter-rouge">GridWorld</code> class provides a simple environment for reinforcement learning:</p>
<ul>
  <li>It defines the grid layout, including walls, rewards, and penalties.</li>
  <li>It allows the agent to move within the grid while handling boundaries and obstacles.</li>
  <li>It provides rewards based on the agent’s position.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">()</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/gridworld1.png" alt="gridworld" width="100%" />
</div>

<h3 id="implementation-of-iterative-policy-evaluation">Implementation of Iterative Policy Evaluation</h3>

<p>First, let’s implement a function that performs a single step of the update.</p>

<ul>
  <li>pi(difaultdict) : <code class="language-plaintext highlighter-rouge">policy</code></li>
  <li>V (defaultdict) : <code class="language-plaintext highlighter-rouge">value function</code></li>
  <li>env(GridWorld) : <code class="language-plaintext highlighter-rouge">environment</code></li>
  <li>gamma (float) : <code class="language-plaintext highlighter-rouge">discount factor</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">})</span>
<span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">env</span><span class="p">.</span><span class="n">goal_state</span><span class="p">:</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="k">continue</span>

        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">new_V</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_prob</span> <span class="ow">in</span> <span class="n">action_probs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">new_V</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_V</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
</code></pre></div></div>

<p>If we try one step of the update, we can see the result below.</p>

<div align="center">
  <img src="/images/gridworld2.png" alt="gridworld" width="100%" />   
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">new_V</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_prob</span> <span class="ow">in</span> <span class="n">action_probs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">new_V</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_V</span>
</code></pre></div></div>

<p>this code is defined as follows.</p>

\[s' = f(s, a)\]

<p>and,</p>

\[V_{k + 1}(s) = \sum_{a} \pi(a | s) \left[r(s, a, s') + \gamma V_k(s') \right]\]

<p>Therefore, we continue repeating this process until the threshold is reached.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">policy_eval</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">old_V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">V</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">t</span><span class="p">:</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="n">t</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/gridworld3.png" alt="gridworld" width="100%" />
</div>

<h3 id="policy-iteration-method">Policy Iteration Method</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Optimal Policy</code>: \(\pi^*(s)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Value Function</code>: \(V^*(s)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Action Value Function</code>: \(Q^*(s, a)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Action</code>:</li>
</ul>
<div style="overflow-x: auto;">

$$ \mu^*(s) = \arg \max_a Q^*(s, a) $$

$$
= \arg \max_a \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]
$$
</div>

<p>What do we call the process of finding the optimal policy through repeated evaluation and greedification?</p>

<p>Policy Iteration is an algorithm for findings the optmal policy in a MDPs by alternating between two phase.</p>

<ul>
  <li>
    <p>1 <strong>Policy Evaluation</strong>: calculate the value function for the current policy by iteratively applying the Bellman expectation equation until convergence.</p>
  </li>
  <li>
    <p>2 <strong>Policy Improvement</strong>: update the policy to be greedy with respect to the current value function. This means for each state, selecting the action that maximizes expected value.</p>
  </li>
</ul>

<p>By repeating these two steps until the policy no longer changes, we can find the optimal policy. This approach is guaranteed to converge to the optimal policy in finite MDPs.</p>

<p>In gridworld, since states transition uniquely, we can define greedification as follows.</p>

<div style="overflow-x: auto;">
$$
\mu^*(s) = \arg \max_a \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]
= \arg \max_a \left[ R(s, a, s') + \gamma V^*(s') \right]
$$
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
        <span class="n">max_action</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">action_values</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="p">{</span><span class="n">a</span><span class="p">:</span> <span class="mf">0.0</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">()}</span>
        <span class="n">action_probs</span><span class="p">[</span><span class="n">max_action</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_probs</span>
    <span class="k">return</span> <span class="n">pi</span>

<span class="k">def</span> <span class="nf">policy_iter</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">is_render</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">})</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">policy_eval</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="n">new_pi</span> <span class="o">=</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_render</span><span class="p">:</span>
            <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_pi</span> <span class="o">==</span> <span class="n">pi</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="n">new_pi</span>
    <span class="k">return</span> <span class="n">pi</span><span class="p">,</span> <span class="n">V</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">policy_iter</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>this result is two step of the policy iteration.</p>

<div align="center">
  <img src="/images/gridworld4.png" alt="gridworld" width="100%" />
</div>

<p>Four steps of the policy iteration are as follows.</p>

<div align="center">
  <img src="/images/gridworld5.png" alt="gridworld" width="100%" />
</div>

<p>By implementing it this way, the value function of all states is updated multiple times. It’s too slow. Is there a way to update only one state’s value function and proceed?</p>

<h3 id="value-iteration-method">Value Iteration Method</h3>

<h4 id="why-value-iteration-works">Why Value Iteration Works</h4>

<p>Policy Iteration has two separate steps - policy evaluation (which runs until convergence) and policy improvement. This is computationally expensive because we’re repeatedly evaluating the entire state space multiple times before making a single policy improvement.</p>

<p>Value Iteration addresses this inefficiency by recognizing that:</p>

<ul>
  <li><strong>Similar Calculations</strong> - Both policy evaluation and improvement use the Bellman equation structure</li>
  <li><strong>Partial Convergence</strong> - We can improve the policy before the value function fully converges</li>
  <li><strong>Combined Steps</strong> - We can directly incorporate the max operation into the value update</li>
</ul>

<h4 id="how-value-iteration-works">How Value Iteration Works</h4>

<p>Value Iteration combines policy evaluation and improvement into a single update:</p>

\[V(s) \leftarrow \max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]\]

<p>This update directly finds the value of the best action for each state, effectively:</p>

<ul>
  <li>Assuming a greedy policy at each step</li>
  <li>Skipping the explicit policy representation</li>
  <li>Performing only one sweep through the state space per iteration</li>
</ul>

<h4 id="value-iteration-algorithm">Value Iteration Algorithm</h4>

<ol>
  <li>Initialize \(V(s) = 0\) for all states</li>
  <li>Repeat until convergence:
    <ul>
      <li>For each state \(s\):</li>
    </ul>
    <div style="overflow-x: auto;">
      $$V(s) \leftarrow \max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]$$
 </div>
  </li>
  <li>Extract the final policy:</li>
</ol>
<div style="overflow-x: auto;"> 
   $$\pi(s) = \arg\max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]$$
</div>

<h4 id="advantages-over-policy-iteration">Advantages Over Policy Iteration</h4>

<ul>
  <li><strong>Computational Efficiency</strong> - No need to perform full policy evaluation at each step</li>
  <li><strong>Fewer Iterations</strong> - Usually converges in fewer sweeps through the state space</li>
  <li><strong>Simplicity</strong> - Only need to maintain a value function, not an explicit policy</li>
  <li><strong>Direct Optimization</strong> - Works towards optimal values from the start</li>
</ul>

<p>For deterministic environments like our grid world example, the update becomes even simpler:</p>

<div style="overflow-x: auto;">
$$V(s) \leftarrow \max_a \left[R(s,a) + \gamma \cdot V(\text{next_state}(s,a)) \right]$$
</div>

<p>This makes Value Iteration particularly efficient for deterministic problems.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">value_iter_onestep</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">env</span><span class="p">.</span><span class="n">goal_state</span><span class="p">:</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">continue</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="k">def</span> <span class="nf">value_iter</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_render</span><span class="p">:</span>
            <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
        <span class="n">old_V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">value_iter_onestep</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">V</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">t</span><span class="p">:</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="n">t</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">value_iter</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
</code></pre></div></div>

<ol>
  <li>one step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld6.png" alt="gridworld" width="100%" />
</div>

<ol>
  <li>two step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld7.png" alt="gridworld" width="100%" />
</div>

<ol>
  <li>three step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld8.png" alt="gridworld" width="100%" />
</div>
<ol>
  <li>four step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld9.png" alt="gridworld" width="100%" />
</div>

<p>So, the result of optimal policy is as follows.</p>

<div align="center">
  <img src="/images/gridworld10.png" alt="gridworld" width="100%" />
</div>

<h2 id="monte-carlo-method">Monte Carlo Method</h2>

<p>We know the transition probabilities \(P( s, a)\) and the reward function \(R\), which allows us to apply Dynamic Programming.</p>

<p>Also, using dynamic programming (DP) is too complex to calculate the entire problem.</p>

<p><strong>What is Monte Carlo method?</strong></p>

<p>It assumes a value function for the agent to gain experience in an environment.
The experience mentioned here refers to the data (state, action, reward) obtained through the interaction between the environment and the agent.</p>

<p>The following situation can be considered: Think about all possible outcomes when rolling a dice twice.</p>

<div align="center">
  <div class="mermaid">
graph TD
    Start((Start))
    Start --&gt; D1((Die 1: 1))
    Start --&gt; D2((Die 1: 2))
    Start --&gt; D3((Die 1: 3))
    Start --&gt; D4((Die 1: 4))
    Start --&gt; D5((Die 1: 5))
    Start --&gt; D6((Die 1: 6))

    D1 --&gt; D1_1((Die 2: 1))
    D1 --&gt; D1_2((Die 2: 2))
    D1 --&gt; D1_3((Die 2: 3))
    D1 --&gt; D1_4((Die 2: 4))
    D1 --&gt; D1_5((Die 2: 5))
    D1 --&gt; D1_6((Die 2: 6))

    D2 --&gt; D2_1((Die 2: 1))
    D2 --&gt; D2_2((Die 2: 2))
    D2 --&gt; D2_3((Die 2: 3))
    D2 --&gt; D2_4((Die 2: 4))
    D2 --&gt; D2_5((Die 2: 5))
    D2 --&gt; D2_6((Die 2: 6))

    D3 --&gt; D3_1((Die 2: 1))
    D3 --&gt; D3_2((Die 2: 2))
    D3 --&gt; D3_3((Die 2: 3))
    D3 --&gt; D3_4((Die 2: 4))
    D3 --&gt; D3_5((Die 2: 5))
    D3 --&gt; D3_6((Die 2: 6))

    D4 --&gt; D4_1((Die 2: 1))
    D4 --&gt; D4_2((Die 2: 2))
    D4 --&gt; D4_3((Die 2: 3))
    D4 --&gt; D4_4((Die 2: 4))
    D4 --&gt; D4_5((Die 2: 5))
    D4 --&gt; D4_6((Die 2: 6))

    D5 --&gt; D5_1((Die 2: 1))
    D5 --&gt; D5_2((Die 2: 2))
    D5 --&gt; D5_3((Die 2: 3))
    D5 --&gt; D5_4((Die 2: 4))
    D5 --&gt; D5_5((Die 2: 5))
    D5 --&gt; D5_6((Die 2: 6))

    D6 --&gt; D6_1((Die 2: 1))
    D6 --&gt; D6_2((Die 2: 2))
    D6 --&gt; D6_3((Die 2: 3))
    D6 --&gt; D6_4((Die 2: 4))
    D6 --&gt; D6_5((Die 2: 5))
    D6 --&gt; D6_6((Die 2: 6))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class Start,D1,D2,D3,D4,D5,D6,D1_1,D1_2,D1_3,D1_4,D1_5,D1_6,D2_1,D2_2,D2_3,D2_4,D2_5,D2_6,D3_1,D3_2,D3_3,D3_4,D3_5,D3_6,D4_1,D4_2,D4_3,D4_4,D4_5,D4_6,D5_1,D5_2,D5_3,D5_4,D5_5,D5_6,D6_1,D6_2,D6_3,D6_4,D6_5,D6_6 circle;
  </div>
</div>

<p>If some outcomes represent a probability distribution, we use the sample distribution.
A sample distribution is a method of observing the results of actual sampling.</p>

<p>Let’s use the incremental method learned earlier to sample and calculate the expected value of the sum when two dice are rolled.</p>

<p><code class="language-plaintext highlighter-rouge">incremental method</code> : \(V_n = V_{n - 1} + \frac{1}{n} (s_n - V_{n - 1})\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trial</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">def</span> <span class="nf">Sample</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="n">trial</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">V</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nc">Sample</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">V</span> <span class="o">+=</span> <span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trial </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">: Sample mean = </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># result
# Trial 100: Sample mean = 7.119999999999997
# Trial 200: Sample mean = 6.8199999999999985
# Trial 300: Sample mean = 6.783333333333331
# Trial 400: Sample mean = 6.8575
# Trial 500: Sample mean = 6.844000000000001
# Trial 600: Sample mean = 6.861666666666671
# Trial 700: Sample mean = 6.8885714285714315
# Trial 800: Sample mean = 6.8999999999999995
# Trial 900: Sample mean = 6.948888888888891
# Trial 1000: Sample mean = 6.938000000000002
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Value - Function</code> : \(V_n = \mathbb{E_{\pi}}[G \mid s]\)</p>

<p>This method applies the Monte Carlo approach to estimate values.</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}
$$
</div>

<p>where \(G^{(i)}\) is the return of the \(i\)-th episode.</p>

<p>Let me explain the first trial episode.</p>

<div align="center">
  <div class="mermaid">
graph TD
    S((S))
    S --&gt;|reward_1| A((A))
    A --&gt;|reward_0| B((B))
    B --&gt;|reward_2| END((END))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class S,A,B,C,END circle;
  </div>
</div>

<div style="overflow-x: auto;">
$$
G^{(1)} = 1 + 0 + 2 = 3
$$
</div>

<p>The second trial episode is as follows.</p>

<div align="center">
  <div class="mermaid">
graph TD
    S((S))
    S --&gt;|reward_1| A((A))
    A --&gt;|reward_0| B((B))
    B --&gt;|reward_1| C((C))
    C --&gt;|reward_1| END((END))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class S,A,B,C,END circle;

  </div>
</div>

<div style="overflow-x: auto;">
$$
G^{(2)} = 1 + 0 + 1 + 1 = 3
$$
</div>

<p>As a result, the expected value is as follows.</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}}{2} = \frac{3 + 3}{2} = 3
$$
</div>

<p>Let’s calculate the value function for all states using the Monte Carlo method. If there are three states (A, B, C), sample data is obtained by performing actual actions.</p>

<div align="center">
  <div class="mermaid">
flowchart TD

    %% A 파이프라인 1
    A1((A)) --&gt; A2([...]) --&gt; A3([...]) --&gt; Aout((○))

    %% A 파이프라인 2
    A1b((A)) --&gt; A2b([...]) --&gt; A3b([...]) --&gt; Aoutb((○))


    %% B 파이프라인 1
    B1((B)) --&gt; B2([...]) --&gt; B3([...]) --&gt; Bout((○))

    %% B 파이프라인 2
    B1b((B)) --&gt; B2b([...]) --&gt; B3b([...]) --&gt; Boutb((○))

    %% C 파이프라인 1
    C1((C)) --&gt; C2([...]) --&gt; C3([...]) --&gt; Cout((○))

    %% C 파이프라인 2
    C1b((C)) --&gt; C2b([...]) --&gt; C3b([...]) --&gt; Coutb((○))

    %% 스타일 지정
    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px

    class A1,A2,A3,Aout,A1b,A2b,A3b,Aoutb aStyle
    class B1,B2,B3,Bout,B1b,B2b,B3b,Boutb bStyle
    class C1,C2,C3,Cout,C1b,C2b,C3b,Coutb cStyle
</div>
</div>

<p>Let’s consider starting from state A, taking actions according to policy \(\pi\), and reaching the final destination.</p>

<div align="center">
  <div class="mermaid">
graph TD
    A((A))
    A --&gt;|R0| B((B))
    B --&gt;|R1| C((C))
    C --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class A aStyle
    class B bStyle
    class C cStyle
    class END circle;
</div>
</div>

<p>The total rewards accumulated from state A to the end are as follows.</p>

<div style="overflow-x: auto;">
$$
G_A = R_0 + \gamma R_1 + \gamma^2 R_2
$$
</div>

<p>Let’s consider starting from state B.</p>

<div align="center">
  <div class="mermaid">
graph TD
    B((B)) --&gt;|R1| C((C))
    C --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class B bStyle
    class C cStyle
    class END circle;
</div>
</div>

<div style="overflow-x: auto;">
$$
G_B = R_1 + \gamma R_2
$$
</div>

<p>Let’s consider starting from state C.</p>

<div align="center">
  <div class="mermaid">
graph TD
    C((C)) --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class C cStyle
    class END circle;
</div>
</div>

<div style="overflow-x: auto;">
$$
G_C = R_2
$$
</div>

<p>So, the following sequence of calculations can eliminate redundant computations.</p>

<div style="overflow-x: auto;">
$$
G_C = R_2
$$
</div>

<div style="overflow-x: auto;">
$$
G_B = R_1 + \gamma G_C
$$
</div>

<div style="overflow-x: auto;">
$$
G_A = R_0 + \gamma G_B
$$
</div>

<h3 id="implement">implement</h3>

<p>Alright, according to the reading, we can implement this for the agent to interact with the environment.</p>

<div align="center">
  <img src="/images/RL.png" alt="RL" width="100%" />
</div>

<p>The start point is (0, 0), the end point is (5, 5), and the black cell represents a wall that the agent cannot pass.</p>

<div align="center">
  <img src="/images/monemap.png" alt="MM" width="80%" />
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">goal</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cnts</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">):</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span>
            <span class="n">self</span><span class="p">.</span><span class="n">cnts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">cnts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>


<span class="n">agent</span> <span class="o">=</span> <span class="nc">RandomAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1e3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Training Progress</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<p>The value function (expected value) of each cell obtained through the Monte Carlo method is as follows.</p>

<div align="center">
  <img src="/images/MM1.png" alt="MM" width="80%" />
</div>

<h3 id="policy-control-using-the-monte-carlo-method">Policy Control Using the Monte Carlo Method</h3>

<p>The optimal policy alternates between evaluation and improvement.</p>
<ul>
  <li><strong>Policy Evaluation</strong>: Calculate the value function for the current policy using the Monte Carlo method.</li>
  <li><strong>Policy Improvement</strong>: Update the policy to be greedy with respect to the current value function.</li>
</ul>

<p>State Value Function Evaluation</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">General Method</code> : \(V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}\)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Incremental Method</code> : \(V_{\pi}(s) = V_{\pi}(s) + \frac{1}{n} (G - V_{\pi}(s))\)</p>
  </li>
</ul>

<p>Q-Function Evaluation</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">General Method</code> : \(Q_{\pi}(s, a) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}\)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Incremental Method</code> : \(Q_{\pi}(s, a) = Q_{\pi}(s, a) + \frac{1}{n} (G - Q_{\pi}(s, a))\)</p>
  </li>
</ul>

<h3 id="important-concept">important concept</h3>

<ol>
  <li>
    <p>Use an epsilon-greedy policy to give the agent opportunities to explore.</p>
  </li>
  <li>
    <p>Train the model by applying an exponential moving average with a fixed value <code class="language-plaintext highlighter-rouge">a</code>, giving greater weight to more recent data.</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">common.gridworld_render</span> <span class="k">as</span> <span class="n">render_helper</span>


<span class="k">class</span> <span class="nc">GridWorld</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># 행동 공간(가능한 행동들)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>  <span class="c1"># 행동의 의미
</span>            <span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">UP</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">:</span> <span class="sh">"</span><span class="s">DOWN</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="sh">"</span><span class="s">LEFT</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">3</span><span class="p">:</span> <span class="sh">"</span><span class="s">RIGHT</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">reward_map</span> <span class="k">if</span> <span class="n">reward_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="p">])</span>

        <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="n">goal</span>
        <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span>
            <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">==</span> <span class="bp">None</span><span class="p">)))</span>  <span class="c1"># 벽 상태(좌표)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="n">start</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>   <span class="c1"># 에이전트 초기 상태(좌표)
</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>

    <span class="k">def</span> <span class="nf">states</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
                <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="c1"># 이동 위치 계산
</span>        <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># 이동한 위치가 그리드 월드의 테두리 밖이나 벽인가?
</span>        <span class="k">if</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">elif</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

        <span class="k">return</span> <span class="n">next_state</span>  <span class="c1"># 다음 상태 반환
</span>
    <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>

    <span class="k">def</span> <span class="nf">render_v</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_q</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>
</code></pre></div></div>

<p>Define the grid environment and the way the agent moves (policy).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">sys</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="k">def</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">action_size</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">qs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Q</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)]</span> <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">action_size</span><span class="p">)]</span>
    <span class="n">max_action</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">qs</span><span class="p">))</span>

    <span class="n">base_prob</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="o">/</span> <span class="n">action_size</span>
    <span class="n">action_probs</span> <span class="o">=</span> <span class="p">{</span><span class="n">action</span><span class="p">:</span> <span class="n">base_prob</span> <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">action_size</span><span class="p">)}</span>  <span class="c1">#{0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4}
</span>    <span class="n">action_probs</span><span class="p">[</span><span class="n">max_action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">action_probs</span>


<span class="k">class</span> <span class="nc">McAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># (첫 번째 개선) ε-탐욕 정책의 ε
</span>        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>    <span class="c1"># (두 번째 개선) Q 함수 갱신 시의 고정값 α
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># self.cnts = defaultdict(lambda: 0)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">):</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span> <span class="o">+</span> <span class="n">reward</span>
            <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="c1"># self.cnts[key] += 1
</span>            <span class="c1"># self.Q[key] += (G - self.Q[key]) / self.cnts[key]
</span>            <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>
            <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span>


<span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">goal</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">McAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Training Progress</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">Q</span><span class="p">)</span>
</code></pre></div></div>

<p>The agent alternates between exploration and exploitation over 10,000 episodes. Additionally, it reduces the weight of past experiences and assigns higher weight to the rewards obtained through current experiences.</p>

<p>The Q(S, a) for each state is as follows.</p>

<div align="center">
  <img src="/images/Figure_1.png" alt="bandit" width="100%" />
</div>

<p>And, The Optimal Policy for each state is as follows.</p>

<div style="display: flex; justify-content: center; gap: 10px;">
  <img src="/images/Figure_2.png" alt="bandit1" style="width: 48%;" />
  <img src="/images/Figure_3.png" alt="bandit2" style="width: 48%;" />
</div>


						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
			<div class="section section--grey">
				<div class="container">
					<div class="row">
						<div class="col-md-7">
							<div class="comments-area">
  <h3 class="comments-title">Comments</h3>
  <div id="comments-list" class="comments-list">
    <!-- Comments will be populated here -->
  </div>
  
  <div class="comment-respond">
    <h4 class="comment-reply-title">Leave a Comment</h4>
    <form id="comment-form" class="comment-form">
      <div class="form-group">
        <label for="name">Name</label>
        <input type="text" id="name" name="name" required class="form-control">
      </div>
      <div class="form-group">
        <label for="email">Email</label>
        <input type="email" id="email" name="email" required class="form-control">
      </div>
      <div class="form-group">
        <label for="comment">Comment</label>
        <textarea id="comment" name="comment" required class="form-control" rows="5"></textarea>
      </div>
      <div class="form-group admin-field">
        <label for="password">Admin Password</label>
        <input type="password" id="password" name="password" class="form-control" placeholder="For comment moderation">
      </div>
      <div class="form-submit">
        <button type="submit" class="btn btn--dark btn--rounded">Submit Comment</button>
      </div>
    </form>
  </div>
</div>

<script>
  (function() {
    // Simple storage for comments using localStorage
    const COMMENTS_STORAGE_KEY = 'page_comments_/page/reinforce/';
    
    // Load comments from storage
    function loadComments() {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      const commentsList = document.getElementById('comments-list');
      commentsList.innerHTML = '';
      
      if (comments.length === 0) {
        commentsList.innerHTML = '<div class="no-comments">No comments yet. Be the first to comment!</div>';
        return;
      }
      
      comments.forEach((comment, index) => {
        const commentDiv = document.createElement('div');
        commentDiv.className = 'comment';
        commentDiv.dataset.id = index;
        
        const commentHTML = `
          <div class="comment-meta">
            <div class="comment-author">
              <strong>${comment.name}</strong>
            </div>
            <div class="comment-metadata">
              <span>${new Date(comment.date).toLocaleDateString()} ${new Date(comment.date).toLocaleTimeString()}</span>
            </div>
          </div>
          <div class="comment-content">
            <p>${comment.text}</p>
          </div>
          <div class="comment-actions">
            <button class="btn-link reply-btn" data-id="${index}">
              <i class="icon icon--arrow-right"></i> Reply
            </button>
            <button class="btn-link delete-btn" data-id="${index}">
              <i class="icon icon--cross"></i> Delete
            </button>
          </div>
          <div class="reply-form-wrapper" id="reply-form-${index}" style="display: none;">
            <form class="reply-form" data-parent="${index}">
              <div class="form-group">
                <label for="reply-name-${index}">Name</label>
                <input type="text" id="reply-name-${index}" name="name" required class="form-control">
              </div>
              <div class="form-group">
                <label for="reply-comment-${index}">Reply</label>
                <textarea id="reply-comment-${index}" name="comment" required class="form-control" rows="3"></textarea>
              </div>
              <div class="form-submit">
                <button type="submit" class="btn btn--dark btn--rounded btn--sm">Submit Reply</button>
              </div>
            </form>
          </div>
          <div class="children" id="replies-${index}">
            ${renderReplies(comment.replies || [])}
          </div>
        `;
        
        commentDiv.innerHTML = commentHTML;
        commentsList.appendChild(commentDiv);
      });
      
      // Add event listeners to reply buttons
      document.querySelectorAll('.reply-btn').forEach(button => {
        button.addEventListener('click', function() {
          const commentId = this.dataset.id;
          const replyForm = document.getElementById(`reply-form-${commentId}`);
          replyForm.style.display = replyForm.style.display === 'none' ? 'block' : 'none';
        });
      });
      
      // Add event listeners to delete buttons
      document.querySelectorAll('.delete-btn').forEach(button => {
        button.addEventListener('click', function() {
          const commentId = this.dataset.id;
          const password = prompt('Enter admin password to delete:');
          
          if (password === 'admin123') { // Simple password for demo
            deleteComment(parseInt(commentId));
          } else {
            alert('Incorrect password');
          }
        });
      });
      
      // Add event listeners to reply forms
      document.querySelectorAll('.reply-form').forEach(form => {
        form.addEventListener('submit', function(e) {
          e.preventDefault();
          const parentId = parseInt(this.dataset.parent);
          const replyName = this.querySelector('[name="name"]').value;
          const replyText = this.querySelector('[name="comment"]').value;
          
          addReply(parentId, replyName, replyText);
          this.reset();
          document.getElementById(`reply-form-${parentId}`).style.display = 'none';
        });
      });
    }
    
    // Render replies
    function renderReplies(replies) {
      if (!replies || replies.length === 0) return '';
      
      let html = '';
      replies.forEach((reply, replyIndex) => {
        html += `
          <div class="comment child-comment" data-id="${replyIndex}">
            <div class="comment-meta">
              <div class="comment-author">
                <strong>${reply.name}</strong>
              </div>
              <div class="comment-metadata">
                <span>${new Date(reply.date).toLocaleDateString()} ${new Date(reply.date).toLocaleTimeString()}</span>
              </div>
            </div>
            <div class="comment-content">
              <p>${reply.text}</p>
            </div>
            <div class="comment-actions">
              <button class="btn-link delete-reply-btn" data-parent="${replyIndex}">
                <i class="icon icon--cross"></i> Delete
              </button>
            </div>
          </div>
        `;
      });
      return html;
    }
    
    // Add a new comment
    function addComment(name, email, text) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      comments.push({
        name: name,
        email: email,
        text: text,
        date: new Date().toISOString(),
        replies: []
      });
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Add a reply to a comment
    function addReply(parentId, name, text) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      if (!comments[parentId].replies) {
        comments[parentId].replies = [];
      }
      
      comments[parentId].replies.push({
        name: name,
        text: text,
        date: new Date().toISOString()
      });
      
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Delete a comment
    function deleteComment(commentId) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      comments.splice(commentId, 1);
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Event listener for comment form
    document.getElementById('comment-form').addEventListener('submit', function(e) {
      e.preventDefault();
      
      const name = document.getElementById('name').value;
      const email = document.getElementById('email').value;
      const comment = document.getElementById('comment').value;
      
      addComment(name, email, comment);
      this.reset();
    });
    
    // Initial load
    document.addEventListener('DOMContentLoaded', loadComments);
  })();
</script>

<style>
  /* Comments styling that matches Doks theme */
  .comments-area {
    margin-top: 2.5rem;
    font-family: 'Noto Sans', sans-serif;
  }
  
  .comments-title {
    margin-bottom: 1.5rem;
    font-size: 1.75em;
    font-weight: 600;
    color: #333;
  }
  
  .comment {
    margin-bottom: 1.5rem;
    padding: 1.25rem;
    background-color: #fff;
    border-radius: 4px;
    box-shadow: 0 1px 4px rgba(0,0,0,.04);
    border-left: 4px solid #253951;
    transition: all .2s ease;
  }
  
  .comment:hover {
    box-shadow: 0 1px 10px rgba(0,0,0,.08);
  }
  
  .comment-meta {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 0.75rem;
  }
  
  .comment-author {
    font-weight: 600;
  }
  
  .comment-metadata {
    font-size: 0.75em;
    color: #8a8a8a;
  }
  
  .comment-content {
    line-height: 1.6;
    color: #333;
  }
  
  .comment-content p {
    margin-bottom: 0.5rem;
  }
  
  .comment-actions {
    margin-top: 0.75rem;
    padding-top: 0.5rem;
    border-top: 1px solid #f5f5f5;
  }
  
  .btn-link {
    background: none;
    border: none;
    padding: 0;
    color: #253951;
    cursor: pointer;
    font-size: 0.85em;
    text-decoration: none;
    margin-right: 1rem;
    transition: color .2s ease;
  }
  
  .btn-link:hover {
    color: #0056b3;
    text-decoration: underline;
  }
  
  .reply-form-wrapper {
    margin-top: 1rem;
    margin-bottom: 1rem;
    padding: 1rem;
    background-color: #f9f9f9;
    border-radius: 4px;
  }
  
  .children {
    margin-top: 1rem;
    margin-left: 1.5rem;
    padding-left: 1rem;
    border-left: 1px solid #eaeaea;
  }
  
  .child-comment {
    margin-bottom: 1rem;
    border-left: 3px solid #6c757d;
  }
  
  .no-comments {
    padding: 1rem;
    background-color: #f5f5f5;
    border-radius: 4px;
    font-style: italic;
    color: #666;
  }
  
  /* Form styling */
  .comment-respond {
    margin-top: 2rem;
    padding: 1.5rem;
    background-color: #f9f9f9;
    border-radius: 4px;
  }
  
  .comment-reply-title {
    margin-bottom: 1.25rem;
    font-size: 1.25em;
    font-weight: 600;
    color: #333;
  }
  
  .comment-form .form-group {
    margin-bottom: 1rem;
  }
  
  .comment-form label {
    display: block;
    margin-bottom: 0.5rem;
    font-weight: 600;
    color: #333;
  }
  
  .comment-form .form-control {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid #e0e0e0;
    border-radius: 4px;
    background-color: #fff;
    font-family: inherit;
    font-size: 0.95em;
    transition: border-color .2s ease;
  }
  
  .comment-form .form-control:focus {
    border-color: #253951;
    outline: none;
  }
  
  .admin-field {
    opacity: 0.8;
  }
  
  .form-submit {
    margin-top: 1.5rem;
  }
  
  /* Match the Doks buttons */
  .btn {
    display: inline-block;
    font-weight: 500;
    text-align: center;
    white-space: nowrap;
    vertical-align: middle;
    user-select: none;
    padding: 0.75rem 1.25rem;
    font-size: 1rem;
    line-height: 1.5;
    border-radius: 4px;
    transition: all .2s ease-in-out;
    text-decoration: none;
    cursor: pointer;
  }
  
  .btn--dark {
    background-color: #253951;
    border-color: #253951;
    color: #fff;
  }
  
  .btn--dark:hover {
    background-color: #1a2a3c;
    border-color: #1a2a3c;
  }
  
  .btn--rounded {
    border-radius: 100px;
  }
  
  .btn--sm {
    padding: 0.5rem 1rem;
    font-size: 0.875rem;
  }
</style>

						</div><!-- /.col -->
					</div><!-- /.row -->
				</div><!-- /.container -->
			</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
				<div class="micro-nav">
	<div class="container">
		<div class="row">
			<div class="col-xs-12">
				<a href="/" class="micro-nav__back">
					<i class="icon icon--arrow-left"></i>
					Back to homepage
				</a><!-- /.micro-nav__back -->
			</div><!-- /.col -->
		</div><!-- /.row -->
	</div><!-- /.container -->
</div><!-- /.micro-nav -->

			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
						<a href="/" class="site-footer__logo">InchanBaek Note</a>
					
					
						<hr>
						<p class="site-footer__copyright">Copyright &copy; 2017. - InchanBaek Note <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
					<div class="col-sm-6 align-right">
						<ul class="social-list">
							
								<li>
									<a href="https://www.linkedin.com/in/inchan-baek-728197265/" target="_blank" class="social-list__item social-list__item--linkedin">
										<i class="icon icon--linkedin"></i>
									</a>
								</li>
							
								<li>
									<a href="https://www.instagram.com/in_chanchan/" target="_blank" class="social-list__item social-list__item--instagram">
										<i class="icon icon--instagram"></i>
									</a>
								</li>
							
								<li>
									<a href="https://www.youtube.com/@icb6048" target="_blank" class="social-list__item social-list__item--youtube">
										<i class="icon icon--youtube"></i>
									</a>
								</li>
							
						</ul><!-- /.social-list -->
					</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>





		</div><!-- /.js-footer-area -->
	</body>
</html>