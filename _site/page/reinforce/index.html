<!DOCTYPE html>
<html >
	<head>
		

<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<meta name="google-site-verification" content="googlef9130e9561130734.html" />
<title>Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology</title>

	<meta name="description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">


	<meta name="keywords" content="강화학습, 머신러닝, 인공지능, 인찬백, InchanBaek, 리워드, 에이전트, 액션, MDP, 마르코프 결정 과정, Q-러닝, reinforcement learning, machine learning, AI, reward, agent, action, Markov decision process, Q-learning, deep reinforcement learning">

<meta name="author" content="Inchan Baek">
<!-- Open Graph / Facebook -->
<meta property="og:type" content="website">
<meta property="og:url" content="http://localhost:4000/page/reinforce/">
<meta property="og:title" content="Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology">
<meta property="og:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">
<!-- Twitter -->
<meta property="twitter:card" content="summary">
<meta property="twitter:url" content="http://localhost:4000/page/reinforce/">
<meta property="twitter:title" content="Reinforcement Learning from Scratch | InchanBaek Note - AI, Architecture & Technology">
<meta property="twitter:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.">
<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="canonical" href="http://localhost:4000/page/reinforce/">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning from Scratch</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Reinforcement Learning from Scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques." />
<meta property="og:description" content="A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques." />
<link rel="canonical" href="http://localhost:4000/page/reinforce/" />
<meta property="og:url" content="http://localhost:4000/page/reinforce/" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reinforcement Learning from Scratch" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.","headline":"Reinforcement Learning from Scratch","url":"http://localhost:4000/page/reinforce/"}</script>
<!-- End Jekyll SEO tag -->

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/moonspam/NanumSquare@1.0/nanumsquare.css">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<link rel="stylesheet" href="/doks-theme/assets/css/custom.css">
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      securityLevel: 'loose'
    });
    
    // Find all pre blocks with mermaid class and render them
    document.querySelectorAll('pre code.language-mermaid').forEach(function(element) {
      // Get the mermaid code
      const mermaidCode = element.textContent;
      
      // Create a new div for the mermaid diagram
      const newDiv = document.createElement('div');
      newDiv.className = 'mermaid';
      newDiv.textContent = mermaidCode;
      
      // Replace the pre element with the new div
      const preElement = element.parentElement;
      preElement.parentElement.replaceChild(newDiv, preElement);
    });
    
    // Trigger mermaid render
    mermaid.init();
  });
</script>

	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
		


	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">InchanBaek Note</a>
					
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Reinforcement Learning from Scratch</h1>
								
								
									<p class="hero-subheader__desc">A complete guide to learning reinforcement learning from basics to advanced algorithms. It explains key concepts such as Markov decision processes, Q-learning, and policy gradient methods, along with step-by-step implementation techniques.</p>
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="content">
							<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h2 id="bandit-problem">Bandit Problem</h2>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>

<ul>
  <li>
    <p>Supervised Learning : When the input and output data are given, it is a method of modeling the relationship between input data and output data.</p>
  </li>
  <li>
    <p>Unsupervised Learning : When the input data is given, it is a method of finding the characteristics of the input data.</p>
  </li>
  <li>
    <p><strong>Reinforcement Learning</strong> : <strong>Agent</strong> is an entity that interacts with the <strong>environment</strong> and receives information about the environment to choose <strong>actions</strong> that <strong>maximize rewards</strong>.</p>
  </li>
</ul>

<h3 id="what-is-bandit-problem">What is Bandit problem?</h3>

<p>Bandit == Slot machine</p>

<p>Each slot machine has a different probability.</p>

<p>At first, we don’t know which slot machine is the best.</p>

<p>We need to find the good machine by actually playing.</p>

<p>The goal is to get as much reward as possible within a limited number of plays.</p>

<div align="center">
  <div class="mermaid">
    graph LR
    A[Agent] --&gt;|action| B[Environment]
    B --&gt;|reward| A
  </div>
</div>

<p><strong>The agent as a player selects actions in a given environment, and the environment provides rewards to the agent.</strong></p>

<p><strong>Goal</strong>: <strong>Select actions that maximize rewards</strong> -&gt; <strong>Get as many coins as possible</strong> -&gt; <strong>Find the best slot machine</strong></p>

<h3 id="value-and-action-value">Value and Action Value</h3>

<ul>
  <li><strong>Value</strong>: Expected reward that can be obtained in a specific state</li>
</ul>

\[E[R_t]\]

<ul>
  <li><strong>Action Value</strong>: Expected reward obtained as a result of an action</li>
</ul>

\[Q(A) = E[R_t | A]\]

<p>(E = Expectation, Q = Quality, A = Action, R = Reward)</p>

<p>Let’s calculate the expected rewards for slot machines a and b.</p>

<p>Below is a table for slot machine a.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Slot machine a</th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Coins obtainable</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">10</td>
    </tr>
    <tr>
      <td style="text-align: center">Reward probability</td>
      <td style="text-align: center">0.70</td>
      <td style="text-align: center">0.15</td>
      <td style="text-align: center">0.12</td>
      <td style="text-align: center">0.03</td>
    </tr>
  </tbody>
</table>

<p>Here’s a table for slot machine b.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Slot machine b</th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Coins obtainable</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">10</td>
    </tr>
    <tr>
      <td style="text-align: center">Reward probability</td>
      <td style="text-align: center">0.50</td>
      <td style="text-align: center">0.40</td>
      <td style="text-align: center">0.09</td>
      <td style="text-align: center">0.01</td>
    </tr>
  </tbody>
</table>

<p>The expected values for the two machines are:</p>

<ul>
  <li>Slot machine a: (0.7 * 0 + 0.15 * 1 + 0.12 * 5 + 0.03 * 10) = 1.05</li>
  <li>Slot machine b: (0.5 * 0 + 0.4 * 1 + 0.09 * 5 + 0.01 * 10) = 0.95</li>
</ul>

<p><strong>Slot machine a is better than slot machine b.</strong></p>

<h3 id="value-estimation">Value Estimation</h3>

<p>Let’s say the rewards obtained during n plays are R1, R2, …, Rn.
Then the action value estimate Qn can be calculated as follows:</p>

\[Q_n = \frac{R_1 + R_2 + ... + R_n}{n}\]

<p>However, if we estimate the value this way after n plays, the computational and memory load becomes large.
We can calculate the nth value estimate using the (n-1)th value estimate.</p>

\[Q_{n-1} = \frac{R_1 + R_2 + ... + R_{n-1}}{n-1}\]

<p>If we multiply both sides of this equation by (n-1):</p>

\[(n - 1)Q_{n-1} = R_1 + R_2 + ... + R_{n-1}\]

<p>Now we can calculate the nth value estimate:</p>

\[Q_n = \frac{1}{n} (R_1 + R_2 + ... + R_{n-1} + R_n)\]

\[=\frac{1}{n} (n - 1)Q_{n-1} + \frac{1}{n} R_n\]

\[= Q_{n - 1} + \frac{1}{n} (R_n - Q_{n - 1})\]

<h3 id="players-policy">Player’s Policy</h3>

<p>If we completely trust uncertain estimates, we might miss the best action. Therefore, the agent needs to reduce uncertainty and increase the reliability of estimation.</p>

<ul>
  <li><strong>Policy</strong>: The strategy that determines the actions an agent selects when interacting with the environment</li>
</ul>

<p>There are two policies that can be used to reduce uncertainty:</p>

<ol>
  <li><strong>Exploration</strong>: Selecting uncertain actions to gain information about the environment</li>
  <li><strong>Exploitation</strong>: Selecting the best action based on information available so far</li>
</ol>

<p><strong>Ultimately, reinforcement learning algorithms are about finding the right ‘balance between exploitation and exploration’!!!!</strong></p>

<h3 id="epsilon-greedy-policy">Epsilon-Greedy Policy</h3>
<p>This is one of the algorithms used to balance exploration and exploitation.
For example, if \(\epsilon\) = 0.1, it selects a random action with 10% probability and selects the best action with 90% probability.</p>

<h3 id="solving-the-bandit-problem">Solving the Bandit Problem</h3>

<ul>
  <li><strong>Action Value Estimation</strong>: Estimate the action value and select the best action.</li>
  <li><strong>Policy</strong>: Use the <strong>epsilon-greedy policy</strong> to balance <strong>exploration</strong> and <strong>exploitation</strong>.</li>
</ul>

<p>Let’s implement the above content in code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arms</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.38991635</span><span class="p">,</span> <span class="mf">0.5937864</span><span class="p">,</span>  <span class="mf">0.55356798</span><span class="p">,</span> <span class="mf">0.46228943</span><span class="p">,</span> <span class="mf">0.48251845</span><span class="p">,</span> <span class="mf">0.47595196</span><span class="p">,</span> <span class="mf">0.53560295</span><span class="p">,</span> <span class="mf">0.43374032</span><span class="p">,</span> <span class="mf">0.55913105</span><span class="p">,</span> <span class="mf">0.57484477</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
        <span class="n">rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">rate</span> <span class="o">&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span> <span class="p">:</span> 
            <span class="k">return</span> <span class="mi">0</span>


<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epslion</span><span class="p">,</span> <span class="n">action_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epslion</span> <span class="o">=</span> <span class="n">epslion</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Ns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Ns</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">])</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">Ns</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">epslion</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">steps</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="nc">Bandit</span><span class="p">()</span>

<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">()</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">bandit</span><span class="p">.</span><span class="nf">play</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
    <span class="n">rates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">act</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Actions</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Action</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<div align="center">
  <img src="/images/bandit.png" alt="bandit" width="100%" />
</div>

<p>After about 10,000 plays, it still doesn’t know that selecting the slot machine at index 1 as an action is optimal.
Let’s try with more steps.</p>

<div align="center">
  <img src="/images/bandit2.png" alt="bandit" width="100%" />
</div>

<p>After about 30,000 plays, it learns that selecting the slot machine at index 1 as an action is optimal.
It took an additional 20,000 plays to recognize a probability difference of about 2%.</p>

<h3 id="non-stationary-problem">Non-stationary Problem</h3>

<p>The bandit problem we’ve covered so far belongs to the category of <strong>stationary problems</strong>. A stationary problem is one where the probability distribution of rewards <strong>does not change</strong>. In the code above, you can see that the probabilities are fixed in the variable called rates.</p>

<p>However, in reality, the probability distribution of rewards often changes. This is called a <strong>non-stationary problem</strong>. How should we handle this?</p>

<p>First, in stationary problems, we updated the action value estimate with the following equation:</p>

\[Q_n = Q_{n - 1} + \frac{1}{n} (R_n - Q_{n - 1})\]

<p>But in <strong>non-stationary problems</strong>, we update the action value estimate with the following equation:</p>

\[Q_n = Q_{n - 1} + \alpha (R_n - Q_{n - 1})\]

<p>This method <strong>reduces the weight of rewards obtained long ago</strong> and <strong>increases the weight of recently obtained rewards</strong>. Here, \(\alpha\) is called the <strong>learning rate</strong>.</p>

<div style="overflow-x: auto;">

$$
= Q_{n - 1} + \alpha (R_n - Q_{n - 1})
$$
$$
= (1 - \alpha) Q_{n - 1} + \alpha R_n
$$
$$
= \alpha R_n + (1 - \alpha) {(\alpha R_{n - 1} + (1 - \alpha) Q_{n - 2})}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 Q_{n - 2}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} + (1 - \alpha)^3 Q_{n - 3}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} +
$$
$$
(1 - \alpha)^3 \alpha R_{n - 3} + ... + (1 - \alpha)^{n - 1} \alpha R_1 + (1 - \alpha)^n Q_0
$$

</div>

<p>\(Q_0\) is the initial value. Depending on the value we set, bias can occur in the learning results. However, when using sample averages, the bias disappears.</p>

<p>This method is called <strong>exponential moving average</strong> or <strong>exponentially weighted moving average</strong>.</p>

<ul>
  <li><strong>Exponential Weighted Moving Average</strong>: A method that gives <strong>more weight to recently obtained rewards</strong> and <strong>less weight to rewards obtained long ago</strong></li>
</ul>

<p>Let’s implement this in Python code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arms</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.38991635</span><span class="p">,</span> <span class="mf">0.7837864</span><span class="p">,</span>  <span class="mf">0.55356798</span><span class="p">,</span> <span class="mf">0.46228943</span><span class="p">,</span> <span class="mf">0.48251845</span><span class="p">,</span> <span class="mf">0.47595196</span><span class="p">,</span> <span class="mf">0.53560295</span><span class="p">,</span> <span class="mf">0.43374032</span><span class="p">,</span> <span class="mf">0.55913105</span><span class="p">,</span> <span class="mf">0.57484477</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
        <span class="n">rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rates</span> <span class="o">+=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rates</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">rate</span> <span class="o">&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span> <span class="p">:</span> 
            <span class="k">return</span> <span class="mi">0</span>

<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epslion</span><span class="p">,</span> <span class="n">action_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epslion</span> <span class="o">=</span> <span class="n">epslion</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">action</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">epslion</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Qs</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> 

<span class="n">steps</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="nc">Bandit</span><span class="p">()</span>

<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">()</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">bandit</span><span class="p">.</span><span class="nf">play</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
    <span class="n">rates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_reward</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">act</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Total Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Actions</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Action</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Reward</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/bandit3.png" alt="bandit" width="100%" />
</div>

<p>When we set the fixed value \(\alpha\) = 0.8, we can see that the results converge faster than when using sample averages.</p>

<h3 id="summary">Summary</h3>

<ul>
  <li><strong>Bandit Problem</strong>: A fundamental problem in reinforcement learning where the goal is to find a method that maximizes rewards among multiple slot machines</li>
  <li><strong>Action Value</strong>: The expected reward obtained as a result of an action</li>
  <li><strong>Policy</strong>: The strategy that determines the actions an agent selects when interacting with the environment</li>
  <li><strong>Epsilon-Greedy Policy</strong>: One of the algorithms used to balance <strong>exploration and exploitation</strong></li>
  <li><strong>Non-stationary Problem</strong>: A problem where the probability distribution of rewards changes</li>
  <li><strong>Exponential Weighted Moving Average</strong>: A method that gives <strong>more weight to recently obtained rewards</strong> and <strong>less weight to rewards obtained long ago</strong></li>
</ul>

<h2 id="markov-decision-process">Markov Decision Process</h2>

<p>Let’s examine problems where the state of the environment changes according to an agent’s actions.</p>

<h3 id="what-is-a-markov-decision-process">What is a Markov Decision Process?</h3>

<ul>
  <li>
    <p><strong>Markov Decision Process (MDP)</strong>: A method of modeling an environment where the agent interacts with the environment, and the environment’s state satisfies the Markov property</p>
  </li>
  <li>
    <p><strong>Markov Property</strong>: The property where the <strong>future state depends only on the current state</strong></p>
  </li>
</ul>

<p>MDPs require the concept of time. At a specific time, the agent takes an action, and as a result, transitions to a new state. The time unit in this case is called a time step.</p>

<div align="center">
  <div class="mermaid">
    graph LR
    A[Agent] --&gt;|action| B[Environment]
    B --&gt;|reward, state| A
  </div>
</div>

<ul>
  <li><strong>State Transition</strong>: How does the state transition?</li>
  <li><strong>Reward</strong>: How is the reward given?</li>
  <li><strong>Policy</strong>: How does the agent determine its actions?</li>
</ul>

<p>The above three elements must be expressed in formulas.</p>

<p>If the state transition is <strong>deterministic</strong>, the next state s’ depends only on the current state s and action a.</p>

<p><strong>State transition function</strong> =&gt; 
\(s' = f(s, a)\)</p>

<p>If the state transition is <strong>probabilistic</strong>, the next state s’ depends only on the current state s and action a.</p>

<p><strong>State transition probability</strong> =&gt;
\(P(s' | s, a)\)</p>

<h3 id="reward-function">Reward Function</h3>

<p>The <strong>reward function</strong> returns the reward for state s and action a. It returns the reward received when the agent takes action a in state s and moves to the next state s’.</p>

<p><strong>Reward function</strong> =&gt;
\(r(s, a, s')\)</p>

<h3 id="agents-policy">Agent’s Policy</h3>

<p>The agent’s <strong>policy</strong> refers to how the agent determines its actions. The agent determines its actions based solely on the <strong>‘current state’</strong>.
This is because <strong>‘all the information needed about the environment is contained in the current state’</strong>.</p>

<p>A policy that the agent decides probabilistically can be expressed as follows:</p>

<p><strong>Policy</strong> =&gt;
\(\pi(a | s) = P(a | s)\)</p>

<h3 id="goal-of-mdp">Goal of MDP</h3>

<p>The goal of MDP is to find a policy that maximizes rewards. The agent behaves according to the policy 
\(\pi(a | s)\)
The next state is determined according to that action and the state transition probability \(P(s' | s, a)\). And the agent receives rewards according to the reward function \(r(s, a, s')\).</p>

<h3 id="return">Return</h3>

<p>The state at time t is \(S_t\), according to the policy \(\pi\), the action is \(A_t\), the reward is \(R_t\), and this leads to a flow that transitions to the new state \(S_{t+1}\). The return at this time can be defined as follows:</p>

\[G_t = R_t + rR_{t+1} + r^2R_{t+2} + ... = \sum_{k=0}^{\infty} r^k R_{t+k}\]

<p>As time passes, the reward decreases exponentially due to \(\gamma\).</p>

<h3 id="state-value-function">State Value Function</h3>

<p>The agent’s goal is to maximize returns. Even if an agent starts in the same state, the returns can vary for each episode. To respond to such stochastic behavior, we use the expectation, i.e., the expected return, as an indicator.</p>

<p>The state value function is a function that represents the expected value of rewards that can be received in the future, starting from a specific state in reinforcement learning. It is generally represented as \(V(s)\), where \(s\) represents the state. The state value function is calculated according to policy \(\pi\), and is defined by the following formula:</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s, \pi \right]
$$
</div>

\[= \mathbb{E}_\pi \left[G_t \mid S_t = s \right]\]

<p>Where:</p>
<ul>
  <li>\(\mathbb{E}_\pi\): Expected value according to policy \(\pi\)</li>
  <li>\(\gamma\): Discount rate (0 ≤ \(\gamma\) &lt; 1)</li>
  <li>\(R_{t+1}\): Reward at time \(t+1\)</li>
  <li>\(S_0 = s\): Initial state</li>
</ul>

<p>In other words, the state value function is used to predict the total rewards that will be received in the long term, starting from a specific state, when following a given policy. This plays an important role in evaluating the quality of a policy or finding the optimal policy.</p>

<h3 id="optimal-policy-and-optimal-value-function">Optimal Policy and Optimal Value Function</h3>

<p>In reinforcement learning, the optimal policy \(\pi^*\) is a policy that maximizes the expected reward in all states. If the agent follows the optimal policy, it can obtain the maximum possible reward.</p>

<p>The optimal value function \(V^*(s)\) is the sum of expected rewards that can be obtained when starting from state \(s\) and following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
V^*(s) = \max_{\pi} V^{\pi}(s) = \max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s \right]
$$
</div>

<p>Similarly, the optimal action-value function \(Q^*(s,a)\) is the sum of expected rewards that can be obtained when taking action \(a\) in state \(s\) and thereafter following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
Q^*(s,a) = \max_{\pi} Q^{\pi}(s,a) = \max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right]
$$
</div>

<p>The optimal policy and optimal value function can be defined through the Bellman Optimality Equation:</p>

<div style="overflow-x: auto;">
$$
V^*(s) = \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]
$$
</div>

<div style="overflow-x: auto;">
$$
Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')
$$
</div>

<p>The goal of reinforcement learning is to find such an optimal policy or optimal value function.</p>

<h2 id="bellman-equation">Bellman equation</h2>

<p>First, Summary of the Above.</p>

<ul>
  <li><strong>❓ What is an MDP?</strong></li>
</ul>

<p>An MDP is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under the control of an agent.</p>

<p>It consists of:</p>

<ul>
  <li>A set of <strong>states (S)</strong></li>
  <li>A set of <strong>actions (A)</strong></li>
  <li>A <strong>transition probability function (P)</strong></li>
  <li>A <strong>reward function (R)</strong></li>
  <li>A <strong>discount factor (γ)</strong></li>
</ul>

<p>So, MDP is the <strong>foundation of reinforcement learning</strong>, where an agent learns to choose actions that maximize cumulative reward over time.</p>

<ul>
  <li><strong>❓Why is important Bellman equation in MDP?</strong></li>
</ul>

<p>The <strong>Bellman equation</strong> is important in Markov Decision Processes (MDPs) because it provides a <strong>recursive decomposition of the value function</strong>, which represents the expected return starting from a given state. It serves as the <strong>foundation for many reinforcement learning algorithms</strong>, enabling <strong>efficient computation of optimal policies</strong> by breaking down complex problems into smaller subproblems.</p>

<p>🔑 <strong>Bellman Equation – Easy Explanation (with Keywords)</strong></p>
<ul>
  <li>
    <p><strong>The Bellman equation expresses</strong>
“<strong>What kind of future reward can I expect if I act well in this state?</strong>”</p>
  </li>
  <li>
    <p><strong>It uses recursion to break down a complex problem into smaller subproblems.</strong></p>
  </li>
  <li>
    <p><strong>This allows us to efficiently and systematically optimize the overall policy.</strong></p>
  </li>
  <li>
    <p><strong>Many reinforcement learning algorithms like Q-learning and Value Iteration</strong>
are based on the Bellman equation.</p>
  </li>
</ul>

<h3 id="derivation-of-bellman-equation">Derivation of Bellman Equation.</h3>

<p>First, let’s define ‘Return at time t’ as the sum of rewards from time ‘t’</p>

<div style="overflow-x: auto;">
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$
</div>

<p>Second, What is ‘Return at time t + 1’?</p>
<div style="overflow-x: auto;">
$$
G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
</div>

<p>So, we can rearrange the equation as above two equatios.</p>

<div style="overflow-x: auto;">
$$
G_t = R_t + \gamma G_{t+1}
$$
</div>

<p>We know the relation between \(G_t\) and \(G_{t+1}\).</p>

<p>Based on the state-vlaue function \(V_\pi(s)\) we obtained earlier, we can derived the following conclusion.</p>

<div style="overflow-x: auto;">
$$
V_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
    = \mathbb{E}_\pi[R_t + \gamma G_{t+1} | S_t = s]
    = \mathbb{E}_\pi[R_t | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
$$
</div>

<p>(Since Linearity of Expectation 👉 \(\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]\))</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        s((s)) --&gt; A((A))
        s((s)) --&gt; B((B))
        s((s)) --&gt; C((C))
        A --&gt; A1((A1))
        A --&gt; A2((A2))
        B --&gt; B1((B1))
        B --&gt; B2((B2))
        C --&gt; C1((C1))
        C --&gt; C2((C2))
        style s fill:#ffffff,stroke:#000000,stroke-width:2px
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
        style A1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style A2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C2 fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>we define \(\pi(a | s)\) as the probability of taking action \(a\) in state \(s\).</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        s((s)) --&gt; A((A))
        s((s)) --&gt; B((B))
        s((s)) --&gt; C((C))
        style s fill:#ffffff,stroke:#000000,stroke-width:2px
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>so 
\(\pi(a_1 | s) = A\), 
\(\pi(a_2 | s) = B\), 
\(\pi(a_3 | s) = C\)</p>

<p>and we choose the action along with the policy \(\pi\). we move \(s\) to \(s'\) with the probability 
\(P(s' | s, a)\). (P is the transition probability function)</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph TD
        A((A)) --&gt; A1((A1))
        A --&gt; A2((A2))
        B((B)) --&gt; B1((B1))
        B --&gt; B2((B2))
        C((C)) --&gt; C1((C1))
        C --&gt; C2((C2))
        style A fill:#ffffff,stroke:#000000,stroke-width:2px
        style B fill:#ffffff,stroke:#000000,stroke-width:2px
        style C fill:#ffffff,stroke:#000000,stroke-width:2px
        style A1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style A2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style B2 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C1 fill:#ffffff,stroke:#000000,stroke-width:2px
        style C2 fill:#ffffff,stroke:#000000,stroke-width:2px
    </div>
</div>

<p>According to above graph,</p>

\[A_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[A_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

\[B_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[B_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

\[C_1 = P(s' | s, a_1) * \pi(a_1 | s)\]

\[C_2 = P(s' | s, a_2) * \pi(a_2 | s)\]

<p>Let’s generalize the above equation.</p>

<div style="overflow-x: auto;">

$$
\mathbb{E}_\pi[R_t | S_t = s] = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s')
$$

</div>

<div style="overflow-x: auto;">

$$
V_\pi(s) = \mathbb{E}_\pi[R_t | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
$$


$$
= \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s') + \gamma \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) V_\pi(s')
$$

$$
= \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right]
$$

</div>

<p>This is the <strong>Bellman euqation</strong> for the state value function.</p>

<h3 id="state-value-function-and-action-value-functionq-function">State Value Function and Action Value Function(Q-function)</h3>

<p>The state value function \(V_\pi(s)\) is the expected return starting from state \(s\) and following policy \(\pi\). It can be expressed as:</p>

<div style="overflow-x: auto;">
$$
V_\pi(s) = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right]
= \mathbb{E}_\pi \left[ G_t | S_t = s \right]
$$
</div>

<p>The Q-function represents the expected return when taking action a in state s at time t, and thereafter following policy π.</p>

<div style="overflow-x: auto;">
$$
q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]
$$
</div>

<p>In short,</p>

<ul>
  <li>
    <p>policy 
\(\pi\)
determines how to act in a given state \(s\)</p>
  </li>
  <li>
    <p>Value function \(V_\pi(s)\) evalutes how good it is to be in a specific state under policy \(\pi\)</p>
  </li>
  <li>
    <p>Action value function(Q-function) \(q_\pi(s, a)\) evalutes how good it is to take a specific action in a given state under policy \(\pi\)</p>
  </li>
</ul>

<div style="overflow-x: auto;">
$$
q_\pi(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right] = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' | s') q_\pi(s', a') \right]
$$
</div>

<h3 id="optimal-action-value-function">optimal Action Value Function</h3>

<p>The optimal action value function \(q^*(s, a)\) is the maximum expected return when taking action \(a\) in state \(s\) and thereafter following the optimal policy:</p>

<div style="overflow-x: auto;">
$$
q^*(s, a) = \max_{\pi} q_\pi(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t = s, A_t = a \right] = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \max_{a'} q^*(s', a') \right]
$$
</div>

<h3 id="optimal-policy">optimal Policy</h3>

<p>We assume that the optimal action value function \(q^*(s, a)\) is known. Then the optimal policy at state \(s\) is defined as follows.</p>

<div style="overflow-x: auto;">
$$
\mu^*(s) = \arg \max_a q^*(s, a)
$$
</div>

<h2 id="dynamic-programming">Dynamic Programming</h2>

<p>Dynamic programming is a method used to solve complex problems by breaking them down into simpler subproblems. It is particularly useful in reinforcement learning for solving Markov Decision Processes (MDPs).</p>

<h3 id="3x4-grid-world">3x4 grid world</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---+---+---+---+
| S |   |   | G |
+---+---+---+---+
|   | # |   | B |
+---+---+---+---+
|   |   |   |   |
+---+---+---+---+
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">S</code>: Start position</li>
  <li><code class="language-plaintext highlighter-rouge">G</code>: Goal position</li>
  <li><code class="language-plaintext highlighter-rouge">#</code>: Obstacle or blocked cell</li>
  <li><code class="language-plaintext highlighter-rouge">B</code>: Bomb location with a reward of <code class="language-plaintext highlighter-rouge">-1.0</code></li>
  <li>Blank cells are navigable spaces.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">common.gridworld_render</span> <span class="k">as</span> <span class="n">render_helper</span>


<span class="k">class</span> <span class="nc">GridWorld</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># 행동 공간(가능한 행동들)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>  <span class="c1"># 행동의 의미
</span>            <span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">UP</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">:</span> <span class="sh">"</span><span class="s">DOWN</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="sh">"</span><span class="s">LEFT</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">3</span><span class="p">:</span> <span class="sh">"</span><span class="s">RIGHT</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>  <span class="c1"># 보상 맵(각 좌표의 보상 값)
</span>            <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
             <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
             <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>    <span class="c1"># 목표 상태(좌표)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>    <span class="c1"># 벽 상태(좌표)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>   <span class="c1"># 시작 상태(좌표)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>   <span class="c1"># 에이전트 초기 상태(좌표)
</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>

    <span class="k">def</span> <span class="nf">states</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
                <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="c1"># 이동 위치 계산
</span>        <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># 이동한 위치가 그리드 월드의 테두리 밖이나 벽인가?
</span>        <span class="k">if</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">elif</span> <span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

        <span class="k">return</span> <span class="n">next_state</span>  <span class="c1"># 다음 상태 반환
</span>
    <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>

    <span class="k">def</span> <span class="nf">render_v</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_q</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="class-gridworld">Class: <code class="language-plaintext highlighter-rouge">GridWorld</code></h3>
<p>This class represents a simple grid-based environment for reinforcement learning. It defines the grid’s structure, the agent’s movement, and the rewards associated with each state.</p>

<hr />

<h4 id="__init__-method"><strong><code class="language-plaintext highlighter-rouge">__init__</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># up, down, left, right
</span>    <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">up</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="sh">'</span><span class="s">down</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">2</span><span class="p">:</span> <span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="sh">'</span><span class="s">right</span><span class="sh">'</span>
    <span class="p">}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">])</span>
    <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
</code></pre></div></div>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">action_space</code></strong>: Defines the possible actions the agent can take:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">0</code>: Move up</li>
      <li><code class="language-plaintext highlighter-rouge">1</code>: Move down</li>
      <li><code class="language-plaintext highlighter-rouge">2</code>: Move left</li>
      <li><code class="language-plaintext highlighter-rouge">3</code>: Move right</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">action_meaning</code></strong>: Maps action indices to human-readable directions.</p>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">reward_map</code></strong>: A 2D NumPy array representing the grid. Each cell contains:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">0</code>: Neutral reward.</li>
      <li><code class="language-plaintext highlighter-rouge">1.0</code>: Positive reward (goal state).</li>
      <li><code class="language-plaintext highlighter-rouge">-1.0</code>: Negative reward (bomb state).</li>
      <li><code class="language-plaintext highlighter-rouge">None</code>: Represents an obstacle (wall).</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">start_state</code></strong>: The agent’s starting position <code class="language-plaintext highlighter-rouge">(2, 0)</code> (row 2, column 0).</p>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">wall_state</code></strong>: The position of the wall <code class="language-plaintext highlighter-rouge">(1, 1)</code> (row 1, column 1), which the agent cannot pass through.</p>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">goal_state</code></strong>: The position of the goal <code class="language-plaintext highlighter-rouge">(0, 3)</code> (row 0, column 3).</p>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">agent_state</code></strong>: Tracks the agent’s current position, initialized to the start state.</li>
</ol>

<hr />

<h4 id="properties"><strong>Properties</strong></h4>
<p>These properties provide useful information about the grid.</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">height</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the number of rows in the grid.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">width</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the number of columns in the grid.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">shape</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the grid’s dimensions as a tuple <code class="language-plaintext highlighter-rouge">(rows, columns)</code>.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">actions</code></strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>
</code></pre></div>    </div>
    <ul>
      <li>Returns the list of possible actions.</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="state-method"><strong><code class="language-plaintext highlighter-rouge">state</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
            <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>A generator that iterates over all possible states (grid cells) in the environment.</li>
  <li>Each state is represented as a tuple <code class="language-plaintext highlighter-rouge">(row, column)</code>.</li>
</ul>

<hr />

<h4 id="next_state-method"><strong><code class="language-plaintext highlighter-rouge">next_state</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">:</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">if</span> <span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">return</span> <span class="n">next_state</span>
</code></pre></div></div>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">action_move_map</code></strong>: Maps actions to their corresponding movements:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">(-1, 0)</code>: Move up (decrease row index).</li>
      <li><code class="language-plaintext highlighter-rouge">(1, 0)</code>: Move down (increase row index).</li>
      <li><code class="language-plaintext highlighter-rouge">(0, -1)</code>: Move left (decrease column index).</li>
      <li><code class="language-plaintext highlighter-rouge">(0, 1)</code>: Move right (increase column index).</li>
    </ul>
  </li>
  <li>
    <p><strong><code class="language-plaintext highlighter-rouge">next_state</code></strong>: Calculates the agent’s next position based on the current state and action.</p>
  </li>
  <li><strong>Boundary Check</strong>:
    <ul>
      <li>If the next state is outside the grid’s boundaries, the agent stays in the current state.</li>
    </ul>
  </li>
  <li><strong>Wall Check</strong>:
    <ul>
      <li>If the next state is a wall, the agent stays in the current state.</li>
    </ul>
  </li>
  <li><strong>Returns</strong>: The valid next state after applying the action.</li>
</ol>

<hr />

<h4 id="reward-method"><strong><code class="language-plaintext highlighter-rouge">reward</code> Method</strong></h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
</code></pre></div></div>

<ol>
  <li><strong>Inputs</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">state</code>: The current state.</li>
      <li><code class="language-plaintext highlighter-rouge">action</code>: The action taken.</li>
      <li><code class="language-plaintext highlighter-rouge">next_state</code>: The resulting state after the action.</li>
    </ul>
  </li>
  <li><strong>Returns</strong>: The reward associated with the <code class="language-plaintext highlighter-rouge">next_state</code>, as defined in the <code class="language-plaintext highlighter-rouge">reward_map</code>.</li>
</ol>

<hr />

<h3 id="summary-1">Summary</h3>
<p>The <code class="language-plaintext highlighter-rouge">GridWorld</code> class provides a simple environment for reinforcement learning:</p>
<ul>
  <li>It defines the grid layout, including walls, rewards, and penalties.</li>
  <li>It allows the agent to move within the grid while handling boundaries and obstacles.</li>
  <li>It provides rewards based on the agent’s position.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">()</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/gridworld1.png" alt="gridworld" width="100%" />
</div>

<h3 id="implementation-of-iterative-policy-evaluation">Implementation of Iterative Policy Evaluation</h3>

<p>First, let’s implement a function that performs a single step of the update.</p>

<ul>
  <li>pi(difaultdict) : <code class="language-plaintext highlighter-rouge">policy</code></li>
  <li>V (defaultdict) : <code class="language-plaintext highlighter-rouge">value function</code></li>
  <li>env(GridWorld) : <code class="language-plaintext highlighter-rouge">environment</code></li>
  <li>gamma (float) : <code class="language-plaintext highlighter-rouge">discount factor</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">})</span>
<span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">env</span><span class="p">.</span><span class="n">goal_state</span><span class="p">:</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="k">continue</span>

        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">new_V</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_prob</span> <span class="ow">in</span> <span class="n">action_probs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">new_V</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_V</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
</code></pre></div></div>

<p>If we try one step of the update, we can see the result below.</p>

<div align="center">
  <img src="/images/gridworld2.png" alt="gridworld" width="100%" />   
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">new_V</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_prob</span> <span class="ow">in</span> <span class="n">action_probs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">new_V</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_V</span>
</code></pre></div></div>

<p>this code is defined as follows.</p>

\[s' = f(s, a)\]

<p>and,</p>

\[V_{k + 1}(s) = \sum_{a} \pi(a | s) \left[r(s, a, s') + \gamma V_k(s') \right]\]

<p>Therefore, we continue repeating this process until the threshold is reached.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">policy_eval</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">old_V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">eval_onestep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">V</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">t</span><span class="p">:</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="n">t</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>
</code></pre></div></div>

<div align="center">
  <img src="/images/gridworld3.png" alt="gridworld" width="100%" />
</div>

<h3 id="policy-iteration-method">Policy Iteration Method</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Optimal Policy</code>: \(\pi^*(s)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Value Function</code>: \(V^*(s)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Action Value Function</code>: \(Q^*(s, a)\)</li>
  <li><code class="language-plaintext highlighter-rouge">Optimal Action</code>:</li>
</ul>
<div style="overflow-x: auto;">

$$ \mu^*(s) = \arg \max_a Q^*(s, a) $$

$$
= \arg \max_a \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]
$$
</div>

<p>What do we call the process of finding the optimal policy through repeated evaluation and greedification?</p>

<p>Policy Iteration is an algorithm for findings the optmal policy in a MDPs by alternating between two phase.</p>

<ul>
  <li>
    <p>1 <strong>Policy Evaluation</strong>: calculate the value function for the current policy by iteratively applying the Bellman expectation equation until convergence.</p>
  </li>
  <li>
    <p>2 <strong>Policy Improvement</strong>: update the policy to be greedy with respect to the current value function. This means for each state, selecting the action that maximizes expected value.</p>
  </li>
</ul>

<p>By repeating these two steps until the policy no longer changes, we can find the optimal policy. This approach is guaranteed to converge to the optimal policy in finite MDPs.</p>

<p>In gridworld, since states transition uniquely, we can define greedification as follows.</p>

<div style="overflow-x: auto;">
$$
\mu^*(s) = \arg \max_a \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]
= \arg \max_a \left[ R(s, a, s') + \gamma V^*(s') \right]
$$
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
        <span class="n">max_action</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">action_values</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="p">{</span><span class="n">a</span><span class="p">:</span> <span class="mf">0.0</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">()}</span>
        <span class="n">action_probs</span><span class="p">[</span><span class="n">max_action</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_probs</span>
    <span class="k">return</span> <span class="n">pi</span>

<span class="k">def</span> <span class="nf">policy_iter</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">is_render</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">})</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">policy_eval</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="n">new_pi</span> <span class="o">=</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_render</span><span class="p">:</span>
            <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_pi</span> <span class="o">==</span> <span class="n">pi</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="n">new_pi</span>
    <span class="k">return</span> <span class="n">pi</span><span class="p">,</span> <span class="n">V</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">policy_iter</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>this result is two step of the policy iteration.</p>

<div align="center">
  <img src="/images/gridworld4.png" alt="gridworld" width="100%" />
</div>

<p>Four steps of the policy iteration are as follows.</p>

<div align="center">
  <img src="/images/gridworld5.png" alt="gridworld" width="100%" />
</div>

<p>By implementing it this way, the value function of all states is updated multiple times. It’s too slow. Is there a way to update only one state’s value function and proceed?</p>

<h3 id="value-iteration-method">Value Iteration Method</h3>

<h4 id="why-value-iteration-works">Why Value Iteration Works</h4>

<p>Policy Iteration has two separate steps - policy evaluation (which runs until convergence) and policy improvement. This is computationally expensive because we’re repeatedly evaluating the entire state space multiple times before making a single policy improvement.</p>

<p>Value Iteration addresses this inefficiency by recognizing that:</p>

<ul>
  <li><strong>Similar Calculations</strong> - Both policy evaluation and improvement use the Bellman equation structure</li>
  <li><strong>Partial Convergence</strong> - We can improve the policy before the value function fully converges</li>
  <li><strong>Combined Steps</strong> - We can directly incorporate the max operation into the value update</li>
</ul>

<h4 id="how-value-iteration-works">How Value Iteration Works</h4>

<p>Value Iteration combines policy evaluation and improvement into a single update:</p>

\[V(s) \leftarrow \max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]\]

<p>This update directly finds the value of the best action for each state, effectively:</p>

<ul>
  <li>Assuming a greedy policy at each step</li>
  <li>Skipping the explicit policy representation</li>
  <li>Performing only one sweep through the state space per iteration</li>
</ul>

<h4 id="value-iteration-algorithm">Value Iteration Algorithm</h4>

<ol>
  <li>Initialize \(V(s) = 0\) for all states</li>
  <li>Repeat until convergence:
    <ul>
      <li>For each state \(s\):</li>
    </ul>
    <div style="overflow-x: auto;">
      $$V(s) \leftarrow \max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]$$
 </div>
  </li>
  <li>Extract the final policy:</li>
</ol>
<div style="overflow-x: auto;"> 
   $$\pi(s) = \arg\max_a \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) \cdot V(s') \right]$$
</div>

<h4 id="advantages-over-policy-iteration">Advantages Over Policy Iteration</h4>

<ul>
  <li><strong>Computational Efficiency</strong> - No need to perform full policy evaluation at each step</li>
  <li><strong>Fewer Iterations</strong> - Usually converges in fewer sweeps through the state space</li>
  <li><strong>Simplicity</strong> - Only need to maintain a value function, not an explicit policy</li>
  <li><strong>Direct Optimization</strong> - Works towards optimal values from the start</li>
</ul>

<p>For deterministic environments like our grid world example, the update becomes even simpler:</p>

<div style="overflow-x: auto;">
$$V(s) \leftarrow \max_a \left[R(s,a) + \gamma \cdot V(\text{next_state}(s,a)) \right]$$
</div>

<p>This makes Value Iteration particularly efficient for deterministic problems.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">value_iter_onestep</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">states</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">env</span><span class="p">.</span><span class="n">goal_state</span><span class="p">:</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">continue</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="nf">actions</span><span class="p">():</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="k">def</span> <span class="nf">value_iter</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_render</span><span class="p">:</span>
            <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
        <span class="n">old_V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">V</span> <span class="o">=</span> <span class="nf">value_iter_onestep</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">V</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">t</span><span class="p">:</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="n">t</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">value_iter</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">is_render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">pi</span><span class="p">)</span>
</code></pre></div></div>

<ol>
  <li>one step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld6.png" alt="gridworld" width="100%" />
</div>

<ol>
  <li>two step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld7.png" alt="gridworld" width="100%" />
</div>

<ol>
  <li>three step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld8.png" alt="gridworld" width="100%" />
</div>
<ol>
  <li>four step of the value iteration</li>
</ol>

<div align="center">
  <img src="/images/gridworld9.png" alt="gridworld" width="100%" />
</div>

<p>So, the result of optimal policy is as follows.</p>

<div align="center">
  <img src="/images/gridworld10.png" alt="gridworld" width="100%" />
</div>

<h2 id="monte-carlo-method">Monte Carlo Method</h2>

<p>We know the transition probabilities \(P( s, a)\) and the reward function \(R\), which allows us to apply Dynamic Programming.</p>

<p>Also, using dynamic programming (DP) is too complex to calculate the entire problem.</p>

<p><strong>What is Monte Carlo method?</strong></p>

<p>It assumes a value function for the agent to gain experience in an environment.
The experience mentioned here refers to the data (state, action, reward) obtained through the interaction between the environment and the agent.</p>

<p>The following situation can be considered: Think about all possible outcomes when rolling a dice twice.</p>

<div align="center">
  <div class="mermaid">
graph TD
    Start((Start))
    Start --&gt; D1((Die 1: 1))
    Start --&gt; D2((Die 1: 2))
    Start --&gt; D3((Die 1: 3))
    Start --&gt; D4((Die 1: 4))
    Start --&gt; D5((Die 1: 5))
    Start --&gt; D6((Die 1: 6))

    D1 --&gt; D1_1((Die 2: 1))
    D1 --&gt; D1_2((Die 2: 2))
    D1 --&gt; D1_3((Die 2: 3))
    D1 --&gt; D1_4((Die 2: 4))
    D1 --&gt; D1_5((Die 2: 5))
    D1 --&gt; D1_6((Die 2: 6))

    D2 --&gt; D2_1((Die 2: 1))
    D2 --&gt; D2_2((Die 2: 2))
    D2 --&gt; D2_3((Die 2: 3))
    D2 --&gt; D2_4((Die 2: 4))
    D2 --&gt; D2_5((Die 2: 5))
    D2 --&gt; D2_6((Die 2: 6))

    D3 --&gt; D3_1((Die 2: 1))
    D3 --&gt; D3_2((Die 2: 2))
    D3 --&gt; D3_3((Die 2: 3))
    D3 --&gt; D3_4((Die 2: 4))
    D3 --&gt; D3_5((Die 2: 5))
    D3 --&gt; D3_6((Die 2: 6))

    D4 --&gt; D4_1((Die 2: 1))
    D4 --&gt; D4_2((Die 2: 2))
    D4 --&gt; D4_3((Die 2: 3))
    D4 --&gt; D4_4((Die 2: 4))
    D4 --&gt; D4_5((Die 2: 5))
    D4 --&gt; D4_6((Die 2: 6))

    D5 --&gt; D5_1((Die 2: 1))
    D5 --&gt; D5_2((Die 2: 2))
    D5 --&gt; D5_3((Die 2: 3))
    D5 --&gt; D5_4((Die 2: 4))
    D5 --&gt; D5_5((Die 2: 5))
    D5 --&gt; D5_6((Die 2: 6))

    D6 --&gt; D6_1((Die 2: 1))
    D6 --&gt; D6_2((Die 2: 2))
    D6 --&gt; D6_3((Die 2: 3))
    D6 --&gt; D6_4((Die 2: 4))
    D6 --&gt; D6_5((Die 2: 5))
    D6 --&gt; D6_6((Die 2: 6))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class Start,D1,D2,D3,D4,D5,D6,D1_1,D1_2,D1_3,D1_4,D1_5,D1_6,D2_1,D2_2,D2_3,D2_4,D2_5,D2_6,D3_1,D3_2,D3_3,D3_4,D3_5,D3_6,D4_1,D4_2,D4_3,D4_4,D4_5,D4_6,D5_1,D5_2,D5_3,D5_4,D5_5,D5_6,D6_1,D6_2,D6_3,D6_4,D6_5,D6_6 circle;
  </div>
</div>

<p>If some outcomes represent a probability distribution, we use the sample distribution.
A sample distribution is a method of observing the results of actual sampling.</p>

<p>Let’s use the incremental method learned earlier to sample and calculate the expected value of the sum when two dice are rolled.</p>

<p><code class="language-plaintext highlighter-rouge">incremental method</code> : \(V_n = V_{n - 1} + \frac{1}{n} (s_n - V_{n - 1})\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trial</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">def</span> <span class="nf">Sample</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="n">trial</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">V</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nc">Sample</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">V</span> <span class="o">+=</span> <span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trial </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">: Sample mean = </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># result
# Trial 100: Sample mean = 7.119999999999997
# Trial 200: Sample mean = 6.8199999999999985
# Trial 300: Sample mean = 6.783333333333331
# Trial 400: Sample mean = 6.8575
# Trial 500: Sample mean = 6.844000000000001
# Trial 600: Sample mean = 6.861666666666671
# Trial 700: Sample mean = 6.8885714285714315
# Trial 800: Sample mean = 6.8999999999999995
# Trial 900: Sample mean = 6.948888888888891
# Trial 1000: Sample mean = 6.938000000000002
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Value - Function</code> : \(V_n = \mathbb{E_{\pi}}[G \mid s]\)</p>

<p>This method applies the Monte Carlo approach to estimate values.</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}
$$
</div>

<p>where \(G^{(i)}\) is the return of the \(i\)-th episode.</p>

<p>Let me explain the first trial episode.</p>

<div align="center">
  <div class="mermaid">
graph TD
    S((S))
    S --&gt;|reward_1| A((A))
    A --&gt;|reward_0| B((B))
    B --&gt;|reward_2| END((END))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class S,A,B,C,END circle;
  </div>
</div>

<div style="overflow-x: auto;">
$$
G^{(1)} = 1 + 0 + 2 = 3
$$
</div>

<p>The second trial episode is as follows.</p>

<div align="center">
  <div class="mermaid">
graph TD
    S((S))
    S --&gt;|reward_1| A((A))
    A --&gt;|reward_0| B((B))
    B --&gt;|reward_1| C((C))
    C --&gt;|reward_1| END((END))

    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;
    class S,A,B,C,END circle;

  </div>
</div>

<div style="overflow-x: auto;">
$$
G^{(2)} = 1 + 0 + 1 + 1 = 3
$$
</div>

<p>As a result, the expected value is as follows.</p>

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}}{2} = \frac{3 + 3}{2} = 3
$$
</div>

<p>Let’s calculate the value function for all states using the Monte Carlo method. If there are three states (A, B, C), sample data is obtained by performing actual actions.</p>

<div align="center">
  <div class="mermaid">
flowchart TD

    %% A 파이프라인 1
    A1((A)) --&gt; A2([...]) --&gt; A3([...]) --&gt; Aout((○))

    %% A 파이프라인 2
    A1b((A)) --&gt; A2b([...]) --&gt; A3b([...]) --&gt; Aoutb((○))


    %% B 파이프라인 1
    B1((B)) --&gt; B2([...]) --&gt; B3([...]) --&gt; Bout((○))

    %% B 파이프라인 2
    B1b((B)) --&gt; B2b([...]) --&gt; B3b([...]) --&gt; Boutb((○))

    %% C 파이프라인 1
    C1((C)) --&gt; C2([...]) --&gt; C3([...]) --&gt; Cout((○))

    %% C 파이프라인 2
    C1b((C)) --&gt; C2b([...]) --&gt; C3b([...]) --&gt; Coutb((○))

    %% 스타일 지정
    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px

    class A1,A2,A3,Aout,A1b,A2b,A3b,Aoutb aStyle
    class B1,B2,B3,Bout,B1b,B2b,B3b,Boutb bStyle
    class C1,C2,C3,Cout,C1b,C2b,C3b,Coutb cStyle
</div>
</div>

<p>Let’s consider starting from state A, taking actions according to policy \(\pi\), and reaching the final destination.</p>

<div align="center">
  <div class="mermaid">
graph TD
    A((A))
    A --&gt;|R0| B((B))
    B --&gt;|R1| C((C))
    C --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class A aStyle
    class B bStyle
    class C cStyle
    class END circle;
</div>
</div>

<p>The total rewards accumulated from state A to the end are as follows.</p>

<div style="overflow-x: auto;">
$$
G_A = R_0 + \gamma R_1 + \gamma^2 R_2
$$
</div>

<p>Let’s consider starting from state B.</p>

<div align="center">
  <div class="mermaid">
graph TD
    B((B)) --&gt;|R1| C((C))
    C --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class B bStyle
    class C cStyle
    class END circle;
</div>
</div>

<div style="overflow-x: auto;">
$$
G_B = R_1 + \gamma R_2
$$
</div>

<p>Let’s consider starting from state C.</p>

<div align="center">
  <div class="mermaid">
graph TD
    C((C)) --&gt;|R2| END((END))

    classDef aStyle fill:#b3d9ff,stroke:#3399ff,stroke-width:2px
    classDef bStyle fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    classDef cStyle fill:#99ffcc,stroke:#33cc99,stroke-width:2px
    classDef circle fill:#ffffff,stroke:#000000,stroke-width:2px,shape:circle;

    class C cStyle
    class END circle;
</div>
</div>

<div style="overflow-x: auto;">
$$
G_C = R_2
$$
</div>

<p>So, the following sequence of calculations can eliminate redundant computations.</p>

<div style="overflow-x: auto;">
$$
G_C = R_2
$$
</div>

<div style="overflow-x: auto;">
$$
G_B = R_1 + \gamma G_C
$$
</div>

<div style="overflow-x: auto;">
$$
G_A = R_0 + \gamma G_B
$$
</div>

<h3 id="implement">implement</h3>

<p>Alright, according to the reading, we can implement this for the agent to interact with the environment.</p>

<div align="center">
  <img src="/images/RL.png" alt="RL" width="100%" />
</div>

<p>The start point is (0, 0), the end point is (5, 5), and the black cell represents a wall that the agent cannot pass.</p>

<div align="center">
  <img src="/images/monemap.png" alt="MM" width="80%" />
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">goal</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cnts</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">):</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span>
            <span class="n">self</span><span class="p">.</span><span class="n">cnts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">cnts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>


<span class="n">agent</span> <span class="o">=</span> <span class="nc">RandomAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1e3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Training Progress</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<p>The value function (expected value) of each cell obtained through the Monte Carlo method is as follows.</p>

<div align="center">
  <img src="/images/MM1.png" alt="MM" width="80%" />
</div>

<h3 id="policy-control-using-the-monte-carlo-method">Policy Control Using the Monte Carlo Method</h3>

<p>The optimal policy alternates between evaluation and improvement.</p>
<ul>
  <li><strong>Policy Evaluation</strong>: Calculate the value function for the current policy using the Monte Carlo method.</li>
  <li><strong>Policy Improvement</strong>: Update the policy to be greedy with respect to the current value function.</li>
</ul>

<p>State Value Function Evaluation</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">General Method</code> : \(V_{\pi}(s) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}\)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Incremental Method</code> : \(V_{\pi}(s) = V_{\pi}(s) + \frac{1}{n} (G - V_{\pi}(s))\)</p>
  </li>
</ul>

<p>Q-Function Evaluation</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">General Method</code> : \(Q_{\pi}(s, a) = \frac{G^{(1)} + G^{(2)}+ G^{(3)} + \cdots + G^{(n)}}{n}\)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Incremental Method</code> : \(Q_{\pi}(s, a) = Q_{\pi}(s, a) + \frac{1}{n} (G - Q_{\pi}(s, a))\)</p>
  </li>
</ul>

<h3 id="important-concept">important concept</h3>

<ol>
  <li>
    <p>Use an epsilon-greedy policy to give the agent opportunities to explore.</p>
  </li>
  <li>
    <p>Train the model by applying an exponential moving average with a fixed value <code class="language-plaintext highlighter-rouge">a</code>, giving greater weight to more recent data.</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">common.gridworld_render</span> <span class="k">as</span> <span class="n">render_helper</span>


<span class="k">class</span> <span class="nc">GridWorld</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># 행동 공간(가능한 행동들)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_meaning</span> <span class="o">=</span> <span class="p">{</span>  <span class="c1"># 행동의 의미
</span>            <span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">UP</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">:</span> <span class="sh">"</span><span class="s">DOWN</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="sh">"</span><span class="s">LEFT</span><span class="sh">"</span><span class="p">,</span>
            <span class="mi">3</span><span class="p">:</span> <span class="sh">"</span><span class="s">RIGHT</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">=</span> <span class="n">reward_map</span> <span class="k">if</span> <span class="n">reward_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="p">])</span>

        <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="n">goal</span>
        <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span>
            <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span> <span class="o">==</span> <span class="bp">None</span><span class="p">)))</span>  <span class="c1"># 벽 상태(좌표)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="n">start</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>   <span class="c1"># 에이전트 초기 상태(좌표)
</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">height</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">width</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span>

    <span class="k">def</span> <span class="nf">states</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">width</span><span class="p">):</span>
                <span class="nf">yield </span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">next_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="c1"># 이동 위치 계산
</span>        <span class="n">action_move_map</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">move</span> <span class="o">=</span> <span class="n">action_move_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">move</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># 이동한 위치가 그리드 월드의 테두리 밖이나 벽인가?
</span>        <span class="k">if</span> <span class="n">nx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">nx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">width</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ny</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">height</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">elif</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>

        <span class="k">return</span> <span class="n">next_state</span>  <span class="c1"># 다음 상태 반환
</span>
    <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">start_state</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">next_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">agent_state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>

    <span class="k">def</span> <span class="nf">render_v</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_q</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">print_value</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">renderer</span> <span class="o">=</span> <span class="n">render_helper</span><span class="p">.</span><span class="nc">Renderer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_state</span><span class="p">,</span>
                                          <span class="n">self</span><span class="p">.</span><span class="n">wall_state</span><span class="p">)</span>
        <span class="n">renderer</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">print_value</span><span class="p">)</span>
</code></pre></div></div>

<p>Define the grid environment and the way the agent moves (policy).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">sys</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>

<span class="k">def</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">action_size</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">qs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Q</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)]</span> <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">action_size</span><span class="p">)]</span>
    <span class="n">max_action</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">qs</span><span class="p">))</span>

    <span class="n">base_prob</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="o">/</span> <span class="n">action_size</span>
    <span class="n">action_probs</span> <span class="o">=</span> <span class="p">{</span><span class="n">action</span><span class="p">:</span> <span class="n">base_prob</span> <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">action_size</span><span class="p">)}</span>  <span class="c1">#{0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4}
</span>    <span class="n">action_probs</span><span class="p">[</span><span class="n">max_action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">action_probs</span>


<span class="k">class</span> <span class="nc">McAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># (첫 번째 개선) ε-탐욕 정책의 ε
</span>        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>    <span class="c1"># (두 번째 개선) Q 함수 갱신 시의 고정값 α
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># self.cnts = defaultdict(lambda: 0)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">):</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span> <span class="o">+</span> <span class="n">reward</span>
            <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="c1"># self.cnts[key] += 1
</span>            <span class="c1"># self.Q[key] += (G - self.Q[key]) / self.cnts[key]
</span>            <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>
            <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span>


<span class="n">reward_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">goal</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">reward_map</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">McAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Training Progress</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">Q</span><span class="p">)</span>
</code></pre></div></div>

<p>The agent alternates between exploration and exploitation over 10,000 episodes. Additionally, it reduces the weight of past experiences and assigns higher weight to the rewards obtained through current experiences.</p>

<p>The Q(S, a) for each state is as follows.</p>

<div align="center">
  <img src="/images/Figure_1.png" alt="bandit" width="100%" />
</div>

<p>And, The Optimal Policy for each state is as follows.</p>

<div style="display: flex; justify-content: center; gap: 10px;">
  <img src="/images/Figure_2.png" alt="bandit1" style="width: 48%;" />
  <img src="/images/Figure_3.png" alt="bandit2" style="width: 48%;" />
</div>

<h3 id="montecarlo-method-quiz">Montecarlo method QUIZ!</h3>

<details>
<summary>1. 상태 가치 함수 \(V(s)\) 추정</summary>

몬테카를로 방법에서 상태 가치 함수 \(V(s)\)를 추정할 때, 전체 에피소드의 평균을 이용하는 일반적인 방법과 점진적(incremental) 방법의 차이점을 설명하라. 이 두 방법의 장단점을 비교하고, 실제 구현 시 어떤 경우에 점진적 방법이 더 유리한지 서술하라.
 

<details>
<summary> 1.정답 </summary>
일반적인 방식은 각 상태에서 얻어진 모든 return 값을 저장한 뒤, 이들의 평균을 통해 상태 가치 함수 \(V(s)\)를 계산하므로 메모리 사용량이 커지고, 에피소드 수가 많아질수록 계산량 또한 누적적으로 증가하게 된다. 반면 증분 방식(incremental update)은 직전까지의 평균값 \(V_{n-1}(s)\)을 기반으로 새로 들어온 return \(G_n\)을 단일 수식으로 갱신하기 때문에, 과거 데이터를 모두 저장할 필요 없이 온라인 환경에서도 효율적으로 작동하며, 메모리 사용이 적고 연산도 가볍다는 장점이 있다. 따라서 데이터가 실시간으로 들어오거나 에피소드 수가 사전에 정해지지 않은 경우에는 증분 방식이 특히 유리하다.
</details>
</details>

<details>
<summary>2. 에피소드와 리턴 \(G_0\)</summary>

\(S_0 \rightarrow a_0, r_0 \rightarrow S_1 \rightarrow a_1, r_1 \rightarrow S_2 \rightarrow a_2, r_2 \rightarrow end\)

할인율 \(\gamma\)가 주어졌을 때, 이 에피소드를 바탕으로 \(S_0\) 상태의 리턴 \(G_0\)을 정의하라. 그리고 이를 기반으로 몬테카를로 방법으로 \(V(S_0)\)을 어떻게 추정하는지 수식과 함께 서술하라.

<br />

<details>
<summary> 2.정답 </summary>
에이전트가 \(s_0\) 상태에서 시작해 \(r_0, r_1, r_2\)의 보상을 순차적으로 받는 경우, 
\(G_0\)는 할인율 \(\gamma\)를 적용한 누적 보상으로 다음과 같이 정의된다:

\[
G_0 = r_0 + \gamma r_1 + \gamma^2 r_2
\]

이는 첫 번째 상태에서의 총 return을 의미하며, 이를 여러 에피소드에서 반복 측정하여 
얻은 평균값을 통해 상태 가치 함수 \(V(s_0)\)를 다음과 같이 추정할 수 있다:

\[
V(s_0) = \mathbb{E}_\pi [G_0 \mid s_0]
\]

Monte Carlo 방식은 이와 같이 각 에피소드의 return을 이용하여 직접적인 추정을 수행하며, 
정책 \(\pi\)에 따라 얻어진 실제 경로의 경험을 바탕으로 기대값을 근사한다.
</details>
</details>

<details>
<summary>3. 중요도 샘플링</summary>

오프-정책 몬테카를로 예측에서 사용되는 중요도 샘플링(importance sampling)의 개념을 설명하라. 그리고 Ordinary Importance Sampling과 Weighted Importance Sampling의 차이점을 설명하고, 각 방식의 수렴 특성과 분산 차이를 서술하라.


<details>
<summary>3.정답</summary>
중요도 샘플링은 off-policy Monte Carlo 학습에서 사용된다. 이는 데이터를 만든 행동 정책과 우리가 학습하려는 목표 정책이 다를 때, 그 차이를 보정하기 위해 사용된다. 예를 들어, 행동 정책이 랜덤으로 영화를 추천하고, 목표 정책이 사용자가 좋아할 가능성이 높은 영화를 추천한다고 하자. 이 경우, 각 에피소드에서 계산된 결과에 행동 확률과 목표 확률의 비율을 곱해 기대값을 다시 계산한다. 일반적 중요도 샘플링은 모든 비율을 곱해 평균을 내며, 수렴은 보장되지만 변동성이 크다. 반면, 가중 중요도 샘플링은 전체 확률의 합으로 정규화해 변동성은 낮지만 약간의 편향이 생길 수 있다.

</details>
</details>

<details>
<summary>4. Epsilon-greedy 정책</summary>

Epsilon-greedy 정책을 사용한 몬테카를로 제어법에서 정책 개선(policy improvement)이 어떤 방식으로 이루어지는지 설명하라. 특히 \(\epsilon\)이 작거나 클 때 정책의 수렴 속도나 안정성에 어떤 영향을 미치는지도 논하라.

<details>
<summary>정답</summary>
Epsilon-greedy 정책에서는 대부분의 경우(1 - ε 확률) 현재까지 가장 가치가 높다고 판단되는 행동을 선택하고, 나머지 ε 확률로는 임의의 행동을 고른다. 이렇게 하면 탐험과 이용의 균형을 맞출 수 있다. ε 값이 작으면 에이전트가 거의 항상 최선의 행동만 선택하므로 빠르게 수렴할 수 있지만, 최적이 아닐 수도 있는 행동에 대한 탐험이 부족해질 수 있다. 반대로 ε 값이 크면 다양한 행동을 많이 시도해 안정적으로 최적 정책을 찾을 수 있지만, 수렴 속도가 느려질 수 있다.
</details>
</details>

<details>
<summary>5. Exploring Starts 가정</summary>

Monte Carlo 예측(Monte Carlo Prediction)에서 Exploring Starts 가정이 필요한 이유는 무엇인가? 이 가정을 현실적으로 적용하기 어려운 이유와, 이를 대신할 수 있는 방법(예: \(\epsilon\)-greedy)을 설명하라.

<details>
<summary>정답</summary>
Monte Carlo 예측에서 Exploring Starts 가정이 필요한 이유는 모든 상태-행동 쌍이 적어도 한 번은 방문되어야 올바른 가치 추정이 가능하기 때문이다. 하지만 실제 환경에서는 임의의 상태와 행동에서 에피소드를 시작하는 것이 어렵다. 이를 대신할 수 있는 방법으로는 ε-greedy 정책처럼 일부러 무작위 행동을 섞어 모든 상태-행동 쌍을 자연스럽게 방문하도록 만드는 방법이 있다.
</details>
</details>

<details>
<summary>6. Q-값 업데이트</summary>

$$
Q[(s, a)] += \alpha (G - Q[(s, a)])
$$

이 업데이트가 의미하는 바를 서술하라. 그리고 \(\alpha\)가 고정값일 때와 \(\frac{1}{N(s, a)}\)일 때 각각의 장단점을 설명하라.

<details>
<summary>정답</summary>
이 업데이트는 최근의 return G 값을 활용해 기존의 Q값을 갱신하는 지수이동평균(Exponential Moving Average) 방식이다. 과거의 Q 추정치를 완전히 버리지 않고 일부 가중치만 남긴 채 새로운 정보를 더 많이 반영한다. α가 고정된 값이면, 최근 경험에 더 큰 비중을 두는 효과가 생겨 환경이 변화하거나 보상이 비정상(non-stationary)한 경우에도 빠르게 적응할 수 있다. 반면 α = 1/N(s, a)로 설정하면, 경험이 쌓일수록 Q값의 변화폭이 점점 줄어들어 값이 안정적으로 수렴한다. 예를 들어, α를 0.1로 고정하면 최근 10번의 경험에 더 민감하게 반응하고, α를 1/N(s, a)로 하면 경험이 많아질수록 Q값이 천천히 변한다.
</details>
</details>

<details>
<summary>7. Monte Carlo Control 단계</summary>

Monte Carlo Control을 사용하는 에이전트가 환경에서 최적 정책을 학습해가는 과정을 단계별로 서술하라. 각 단계에서 수행되는 연산의 목적과 의의를 명확히 설명하라.

<details>
<summary>정답</summary>
Monte Carlo Control 방법은 매 에피소드에서 에이전트가 현재 ε-greedy 정책에 따라 행동을 선택하고, 에피소드가 끝날 때까지 상태-행동-보상 정보를 순서대로 저장한다. 에피소드가 끝난 뒤에는 마지막 상태부터 거꾸로 누적 보상(return G)을 계산한다. 이렇게 얻은 return 값을 이용해 Q(s, a) 값을 점진적으로 갱신한다. 이후 각 상태에서의 정책은 현재 Q 함수에 대해 가장 높은 값을 주는 행동을 선택하도록 개선한다. 이 과정을 여러 번 반복하면 정책 평가와 정책 개선이 번갈아 일어나면서 최적 정책에 점점 가까워진다.
</details>
</details>

<details>
<summary>8. Monte Carlo vs. DP/TD</summary>

Monte Carlo 방법이 Dynamic Programming이나 Temporal-Difference 학습과 비교하여 갖는 특징(장점과 단점)을 최소 3가지 이상 비교 설명하라.


<details>
<summary>정답</summary>
Dynamic Programming(DP)은 환경의 완전한 모델(P, R)을 알고 있을 때 Bellman 방정식을 이용해 상태 가치나 정책을 반복적으로 계산할 수 있다. 이 방식은 빠르고 정확하지만, 실제 환경에서는 모델을 알기 어렵다는 한계가 있다. Monte Carlo(MC) 방법은 환경 모델이 없어도 되고, 에피소드가 끝난 후 실제 경험을 바탕으로 학습할 수 있다. 하지만 에피소드가 끝날 때까지 기다려야 하고, 결과의 변동성이 커서 수렴이 느릴 수 있다. 반면 Temporal-Difference(TD) 학습은 경험을 바탕으로 한 단계씩 바로 업데이트할 수 있어 빠르고 변동성이 적으며, 온라인 학습에 적합하다. 예를 들어, 게임을 플레이할 때 DP는 게임의 모든 규칙과 결과를 알아야 하고, MC는 한 판이 끝난 뒤에만 학습하지만, TD는 매 턴마다 바로 학습할 수 있다.
</details>
</details>

<details>
<summary>9. 그리드월드 환경</summary>

다음 그리드월드 환경에서 에이전트가 목적지까지 도달하는 데 걸리는 평균 리턴을 Monte Carlo 방식으로 추정하고자 한다.
1. 샘플 경로들을 어떻게 생성할 것이며,
2. 각 상태의 가치 함수를 어떻게 업데이트할지 수식 및 알고리즘 흐름 중심으로 서술하라.

<details>
<summary>정답</summary>
먼저 에이전트를 시작 상태에 두고 ε-greedy 정책에 따라 행동을 선택하면서 각 스텝에서 받은 보상과 전이된 상태를 순서대로 기록한다. 도착 지점에 이르거나 최대 스텝 수에 도달하면 에피소드를 종료하고 이 기록을 하나의 샘플 경로로 취급한다.

샘플 경로에 대해 마지막 시점 \(T\)부터 역순으로 리턴 \(G_t\)를 계산한다. 먼저 \(G_T = R_{T+1}\)로 정의하고 이후 시점 \(t\)에서는 \(G_t = R_{t+1} + \gamma G_{t+1}\)로 계산한다.

가치 함수 \(V(s)\)는 First-Visit Monte Carlo 방식으로 업데이트한다. 경로에서 상태 \(s\)를 처음 방문한 시점 \(t\)에 대해 방문 횟수 \(N(s)\)를 1 증가시키고
\[
V(s) \leftarrow V(s) + \frac{G_t - V(s)}{N(s)}
\]
으로 갱신한다.

전체 알고리즘 흐름은 다음과 같다.
1. 모든 상태 \(s\)에 대해 \(V(s)\)를 임의의 값으로 초기화하고 \(N(s)\)를 0으로 설정한다.
2. 에피소드를 생성해 상태, 행동, 보상 시퀀스를 기록한다.
3. 역순으로 \(G_t = R_{t+1} + \gamma G_{t+1}\)를 계산한다.
4. 각 상태 \(s\)의 첫 방문 시점마다 \(N(s)\)를 증가시키고 \(V(s)\)를 갱신한다.
5. 충분한 에피소드를 반복하여 \(V(s)\)가 수렴할 때까지 과정을 반복한다.

이 과정을 통해 얻은 \(V(s)\)는 각 상태에서 목적지까지의 평균 리턴을 근사한다.
</details>
</details>

<details>
<summary>10. Q-함수 수렴 문제</summary>

Monte Carlo Control을 사용하여 최적 Q-함수 \(Q^*(s, a)\)을 근사할 때, 다음 상황에서 Q-함수가 수렴하지 않을 수 있는 이유를 서술하라:
- 정책이 항상 탐욕적(greedy)이고
- 동일 상태에서의 행동 선택이 항상 동일하며
- 에피소드 수가 제한적일 때

그리고 이를 방지하기 위한 두 가지 방법을 제시하라.

<details>
<summary>정답</summary>
추가적인 탐험 기법에는 여러 가지가 있다. 첫 번째로, 중요도가 높은 에피소드를 우선적으로 다시 사용하는 방법이 있다. 이 방법은 모든 경험을 똑같이 사용하는 것이 아니라, 에이전트의 학습에 더 큰 영향을 줄 수 있는 경험을 더 자주 선택해 학습에 반영한다. 예를 들어, 보상이 크거나 예측과 실제 결과의 차이가 큰 에피소드를 우선적으로 샘플링하면, 중요한 정보를 더 빠르게 반영할 수 있다.

두 번째로, UCB(Upper Confidence Bound) 방식이 있다. 이 방법은 Q(s,a) 값에 추가적인 보정항을 더해준다. 구체적으로, Q(s,a)에 c 곱하기 루트 로그 N(s)를 N(s,a)로 나눈 값을 더한다. 여기서 N(s)는 상태 s가 선택된 총 횟수이고, N(s,a)는 상태 s에서 행동 a가 선택된 횟수이다. 이 보정항은 자주 선택되지 않은 행동일수록 값이 커지도록 설계되어 있다. 따라서 Q값이 낮더라도 아직 충분히 시도되지 않은 행동은 더 높은 우선순위를 갖게 되어, 에이전트가 다양한 행동을 시도할 수 있도록 유도한다. 이 방식은 단순히 무작위로 행동을 고르는 것보다, 아직 정보가 부족한 행동을 체계적으로 탐험할 수 있게 해준다.
</details>
</details>

<h2 id="td-method">TD method</h2>

<p>TD법을 시작하기 전에 앞에 내용을 문제를 풀며 복습을 해보자.</p>

<details>
<summary>1. MDP의 구성 요소를 정의하고, 각 요소가 에이전트 학습에서 어떤 역할을 하는지 논의하시오.</summary>

MDP를 이루는 상태 집합 \(\mathcal{S}\), 행동 집합 \(\mathcal{A}\), 상태 전이 확률 \(P(s'\mid s,a)\), 보상 함수 \(r(s,a,s')\), 할인율 \(\gamma\)를 수식과 함께 정의하고, 각 요소가 에이전트 학습에서 어떤 역할을 하는지 논의한다.
<details>
<summary>정답</summary>
마르코프 결정 과정(MDP)은 순차적 의사결정 문제를 수학적으로 모델링하는 프레임워크로, 다섯 가지 핵심 요소로 구성된다.<br />

상태 집합 \(\mathcal{S}\)는 에이전트가 관찰할 수 있는 모든 환경 상태들의 집합이다. 이는 에이전트가 현재 환경에서 어떤 상황에 처해 있는지를 표현하며, 학습 과정에서 상태 인식과 표현의 기초가 된다. 상태 정보의 품질은 에이전트가 최적 행동을 결정하는 데 직접적인 영향을 미친다.<br />

행동 집합 \(\mathcal{A}\)는 각 상태에서 에이전트가 선택할 수 있는 모든 행동들의 집합이다. 이를 통해 에이전트는 환경과 상호작용하며, 다양한 행동을 탐색함으로써 최적의 행동 방침을 학습한다. 행동 선택은 정책 개선 과정의 핵심이며, 행동 공간의 구조는 학습 복잡도에 직접적인 영향을 준다.<br />

상태 전이 확률 \(P(s'\mid s,a) = \Pr\{S_{t+1}=s' \mid S_t=s, A_t=a\}\)은 현재 상태 \(s\)에서 행동 \(a\)를 취했을 때 다음 상태 \(s'\)로 전이될 확률을 나타낸다. 이는 환경의 동적 특성을 확률적으로 모델링하며, 에이전트가 행동의 결과를 예측하고 계획을 세울 수 있게 한다. 상태 전이 확률은 모델 기반 학습에서 환경 시뮬레이션의 기반이 된다.<br />

보상 함수 \(r(s,a,s') = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a, S_{t+1}=s']\)는 상태 \(s\)에서 행동 \(a\)를 취해 상태 \(s'\)로 전이될 때 받을 것으로 기대되는 즉각적 보상을 정의한다. 보상 함수는 에이전트의 목표를 명시적으로 정의하고, 학습 과정에서 행동의 가치를 평가하는 핵심 신호가 된다. 적절한 보상 설계는 강화학습의 성공에 결정적인 역할을 한다.<br />

할인율 \(\gamma\in[0,1)\)은 미래 보상의 현재 가치를 결정하는 파라미터이다. 이는 에이전트가 단기적 보상과 장기적 보상 사이의 균형을 맞추는 데 중요한 역할을 한다. \(\gamma\)가 0에 가까울수록 즉각적인 보상을 중시하고, 1에 가까울수록 미래의 보상을 현재와 거의 동등하게 가치 있게 평가한다.<br />

이 다섯 요소가 유기적으로 결합되어 에이전트가 장기적 보상을 최대화하는 최적 정책을 학습할 수 있는 수학적 기반을 제공한다. MDP 프레임워크는 강화학습의 이론적 토대를 형성하며, 실제 문제를 이 프레임워크로 정확히 모델링하는 것이 강화학습 응용의 첫 번째 단계이다.
</details>
</details>

<details>
<summary>2. 마르코프 성질이 무엇인지 정의하고, "미래 상태가 오직 현재 상태에만 의존한다"는 점을 예시를 들어 설명하시오.</summary>

마르코프 성질이 무엇인지 정의하고, "미래 상태가 오직 현재 상태에만 의존한다"는 점을 간단한 예시(예: 2×2 그리드월드)로 설명한다.
<details>
<summary>정답</summary>
마르코프 성질은 시스템의 미래 상태가 과거의 모든 상태와 행동의 이력이 아니라 오직 현재 상태에만 의존한다는 속성이다. 수학적으로는 상태 전이의 조건부 확률 분포가 현재 상태와 행동만으로 결정된다는 것을 의미한다. 즉, \(\Pr(S_{t+1}\mid S_{0:t},A_{0:t})=\Pr(S_{t+1}\mid S_t,A_t)\)로 표현된다.<br />

이러한 마르코프 성질은 복잡한 의사결정 과정을 단순화하는 매우 강력한 가정이다. 이는 시스템이 '기억이 없는' 속성을 가진다는 것을 의미하며, 현재 상태가 미래 예측에 필요한 모든 정보를 포함하고 있다고 가정한다.<br />

구체적인 예로, 2×2 그리드월드 환경을 생각해 보자. 이 환경에는 A, B, C, D 네 개의 격자 칸이 있고, 에이전트는 상, 하, 좌, 우로 이동할 수 있다. 에이전트가 현재 C 칸에 있다고 가정해 보자. 마르코프 성질에 따르면, 에이전트가 다음에 어느 칸으로 이동할지는 오직 현재 위치인 C 칸과 선택한 행동(예: '위로 이동')에만 의존한다.<br />

이는 에이전트가 어떤 경로를 통해 C 칸에 도달했는지는 전혀 중요하지 않다는 것을 의미한다. 에이전트가 A → B → C 경로로 왔든, D → C 경로로 왔든, 또는 C에서 여러 번 제자리에 머물렀든 상관없이, 현재 C에 있다는 사실과 선택한 행동만이 다음 상태를 결정한다.<br />

이러한 마르코프 성질은 강화학습 알고리즘에서 계산 효율성을 크게 높인다. 시스템의 모든 이전 이력을 저장하고 처리하는 대신, 현재 상태만 고려하면 되기 때문에 메모리 요구사항과 계산 복잡도가 크게 감소한다. 또한 가치 함수와 정책을 현재 상태에만 의존하는 함수로 정의할 수 있게 해주어, 강화학습의 이론적 기반이 되는 벨만 방정식을 유도할 수 있게 한다.
</details>
</details>

<details>
<summary>3. 결정론적 전이와 확률론적 전이의 차이를 수식으로 비교하고, 각 방식이 모델링에 미치는 장단점을 논의하시오.</summary>

결정론적 전이 함수 \(s'=f(s,a)\)와 확률론적 전이 확률 \(P(s'\mid s,a)\)의 차이를 수식으로 비교하고, 각 방식이 모델링에 미치는 장단점을 논의한다.
<details>
<summary>정답</summary>
결정론적 전이와 확률론적 전이는 환경 모델링의 두 가지 근본적으로 다른 접근 방식이다. 결정론적 전이는 함수 \(f:\mathcal{S}\times\mathcal{A}\to\mathcal{S}\)로 표현되며, 이는 현재 상태 \(s\)와 행동 \(a\)가 주어졌을 때 다음 상태가 정확히 \(s'=f(s,a)\)로 결정된다는 것을 의미한다. 반면, 확률론적 전이는 \(P(s'\mid s,a)\)로 표현되는 확률 분포로, 각 가능한 다음 상태 \(s'\)로 전이될 확률을 제공하며, 모든 상태에 대해 합산하면 1이 된다 (\(\sum_{s'}P(s'\mid s,a)=1\)).<br />

결정론적 전이 모델은 계산 효율성이 주요 장점이다. 다음 상태가 명확하게 결정되므로, 계획 알고리즘이나 동적 프로그래밍에서 각 상태-행동 쌍에 대해 하나의 다음 상태만 고려하면 된다. 이는 계산량을 크게 줄여주고, 구현을 단순화한다. 그러나 결정론적 모델은 환경의 불확실성이나 노이즈를 표현할 수 없다는 중대한 한계가 있다. 현실 세계의 많은 응용 분야에서는 동일한 상태와 행동에서도 다양한 결과가 발생할 수 있으므로, 이러한 모델은 현실을 지나치게 단순화할 수 있다.<br />

반면, 확률론적 전이 모델은 환경의 내재적 불확실성을 명시적으로 모델링할 수 있다. 이는 로봇 제어, 자율 주행, 금융 의사결정과 같이 불확실성이 중요한 역할을 하는 복잡한 실제 문제에 더 적합하다. 확률론적 모델은 또한 탐색과 활용 사이의 균형을 자연스럽게 가능하게 하여, 에이전트가 불확실한 행동의 결과를 탐색하도록 장려한다. 그러나 계산 비용이 크게 증가한다는 단점이 있다. 동적 프로그래밍에서 확률론적 모델을 사용할 때는 각 상태-행동 쌍에 대해 모든 가능한 다음 상태를 고려해야 하며, 이로 인해 계산 복잡도가 \(O(|S|^2|A|)\)까지 증가할 수 있다.<br />

실제 응용에서는 환경의 복잡성과 요구되는 정확도에 따라 적절한 모델을 선택해야 한다. 단순한 그리드 월드나 결정론적 게임과 같은 환경에서는 결정론적 모델이 효율적이고 충분할 수 있다. 반면, 날씨 영향을 받는 시스템, 금융 시장, 또는 다중 에이전트 환경과 같이 본질적으로 불확실한 환경에서는 확률론적 모델이 필수적이다.
</details>
</details>

<details>
<summary>4. 보상 함수의 정의와 즉시 보상과 장기 보상의 차이를 논의하시오.</summary>

보상 함수 \(r(s,a,s')\)의 정의와, "즉시 보상(immediate reward)"과 "장기 보상(long-term return)"의 차이를 설명한다.
<details>
<summary>정답</summary>
보상 함수 \(r(s,a,s')\)는 강화학습에서 에이전트의 목표를 수학적으로 정의하는 핵심 요소이다. 이 함수는 상태 \(s\)에서 행동 \(a\)를 취하여 새로운 상태 \(s'\)로 전이될 때 에이전트가 받는 즉각적인 피드백을 나타낸다. 수학적으로는 \(r(s,a,s') = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a, S_{t+1}=s']\)로 정의되며, 이는 특정 상태 전이에 대한 보상의 기대값을 의미한다.<br />

즉시 보상(immediate reward)은 에이전트가 한 단계의 행동 후 즉각적으로 받는 보상을 의미한다. 이는 \(R_{t+1}\)로 표기되며, 시간 \(t\)에서의 상태와 행동 이후 직접적으로 관찰되는 가치이다. 즉시 보상은 행동의 직접적인 결과를 평가하는 방법을 제공하지만, 장기적인 결과를 고려하지 않는다. 예를 들어, 체스 게임에서 말 하나를 잡는 것은 즉시 보상을 가져오지만, 그 결과로 자신의 중요한 말이 위험에 처할 수 있다.<br />

반면, 장기 보상(long-term return)은 시간 \(t\)부터 에피소드 끝까지 받게 될 모든 미래 보상의 할인된 합으로 정의된다. 수학적으로는 \(G_t=\sum_{k=0}^\infty\gamma^kR_{t+k+1}\)로 표현된다. 여기서 \(\gamma\in[0,1)\)는 할인율로, 미래 보상이 현재 가치에 기여하는 정도를 조절한다. 장기 보상은 에이전트가 일련의 행동 결과를 종합적으로 평가할 수 있게 해주며, 이는 강화학습의 중심 목표인 장기적 가치 최대화에 필수적이다.<br />

즉시 보상과 장기 보상의 차이는 시간적 관점에서 중요하다. 즉시 보상은 짧은 시간 범위에서의 성과만을 평가하므로, 이에만 집중하면 근시안적인 행동을 초래할 수 있다. 장기 보상은 더 넓은 시간 범위에서 행동의 결과를 고려하므로, 때로는 즉각적인 보상을 희생하더라도 장기적으로 더 가치 있는 결정을 내릴 수 있게 한다.<br />

강화학습에서는 이상적으로 에이전트가 장기 보상을 최대화하는 정책을 학습하고자 한다. 그러나 장기 보상은 직접 관찰할 수 없으므로, 가치 함수나 Q-함수와 같은 추정치를 사용하여 학습한다. 즉시 보상은 이러한 가치 함수 업데이트의 기초가 되며, 벨만 방정식을 통해 단기적 피드백을 장기적 가치 추정으로 연결한다.
</details>
</details>

<details>
<summary>5. 결정적 정책과 확률적 정책의 정의를 비교하고, 각각의 장단점을 논의하시오.</summary>

결정적(deterministic) 정책 \(\pi(s)=a\)와 확률적(stochastic) 정책 \(\pi(a\mid s)\)을 정의하고, 각각의 장단점을 사례와 함께 설명한다.
<details>
<summary>정답</summary>
강화학습에서 정책은 에이전트가 각 상태에서 어떤 행동을 선택할지 결정하는 규칙을 의미한다. 결정적 정책과 확률적 정책은 이 규칙을 정의하는 두 가지 서로 다른 방식이다.<br />

결정적 정책(deterministic policy)은 각 상태에서 정확히 하나의 행동만을 매핑하는 함수 \(\pi: \mathcal{S} \to \mathcal{A}\)로 정의된다. 즉, \(\pi(s)=a\)는 상태 \(s\)에서 에이전트가 항상 행동 \(a\)를 선택한다는 것을 의미한다. 결정적 정책의 주요 장점은 구현이 단순하고 계산 효율성이 높다는 점이다. 각 상태에서 하나의 행동만 고려하면 되므로, 계획 알고리즘이나 정책 평가 시 계산 복잡도가 낮다. 또한 학습된 최적 정책을 실행할 때 일관된 행동을 보장한다. 그러나 결정적 정책의 가장 큰 단점은 탐험 능력이 제한된다는 것이다. 새로운 행동을 시도하지 않으면 더 나은 전략을 발견하지 못할 수 있으며, 이로 인해 지역 최적해(local optima)에 갇힐 위험이 있다.<br />

반면, 확률적 정책(stochastic policy)은 각 상태에서 가능한 모든 행동에 대한 확률 분포로 정의된다. 수학적으로는 \(\pi(a\mid s)=\Pr\{A_t=a\mid S_t=s\}\)로 표현되며, 이는 상태 \(s\)에서 행동 \(a\)를 선택할 확률을 나타낸다. 확률적 정책의 가장 큰 장점은 자연스러운 탐험 메커니즘을 제공한다는 것이다. 다양한 행동에 일정 확률을 할당함으로써, 에이전트는 최선으로 생각되는 행동 외에도 다른 행동을 시도할 수 있다. 이는 복잡한 환경에서 더 나은 전략을 발견할 가능성을 높이고, 지역 최적해를 피하는 데 도움이 된다. 또한 확률적 정책은 멀티 에이전트 환경이나 부분 관찰 가능한 환경에서 더 강건한 성능을 보일 수 있다. 그러나 확률적 정책은 학습 속도가 상대적으로 느릴 수 있으며, 최종 정책이 결정적 정책만큼 최적화되지 않을 수 있다.<br />

실제 사례로, 미로 탐색 문제를 생각해 보자. 결정적 정책을 사용하면 에이전트는 각 위치에서 항상 같은 방향으로 이동한다. 이는 한 번 경로를 찾았다면 효율적으로 목표에 도달할 수 있지만, 더 나은 경로가 있을 수 있음에도 계속 같은 경로만 탐색한다. 확률적 정책을 사용하면 에이전트는 주로 가장 유망한 방향으로 이동하지만, 가끔 다른 방향도 시도한다. 이로 인해 처음에는 비효율적일 수 있지만, 결국 최적의 경로를 발견할 가능성이 높아진다.<br />

강화학습 실무에서는 종종 학습 초기에는 탐험을 촉진하기 위해 확률적 정책을 사용하고, 학습이 진행됨에 따라 점차 결정적 정책으로 전환하는 전략을 채택한다. 이를 통해 탐험과 활용의 균형을 효과적으로 조절할 수 있다.
</details>
</details>

<details>
<summary>6. 할인된 리턴이 수렴하기 위한 수학적 조건을 유도하고, \(\gamma\) 값에 따른 학습 결과의 차이를 논의하시오.</summary>

리턴 \(G_t=\sum_{k=0}^\infty\gamma^kR_{t+k+1}\)이 수렴하기 위한 수학적 조건을 유도하고, \(\gamma\) 값이 0에 가까울 때와 1에 가까울 때 학습 결과에 어떤 차이가 나는지 서술한다.
<details>
<summary>정답</summary>
강화학습에서 할인된 리턴(discounted return)은 시간 \(t\)부터 시작하여 에이전트가 받게 될 모든 미래 보상의 가중 합으로 정의된다. 수학적으로 이는 \(G_t=\sum_{k=0}^\infty\gamma^kR_{t+k+1}\)로 표현된다. 이 무한 합이 유한한 값으로 수렴하기 위한 조건을 살펴보자.<br />

먼저, 보상 \(R_t\)가 어떤 상수 \(R_{max}\)로 제한된다고 가정한다. 즉, 모든 시간 \(t\)에 대해 \(|R_t| \leq R_{max}\)이다. 이제 할인된 리턴의 절대값에 대한 상한을 계산해 보면:<br />

\(|G_t| = |\sum_{k=0}^\infty\gamma^kR_{t+k+1}| \leq \sum_{k=0}^\infty\gamma^k|R_{t+k+1}| \leq R_{max}\sum_{k=0}^\infty\gamma^k\)<br />

마지막 합 \(\sum_{k=0}^\infty\gamma^k\)는 기하급수이다. 이 급수가 수렴하기 위한 필요충분조건은 \(|\gamma| &lt; 1\)이다. 이 조건이 만족되면, 급수는 \(\frac{1}{1-\gamma}\)로 수렴한다. 따라서 \(|G_t| \leq \frac{R_{max}}{1-\gamma}\)가 되어 할인된 리턴이 유한한 값으로 수렴한다.<br />

실제 강화학습에서는 \(\gamma \in [0,1)\)의 범위를 사용하며, \(\gamma\) 값의 선택은 학습 결과에 중요한 영향을 미친다.<br />

\(\gamma\) 값이 0에 가까울 때(예: \(\gamma = 0.1\)), 에이전트는 즉각적인 보상만을 고려한다. 이는 리턴이 \(G_t \approx R_{t+1}\)로 단순화되기 때문이다. 이로 인해 에이전트는 단기적 이익을 추구하고, 장기적 결과를 무시하는 근시안적인 행동을 하게 된다. 이러한 설정은 보상 신호가 명확하고 즉각적인 피드백이 중요한 단순한 환경에서 유용할 수 있다. 또한 학습 속도가 빠르다는 장점이 있지만, 복잡한 문제에서는 최적이 아닌 정책을 학습할 가능성이 높다.<br />

반면, \(\gamma\) 값이 1에 가까울 때(예: \(\gamma = 0.99\)), 에이전트는 먼 미래의 보상도 현재와 거의 동등하게 중요하게 고려한다. 이는 에이전트가 장기적인 결과를 위해 단기적 보상을 희생할 수 있게 해준다. 복잡한 환경이나 지연된 보상이 있는 문제에서는 높은 \(\gamma\) 값이 필요하다. 예를 들어, 체스나 바둑과 같은 게임에서는 게임이 끝날 때까지 최종 결과를 알 수 없으므로, 일련의 행동의 장기적 가치를 평가하기 위해 높은 \(\gamma\) 값이 중요하다. 그러나 높은 \(\gamma\) 값은 보상의 분산을 증가시키고 학습을 불안정하게 만들 수 있으며, 수렴 속도가 느려질 수 있다.<br />

실무에서는 문제의 특성에 따라 적절한 \(\gamma\) 값을 선택하는 것이 중요하다. 일반적으로 \(\gamma = 0.9\)에서 \(\gamma = 0.99\) 사이의 값이 많이 사용되며, 이는 단기적 행동과 장기적 계획 사이의 합리적인 균형을 제공한다. 또한, 학습 초기에는 낮은 \(\gamma\) 값으로 시작하여 학습이 진행됨에 따라 점진적으로 증가시키는 방법도 효과적일 수 있다.
</details>
</details>

<details>
<summary>7. 상태 가치 함수의 정의와 기대값 관점에서의 해석을 논의하시오.</summary>

정책 \(\pi\)하에서 상태 \(s\)의 가치 함수 \(V_\pi(s)\)를 수식으로 정의하고, 기대값(\(\mathbb{E}_\pi\)) 관점에서 해석한다.
<details>
<summary>정답</summary>
상태 가치 함수 \(V_\pi(s)\)는 강화학습의 핵심 개념으로, 에이전트가 정책 \(\pi\)를 따를 때 상태 \(s\)에서 기대할 수 있는 미래 보상의 총합을 나타낸다. 수학적으로 이는 다음과 같이 정의된다:<br />

\[
V_\pi(s)=\mathbb{E}_\pi\Bigl[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\mid S_t=s\Bigr]
\]<br />

이 수식은 상태 \(s\)에서 시작하여 정책 \(\pi\)를 따를 때 에이전트가 받을 것으로 기대되는 할인된 누적 보상(discounted cumulative reward)을 의미한다. 여기서 \(\mathbb{E}_\pi\)는 정책 \(\pi\)를 따를 때의 기대값 연산자이고, \(\gamma\)는 미래 보상의 중요도를 조절하는 할인율이다.<br />

기대값 관점에서 상태 가치 함수는 특정 상태에서 시작하여 정책을 따를 때 발생할 수 있는 모든 가능한 궤적(trajectory)에 대한 가중 평균으로 해석할 수 있다. 각 궤적은 상태와 행동의 시퀀스로, 서로 다른 환경 역학(dynamics)과 정책의 확률적 특성으로 인해 다양한 궤적이 발생할 수 있다.<br />

구체적으로, 정책 \(\pi\)가 각 상태에서 행동을 선택할 확률 \(\pi(a|s)\)를 결정하고, 환경의 역학이 상태 전이 확률 \(P(s'|s,a)\)를 결정한다. 이러한 확률들의 조합으로 다양한 궤적이 생성되며, 각 궤적은 자신만의 보상 시퀀스를 가진다. 가치 함수 \(V_\pi(s)\)는 이러한 모든 가능한 궤적의 할인된 보상 합을 각 궤적이 발생할 확률로 가중 평균한 것이다.<br />

이러한 기대값 관점은 강화학습에서 몇 가지 중요한 의미를 갖는다. 첫째, 상태 가치 함수는 정책의 품질을 평가하는 객관적인 척도를 제공한다. 서로 다른 정책 하에서 상태의 가치를 비교함으로써, 어떤 정책이 더 나은지 판단할 수 있다. 둘째, 기대값 관점은 확률적 환경과 정책을 자연스럽게 다룰 수 있게 해준다. 환경이나 정책이 결정적이지 않더라도, 가치 함수는 여전히 의미 있는 평가를 제공한다. 셋째, 이 관점은 벨만 방정식과 같은 재귀적 관계를 유도하는 데 필수적이다. 이를 통해 동적 프로그래밍이나 시간차 학습과 같은 효율적인 계산 방법이 가능해진다.<br />

실제 강화학습 알고리즘에서는 이 가치 함수를 직접 계산하거나 근사하는 것이 핵심 과제이다. 정책 평가(policy evaluation)는 주어진 정책에 대한 가치 함수를 계산하는 과정이며, 정책 개선(policy improvement)은 계산된 가치 함수를 기반으로 더 나은 정책을 찾는 과정이다. 이 두 과정이 반복되면서 최적 정책에 접근하게 된다.
</details>
</details>

<details>
<summary>8. 행동 가치 함수의 정의와 상태 가치 함수와의 관계를 수식으로 설명하고, Q함수의 유용성을 논의하시오.</summary>

\(Q_\pi(s,a)\)를 수식으로 정의하고, 상태 가치 함수 \(V_\pi(s)\)와의 관계를 수식으로 보여준 뒤, 사례를 통해 Q함수의 유용성을 설명한다.
<details>
<summary>정답</summary>
행동 가치 함수 \(Q_\pi(s,a)\), 일반적으로 Q-함수라고 불리는 이 개념은 상태 \(s\)에서 행동 \(a\)를 취한 후 정책 \(\pi\)를 따를 때 기대되는 미래 누적 보상을 나타낸다. 수학적으로 이는 다음과 같이 정의된다:<br />

\[
Q_\pi(s,a)=\mathbb{E}_\pi\Bigl[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\mid S_t=s,A_t=a\Bigr]
\]<br />

이 정의는 상태 가치 함수 \(V_\pi(s)\)와 유사하지만, 중요한 차이점은 Q-함수는 초기 행동 \(a\)가 명시적으로 지정된다는 점이다. 이는 특정 상태에서 특정 행동을 취했을 때의 가치를 평가할 수 있게 해준다.<br />

상태 가치 함수 \(V_\pi(s)\)와 행동 가치 함수 \(Q_\pi(s,a)\) 사이에는 다음과 같은 중요한 관계가 있다:<br />

\[
V_\pi(s)=\sum_a\pi(a\mid s)Q_\pi(s,a)
\]<br />

이 수식은 상태 \(s\)의 가치가 해당 상태에서 정책 \(\pi\)에 따라 취할 수 있는 각 행동의 가치를 해당 행동을 선택할 확률로 가중 평균한 것임을 보여준다. 다시 말해, \(V_\pi(s)\)는 정책 \(\pi\)에 따른 \(Q_\pi(s,a)\)의 기대값이다.<br />

Q-함수는 강화학습에서 여러 가지 중요한 이유로 특히 유용하다. 첫째, Q-함수는 직접적으로 행동 선택에 사용될 수 있다. 특정 상태에서 가능한 모든 행동의 Q-값을 비교함으로써, 에이전트는 가장 가치 있는 행동을 선택할 수 있다. 이는 최적 정책 \(\pi^*\)를 다음과 같이 정의할 수 있게 한다: \(\pi^*(s) = \arg\max_a Q^*(s,a)\).<br />

둘째, Q-함수는 모델 없는(model-free) 학습을 가능하게 한다. 환경의 전이 확률이나 보상 함수를 모르더라도, 경험을 통해 직접 Q-값을 추정할 수 있다. 이는 실제 세계의 복잡한 문제에서 특히 중요한데, 이러한 환경에서는 정확한 모델을 구하기 어렵기 때문이다.<br />

실제 사례로, Q-함수의 유용성을 보여주는 대표적인 알고리즘이 Q-러닝(Q-learning)이다. 자율주행차 시나리오를 고려해보자. 차량은 현재 도로 상황(상태 \(s\))에서 가속, 감속, 좌회전, 우회전 등의 행동(행동 \(a\))을 선택할 수 있다. Q-러닝을 통해 차량은 각 상황에서 각 행동의 Q-값을 학습한다. 예를 들어, 앞에 보행자가 있는 상황에서 감속하는 행동은 높은 Q-값을 가질 것이고, 가속하는 행동은 낮은 Q-값을 가질 것이다. 학습이 완료되면, 차량은 각 상황에서 최고의 Q-값을 가진 행동을 선택함으로써 안전하고 효율적인 주행을 할 수 있다.<br />

또 다른 예로, 비디오 게임에서의 강화학습 에이전트를 생각해볼 수 있다. DQN(Deep Q-Network)과 같은 알고리즘은 딥러닝과 Q-함수를 결합하여 픽셀 데이터로부터 직접 게임 플레이를 학습한다. 에이전트는 게임 화면(상태)에서 여러 행동(버튼 입력)의 Q-값을 예측하고, 가장 높은 Q-값을 가진 행동을 선택한다. 이를 통해 모델 정보 없이도 아타리나 스타크래프트와 같은 복잡한 게임에서 인간 수준 이상의 성능을 달성할 수 있다.<br />

요약하면, Q-함수는 상태 가치 함수보다 더 세분화된 정보를 제공하고, 모델 없는 학습을 가능하게 하며, 직접적인 행동 선택에 사용될 수 있어 강화학습에서 핵심적인 역할을 한다.
</details>
</details>

<details>
<summary>9. Bellman 기대 방정식을 return 정의에서 출발하여 단계별로 유도하시오.</summary>

Bellman 기대 방정식  
\[
V_\pi(s)=\sum_a\pi(a\mid s)\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma V_\pi(s')]
\]  
를 return 정의에서 출발하여 단계별로 유도 과정을 상세히 서술한다.
<details>
<summary>정답</summary>
Bellman 기대 방정식은 강화학습의 이론적 기반을 형성하는 핵심 방정식으로, 상태 가치 함수의 재귀적 특성을 표현한다. 이 방정식을 리턴(return) 정의에서 시작하여 단계별로 체계적으로 유도해 보자.<br />

먼저, 시간 \(t\)에서의 리턴 \(G_t\)는 미래의 모든 할인된 보상의 합으로 정의된다:<br />
\[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]<br />

이 리턴을 현재 보상 \(R_{t+1}\)과 미래 보상의 합으로 분리할 수 있다:<br />
\[
G_t = R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+2}
\]<br />

여기서 두 번째 항은 \(G_{t+1}\)의 정의와 일치한다:<br />
\[
G_t = R_{t+1} + \gamma G_{t+1}
\]<br />

이제 상태 가치 함수 \(V_\pi(s)\)는 상태 \(s\)에서 시작하여 정책 \(\pi\)를 따를 때 기대되는 리턴으로 정의된다:<br />
\[
V_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
\]<br />

위의 리턴 분해를 이용하여 가치 함수를 다시 작성할 수 있다:<br />
\[
V_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]
\]<br />

기대값의 선형성을 이용하면 다음과 같이 분리할 수 있다:<br />
\[
V_\pi(s) = \mathbb{E}_\pi[R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
\]<br />

두 번째 항에서 \(G_{t+1}\)의 기대값은 \(S_{t+1}\)의 가치 함수와 동일하다. 그러나 현재는 \(S_{t+1}\)이 어떤 값이 될지 알 수 없으므로, 가능한 모든 다음 상태에 대해 조건부 기대값을 계산해야 한다. 조건부 기대값의 법칙을 적용하면:<br />
\[
\mathbb{E}_\pi[G_{t+1} | S_t = s] = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a, S_{t+1} = s']
\]<br />

마르코프 속성으로 인해, \(G_{t+1}\)은 \(S_t\)와 \(A_t\)가 주어졌을 때 오직 \(S_{t+1}\)에만 의존한다:<br />
\[
\mathbb{E}_\pi[G_{t+1} | S_t = s, A_t = a, S_{t+1} = s'] = \mathbb{E}_\pi[G_{t+1} | S_{t+1} = s'] = V_\pi(s')
\]<br />

따라서:<br />
\[
\mathbb{E}_\pi[G_{t+1} | S_t = s] = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) V_\pi(s')
\]<br />

유사하게, 첫 번째 항인 \(\mathbb{E}_\pi[R_{t+1} | S_t = s]\)도 다음과 같이 전개할 수 있다:<br />
\[
\mathbb{E}_\pi[R_{t+1} | S_t = s] = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) r(s,a,s')
\]<br />

두 항을 결합하면:<br />
\[
V_\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [r(s,a,s') + \gamma V_\pi(s')]
\]<br />

이것이 Bellman 기대 방정식이다. 이 방정식은 상태 \(s\)의 가치가 현재 보상과 다음 상태의 할인된 가치의 기대값의 합임을 보여준다. 이는 가치 함수의 재귀적 특성을 포착하며, 동적 프로그래밍 기법을 통해 가치 함수를 계산하는 기반이 된다.<br />

Bellman 기대 방정식은 정책 평가, 정책 반복, 그리고 가치 반복과 같은 많은 강화학습 알고리즘의 기초가 된다. 이 방정식을 통해 복잡한 순차적 결정 문제를 재귀적 하위 문제로 분해하여 효율적으로 해결할 수 있다.
</details>
</details>

<details>
<summary>10. Bellman 최적 방정식의 의미와 특성을 논의하시오.</summary>

최적 상태 가치 함수 \(V^*(s)\)는  
\[
V^*(s)=\max_a\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma V^*(s')]
\]
로 정의되고,  
최적 행동 가치 함수 \(Q^*(s,a)\)는  
\[
Q^*(s,a)=\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma\max_{a'}Q^*(s',a')]
\]
로 표현된다.
<details>
<summary>정답</summary>
Bellman 최적 방정식은 강화학습에서 최적 가치 함수와 최적 정책을 정의하는 중요한 수학적 표현이다. 이 방정식은 최적 의사결정 원칙을 반영하며, 순차적 결정 문제에서 장기적 보상을 최대화하는 방법을 설명한다.<br />

최적 상태 가치 함수 \(V^*(s)\)는 상태 \(s\)에서 시작하여 최적 정책을 따를 때 얻을 수 있는 최대 기대 리턴으로 정의된다. Bellman 최적 방정식에 따르면 이는 다음과 같이 표현된다:<br />

\[
V^*(s)=\max_a\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma V^*(s')]
\]<br />

이 방정식은 현재 상태 \(s\)에서 최적의 가치를 얻기 위해서는 가능한 모든 행동 중에서 현재 보상과 다음 상태의 할인된 최적 가치의 합을 최대화하는 행동을 선택해야 함을 나타낸다. 즉, 최적 가치 함수는 각 상태에서 '최선의 행동'을 선택하는 것을 가정한다.<br />

유사하게, 최적 행동 가치 함수 \(Q^*(s,a)\)는 상태 \(s\)에서 행동 \(a\)를 취한 후 최적 정책을 따를 때 얻을 수 있는 최대 기대 리턴으로 정의된다:<br />

\[
Q^*(s,a)=\sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma\max_{a'}Q^*(s',a')]
\]<br />

이 방정식에서 중요한 점은 현재 상태-행동 쌍 \((s,a)\)의 최적 가치가 다음 상태 \(s'\)에서 최적 행동을 선택한다는 가정 하에 계산된다는 것이다. 즉, \(\max_{a'}Q^*(s',a')\)는 다음 상태에서 가능한 모든 행동 중 최대 Q-값을 가진 행동을 선택함을 의미한다.<br />

Bellman 최적 방정식의 몇 가지 중요한 특성과 의미를 살펴보자:<br />

첫째, 이 방정식들은 최적 가치 함수의 재귀적 특성을 나타낸다. 최적 가치 함수는 자기 자신을 참조하는 방정식으로 표현되며, 이는 동적 프로그래밍 방법을 통해 이를 계산할 수 있게 해준다.<br />

둘째, 최적 가치 함수는 유일하게 존재한다. Bellman 최적 방정식은 수축 매핑(contraction mapping)의 특성을 가지며, 이는 반복적인 계산을 통해 결국 유일한 해에 수렴함을 보장한다.<br />

셋째, 최적 정책은 최적 가치 함수로부터 직접 유도될 수 있다. 구체적으로, 최적 정책 \(\pi^*(s)\)는 다음과 같이 정의된다:<br />

\[
\pi^*(s) = \arg\max_a Q^*(s,a)
\]<br />

또는 환경 모델이 있는 경우:<br />

\[
\pi^*(s) = \arg\max_a \sum_{s'}P(s'\mid s,a)[r(s,a,s')+\gamma V^*(s')]
\]<br />

넷째, 이 방정식은 최적성 원칙(principle of optimality)을 반영한다. 이 원칙에 따르면, 최적 정책의 일부분은 그 자체로도 최적 정책이다. 즉, 현재 상태에서 최적 행동을 선택하고, 그 결과로 도달한 모든 상태에서도 계속해서 최적으로 행동한다면, 전체 정책은 최적이다.<br />

다섯째, Bellman 최적 방정식은 가치 반복, Q-러닝, SARSA와 같은 많은 강화학습 알고리즘의 이론적 기반을 제공한다. 이러한 알고리즘들은 직간접적으로 이 방정식을 근사하거나 해결하는 방법을 구현한다.<br />

실제 응용에서 Bellman 최적 방정식의 주요 과제는 대규모 상태 공간에서의 계산 복잡성이다. 이를 해결하기 위해 함수 근사, 경험 재생, 타겟 네트워크와 같은 다양한 기법이 개발되었으며, 이러한 기법들은 복잡한 실제 문제에서 Bellman 최적 방정식의 원리를 효과적으로 적용할 수 있게 해준다.
</details>
</details>

<details>
<summary>11. 반복적 정책 평가 알고리즘의 동작 원리와 수렴 특성을 논의하시오.</summary>

```pseudo
Initialize V(s) arbitrarily for all s
Repeat:
  Δ=0
  For each state s:
    v=V(s)
    V(s)=Σ_aπ(a|s)Σ_{s'}P(s'|s,a)[r(s,a,s')+γV(s')]
    Δ=max(Δ,|v−V(s)|)
Until Δ&lt;θ
```
이 알고리즘은 \(γ&lt;1\) 및 유한 상태공간에서 수렴을 보장한다. 한 스윕당 계산 복잡도는 \(O(|S|⋅|A|⋅|S|)\)이며, \(γ\)와 초기값이 클수록 수렴 속도가 느려진다.
<details>
<summary>정답</summary>
반복적 정책 평가 알고리즘은 주어진 정책 π에 대한 상태 가치 함수를 계산하는 동적 프로그래밍 방법이다. 이 알고리즘은 벨만 기대 방정식을 반복적으로 적용하여 모든 상태의 가치를 점진적으로 개선한다.<br />

알고리즘의 핵심은 각 반복에서 모든 상태에 대해 벨만 업데이트를 수행하는 것이다. 구체적으로, 각 상태 s에서 현재 정책 π를 따라 선택 가능한 모든 행동의 기대값을 계산하고, 이를 통해 도달할 수 있는 모든 다음 상태의 가치를 고려한다. 이 과정에서 전이 확률과 보상, 그리고 다음 상태의 현재 추정 가치를 사용하여 상태 s의 새로운 가치를 계산한다.<br />

수렴 특성에 있어서, 할인율 γ가 1보다 작고 상태 공간이 유한하다면, 반복적 정책 평가는 진정한 가치 함수 Vπ로 수렴함이 수학적으로 증명되어 있다. 이는 벨만 연산자가 수축 매핑(contraction mapping)이라는 성질에 기인한다. 매 반복에서 최대 오차는 적어도 감소율 γ만큼 줄어들게 된다.<br />

계산 복잡도 측면에서, 한 번의 완전한 스윕(모든 상태 업데이트)은 O(|S|²|A|)의 시간 복잡도를 가진다. 이는 각 상태 s에 대해 모든 가능한 행동 a와 다음 상태 s'에 대한 계산이 필요하기 때문이다. 이러한 계산량은 상태 공간과 행동 공간이 커질수록 급격히 증가한다.<br />

수렴 속도는 여러 요인에 의해 영향을 받는다. 할인율 γ가 1에 가까울수록 먼 미래의 보상이 현재 가치에 더 큰 영향을 미치므로 수렴이 느려진다. 또한 초기 가치 함수 추정치가 실제 값과 크게 다를 경우에도 수렴 속도가 저하된다. 실제 구현에서는 적절한 초기화 전략과 종료 조건(θ)을 선택하는 것이 중요하다.
</details>
</details>

<details>
<summary>12. 정책 반복 알고리즘의 작동 원리와 수렴 특성을 논의하시오.</summary>

1. 정책 평가: 반복적 정책 평가로 V_π(s) 수렴  
2. 정책 개선:  
\[
π_{new}(s)=\arg\max_a\sum_{s'}P(s'\mid s,a)[r(s,a,s')+γV_π(s')]
\]  
이 과정을 정책이 변하지 않을 때까지 반복해 유한 MDP에서 최적 정책을 찾는다.
<details>
<summary>정답</summary>
정책 반복 알고리즘은 최적 정책을 찾기 위해 정책 평가와 정책 개선을 번갈아 수행하는 동적 프로그래밍 방법이다. 이 알고리즘은 두 핵심 단계로 구성되어 있으며, 이를 통해 점진적으로 더 나은 정책을 찾아간다.

첫 번째 단계인 정책 평가에서는, 현재 정책 π에 대한 가치 함수 Vπ를 계산한다. 이는 반복적 정책 평가 알고리즘을 사용하여 벨만 기대 방정식이 수렴할 때까지 반복적으로 적용함으로써 달성된다. 이 과정을 통해 현재 정책이 얼마나 좋은지를 정확히 평가할 수 있다.

두 번째 단계인 정책 개선에서는, 계산된 가치 함수를 기반으로 각 상태에서 더 나은 행동을 선택하여 정책을 개선한다. 구체적으로, 각 상태 s에서 모든 가능한 행동 a에 대해, 해당 행동을 취한 후 현재 가치 함수에 따라 기대되는 리턴을 계산하고, 가장 큰 기대 리턴을 제공하는 행동을 새로운 정책으로 선택한다.

정책 반복의 수렴 특성은 정책 개선 정리(Policy Improvement Theorem)에 의해 보장된다. 이 정리는 정책 개선 단계에서 생성된 새로운 정책 π'이 기존 정책 π보다 항상 같거나 더 나은 가치를 제공한다는 것을 증명한다. 또한, 유한한 MDP에서는 정책의 수가 유한하므로, 정책 반복은 유한한 반복 후에 최적 정책에 도달함이 보장된다.

계산 복잡도 측면에서, 정책 반복의 총 비용은 정책 평가 단계의 반복 횟수에 크게 의존한다. 각 정책 평가는 O(|S|²|A|)의 복잡도를 가지며, 정책 개선 단계도 유사한 복잡도를 가진다. 실제로는 정책 평가 단계에서 완전한 수렴을 기다리지 않고, 근사적으로 수렴한 후 정책 개선을 수행하는 변형된 알고리즘을 사용하여 계산 효율성을 높이기도 한다.
</details>
</details>

<details>
<summary>13. 가치 반복 알고리즘의 작동 원리와 Policy Iteration과의 차이점을 논의하시오.</summary>

```pseudo
Initialize V(s)=0 for all s
Repeat:
  Δ=0
  For each state s:
    v=V(s)
    V(s)=max_aΣ_{s'}P(s'|s,a)[r(s,a,s')+γV(s')]
    Δ=max(Δ,|v−V(s)|)
Until Δ&lt;θ
```
가치 반복은 정책 평가 없이 Bellman 최적 업데이트를 반복해 Policy Iteration보다 더 적은 연산으로 최적 해에 도달한다.
<details>
<summary>정답</summary>
가치 반복 알고리즘은 최적 가치 함수를 직접 계산하는 동적 프로그래밍 방법으로, 벨만 최적성 방정식을 반복적으로 적용하여 상태 가치를 갱신한다. 이 알고리즘은 정책을 명시적으로 유지하지 않고, 대신 각 상태의 최적 가치를 직접 계산한다.

가치 반복의 핵심 아이디어는 각 상태에서 모든 가능한 행동에 대한 기대 수익을 계산하고, 그 중 최대값을 해당 상태의 새로운 가치로 설정하는 것이다. 이는 벨만 최적성 방정식을 직접 적용하는 것으로, 각 업데이트가 일종의 '탐욕적인' 정책 개선과 제한된 정책 평가를 동시에 수행하는 효과가 있다.

정책 반복과 가치 반복의 주요 차이점은 정책 평가 단계의 처리 방식에 있다. 정책 반복은 각 반복에서 현재 정책에 대한 완전한 정책 평가를 수행하여 정확한 가치 함수를 계산한 후 정책을 개선한다. 반면, 가치 반복은 정책 평가와 개선을 각 상태 업데이트마다 즉시 결합하여 수행한다. 즉, 가치 반복은 정책 평가를 한 번만 반복한 후 바로 개선 단계로 넘어가는 것과 유사하다.

이러한 차이로 인해 가치 반복은 일반적으로 정책 반복보다 계산 효율성이 높다. 특히 상태 공간이 크고 정책 평가가 많은 반복을 필요로 할 때 이점이 두드러진다. 또한 가치 반복은 할인율 γ가 1에 가까울 때도 상대적으로 더 효율적인 경향이 있다.

수렴 측면에서, 가치 반복은 할인율이 1보다 작고 유한한 MDP에서 최적 가치 함수로 수렴함이 보장된다. 수렴 후에는 최적 정책을 쉽게 추출할 수 있으며, 이는 각 상태에서 최대 가치를 제공하는 행동을 선택하는 것으로 구성된다.
</details>
</details>

<details>
<summary>14. 비동기적 DP의 원리와 장점을 설명하시오.</summary>

비동기적 DP는 전체 상태 스윕 대신 임의 순서로 상태를 선택해 즉시 V(s)를 갱신하는 방식이다.  
이 방법은 중요도가 높은 상태를 우선 갱신해 수렴 속도를 높인다.
<details>
<summary>정답</summary>
비동기적 동적 프로그래밍(Asynchronous Dynamic Programming)은 전통적인 동기적 DP 방법과 달리 모든 상태를 동시에 업데이트하지 않고, 특정 순서나 선택 기준에 따라 상태를 개별적으로 업데이트하는 접근 방식이다. 이는 가우스-자이델(Gauss-Seidel) 방식의 최적화와 유사하게, 새롭게 업데이트된 가치 정보를 즉시 다른 상태 업데이트에 활용한다.

비동기적 DP의 핵심 원리는 모든 상태를 지속적으로 업데이트하되, 갱신된 값을 즉시 사용한다는 것이다. 예를 들어, 상태 s₁을 업데이트한 후, 그 갱신된 가치를 상태 s₂ 업데이트에 바로 사용할 수 있다. 이러한 방식은 정보가 더 빠르게 전파되도록 하며, 특히 가치 정보가 특정 방향으로 흐르는 경우에 효과적이다.

비동기적 DP의 주요 장점 중 하나는 계산 자원을 더 효율적으로 할당할 수 있다는 점이다. 모든 상태를 균등하게 처리하는 대신, 에이전트의 현재 관심 영역이나 가치 변화가 큰 영역에 더 많은 계산 자원을 투입할 수 있다. 이는 특히 대규모 상태 공간에서 중요한 이점이 된다.

또한, 비동기적 DP는 실시간 학습 상황에서도 유용하다. 에이전트가 환경과 상호작용하는 동안 접근하는 상태만을 선택적으로 업데이트함으로써, 전체 상태 공간을 다룰 필요 없이 관련 영역에 집중할 수 있다. 이를 통해 실시간 성능을 개선하고 계산 비용을 절감할 수 있다.

우선순위 기반 스위핑(Prioritized Sweeping)과 같은 발전된 비동기적 DP 방법은 벨만 오차가 큰 상태에 우선순위를 부여하여 더욱 효율적인 학습을 가능하게 한다. 이러한 방식은 가치 정보가 빠르게 변하는 상태에 계산 자원을 집중시켜 전체적인 수렴 속도를 높일 수 있다.
</details>
</details>

<details>
<summary>15. DP의 한계점과 이를 극복하기 위한 대안적 접근법을 논의하시오.</summary>

모델 기반 DP는 전이 확률과 보상 함수를 모두 알아야만 동작한다. 실제 환경에서는 이 정보를 알기 어려우므로, Monte Carlo, TD 학습, Dyna 방식과 같이 모델이 없어도 학습하거나 모델을 근사해 계획과 학습을 결합하는 기법을 사용한다.
<details>
<summary>정답</summary>
동적 프로그래밍(DP)은 마르코프 결정 과정(MDP)의 최적 정책을 찾는 강력한 방법이지만, 실제 응용에 있어 여러 중요한 한계점을 가지고 있다. 이러한 한계점을 이해하고 이를 극복하기 위한 대안적 접근법을 살펴보는 것은 실용적인 강화학습 시스템 개발에 필수적이다.

DP의 가장 큰 한계점은 환경 모델에 대한 완전한 지식을 필요로 한다는 점이다. 구체적으로, 모든 상태 전이 확률 P(s'|s,a)와 보상 함수 r(s,a,s')를 정확히 알아야 한다. 그러나 실제 환경에서는 이러한 정보를 정확하게 알기 어려운 경우가 많다. 복잡한 물리적 시스템, 인간 행동이 관여된 환경, 또는 미지의 변수가 많은 상황에서는 정확한 모델을 구축하는 것이 불가능할 수 있다.

또 다른 중요한 한계는 상태 공간과 행동 공간의 크기로 인한 계산 복잡성이다. DP 알고리즘의 계산 복잡도는 상태 수와 행동 수에 비례하여 증가하므로, 큰 규모의 문제에서는 계산량이 감당하기 어려울 정도로 증가한다. 이른바 '차원의 저주'로 인해 현실적인 시간 내에 문제를 해결하기 어려워진다.

또한 DP는 연속적인 상태 및 행동 공간을 직접 다루기 어렵다는 한계가 있다. 전통적인 DP 알고리즘은 이산적인 상태와 행동을 가정하므로, 연속적인 공간을 다루기 위해서는 이산화가 필요하지만, 이는 정확성 손실이나 계산 복잡성 증가로 이어질 수 있다.

이러한 한계를 극복하기 위해 여러 대안적 접근법이 개발되었다. 모델 없는(Model-free) 방법론인 몬테카를로 방법과 시간차 학습은 환경 모델에 대한 사전 지식 없이 직접 경험으로부터 최적 정책을 학습한다. 몬테카를로 방법은 완전한 에피소드의 경험을 통해 가치를 추정하고, 시간차 학습은 부트스트래핑을 통해 온라인으로 가치를 업데이트한다.

함수 근사 방법론은 신경망과 같은 파라미터화된 함수를 사용하여 가치 함수나 정책을 표현함으로써 대규모 또는 연속적인 상태 공간을 효과적으로 다룬다. 이 방법은 상태 공간의 일반화를 통해 차원의 저주 문제를 완화한다.

모델 기반 학습과 모델 없는 학습을 결합한 하이브리드 접근법인 Dyna 아키텍처는 실제 경험에서 환경 모델을 점진적으로 학습하고, 이 모델을 시뮬레이션하여 추가적인 계획을 수행한다. 이를 통해 직접 경험의 데이터 효율성과 모델 기반 계획의 장점을 모두 활용할 수 있다.

실시간 동적 프로그래밍은 현재 상태와 관련된 제한된 영역만 업데이트함으로써 계산 효율성을 높인다. 이는 대규모 상태 공간에서 효율적인 의사결정을 가능하게 한다.
</details>
</details>

<h3 id="bootstrap">Bootstrap</h3>

<p>부트스트래핑부트스트래핑은 다음 상태의 현재 추정값을 이용하여 현재 상태의 가치를 갱신하는 방법이다.
Monte Carlo, Dynamic Programming, 그리고 TD(0)를 비교·증명하고 특징을 살펴보자.</p>

<h3 id="1-monte-carlo-mc-방법">1. Monte Carlo (MC) 방법</h3>

<ul>
  <li><strong>전체 에피소드 리턴</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    G_t = \sum_{k=0}^{T-t-1} \gamma^k\,r_{t+k+1}
  \]
</div>
<ul>
  <li><strong>업데이트 식</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    V(s_t) \leftarrow V(s_t) + \alpha\bigl[G_t - V(s_t)\bigr]
  \]
</div>
<ul>
  <li><strong>증명 요약</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    \mathbb{E}[G_t]
    = \sum_{k=0}^{T-t-1}\gamma^k\,\mathbb{E}[r_{t+k+1}\mid s_t=s]
    = v_\pi(s)
  \]
</div>

<pre><code class="language-mermaid">flowchart LR
    classDef circleStyle fill:#fff,stroke:#000,stroke-width:2px;

    Start(("시작"))
    Init(("초기화: V(s) 및 returns 리스트"))
    Episode(("에피소드 생성"))
    Compute(("총 리턴 Gₜ 계산"))
    Update(("V(sₜ) ← V(sₜ) + α·[Gₜ − V(sₜ)]"))
    Check(("추가 에피소드?"))
    End(("종료"))

    class Start,Init,Episode,Compute,Update,Check,End circleStyle;

    Start --&gt; Init --&gt; Episode --&gt; Compute --&gt; Update --&gt; Check
    Check --&gt;|"예"| Init
    Check --&gt;|"아니오"| End
</code></pre>

<h3 id="2-dynamic-programming-dp">2. Dynamic Programming (DP)</h3>
<ul>
  <li><strong>Bellman 기대 방정식</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    v_\pi(s) = \mathbb{E}\bigl[r_{t+1} + \gamma\,v_\pi(s_{t+1}) \mid s_t=s\bigr]
  \]
</div>
<ul>
  <li><strong>반복 갱신 (정책 평가)</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    V_{k+1}(s) = \mathbb{E}\bigl[r_{t+1} + \gamma V_k(s_{t+1}) \mid s_t=s\bigr]
  \]
</div>
<ul>
  <li><strong>수렴성 (Contractive mapping)</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    T[V](s) = \mathbb{E}[r_{t+1} + \gamma V(s_{t+1}) \mid s_t=s]
  \]
  \[
    \|T[V] - T[V']\|_\infty \le \gamma\,\|V - V'\|_\infty
  \]
</div>

<pre><code class="language-mermaid">flowchart LR
    classDef circleStyle fill:#fff,stroke:#000,stroke-width:2px;

    Start(("시작"))
    Init(("초기화: V₀(s)"))
    Iterate(("값 반복 갱신:\nVₖ₊₁(s) = E[r + γ·Vₖ(s')]"))
    Converge(("‖Vₖ₊₁ − Vₖ‖ &lt; θ ?"))
    End(("종료"))

    class Start,Init,Iterate,Converge,End circleStyle;

    Start --&gt; Init --&gt; Iterate --&gt; Converge
    Converge --&gt;|"아니오"| Iterate
    Converge --&gt;|"예"| End

</code></pre>

<h3 id="3-temporal-difference-td0">3. Temporal-Difference (TD(0))</h3>

<p>MC와 DP의 절충: 부트스트래핑을 활용한 온라인 업데이트</p>
<ul>
  <li><strong>TD 오차 (Temporal Difference Error)</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    \delta_t = r_{t+1} + \gamma\,V(s_{t+1}) - V(s_t)
  \]
</div>

<ul>
  <li><strong>업데이트 식</strong></li>
</ul>
<div style="overflow-x: auto;">
  \[
    V(s_t) \leftarrow V(s_t) + \alpha\,\delta_t
  \]
</div>

<ul>
  <li>
    <p><strong>특징</strong></p>

    <ul>
      <li>온라인 업데이트: 한 스텝마다 즉시 갱신</li>
      <li>부트스트래핑을 활용</li>
      <li>편향↑, 분산↓ (MC와 반대)</li>
    </ul>
  </li>
</ul>

<pre><code class="language-mermaid">
flowchart LR
    classDef circleStyle fill:#fff,stroke:#000,stroke-width:2px;

    Start(("시작"))
    Init(("초기화 V(s)"))
    Observe(("상태 관측: sₜ"))
    Action(("행동 선택: aₜ ← π(sₜ)"))
    Step(("보상 및 다음 상태 관측:\nrₜ₊₁, sₜ₊₁"))
    Update(("TD 업데이트:\nδₜ 계산 및 V(sₜ) 갱신"))
    Term(("종료 여부 확인"))
    End(("종료"))

    class Start,Init,Observe,Action,Step,Update,Term,End circleStyle;

    Start --&gt; Init --&gt; Observe --&gt; Action --&gt; Step --&gt; Update --&gt; Term
    Term --&gt;|"아니오"| Observe
    Term --&gt;|"예"| End

</code></pre>

<div align="center">
  <img src="/images/backups.png" alt="bandit2" style="width: 100%;" />
</div>

<h3 id="on---policy-sarsa">On - policy SARSA</h3>

<p>TD 법은 다음 식을 따른다.</p>

<div style="overflow-x: auto;">
  \[
    V(s_t) \gets V(s_t) + \alpha \big(R_{t} + \gamma V(s_{t+1}) - V(s_t)\big)
  \]
</div>

<div style="overflow-x: auto;">
  \[
    Q_\pi(S_t, A_t) \gets Q_\pi(S_t, A_t) + \alpha \big(R_{t} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) - Q_\pi(S_t, A_t)\big)
  \]
</div>

<div align="center"> 
<div class="mermaid">
graph LR
st((s<sub>t</sub>)) --&gt; at((a<sub>t</sub>))
at --&gt; |Rt| st1((s<sub>t+1</sub>))
st1 --&gt; at1((a<sub>t+1</sub>))
</div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>
<span class="kn">from</span> <span class="n">common.utils</span> <span class="kn">import</span> <span class="n">greedy_probs</span>


<span class="k">class</span> <span class="nc">SarsaAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># deque 사용
</span>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>  <span class="c1"># pi에서 선택
</span>        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">next_q</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">]</span>  <span class="c1"># 다음 Q 함수
</span>
        <span class="c1"># TD법으로 self.Q 갱신
</span>        <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>
        
        <span class="c1"># 정책 개선
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span>


<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">SarsaAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>  <span class="c1"># 매번 호출
</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># 목표에 도달했을 때도 호출
</span>            <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">Q</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="off-sarsa">off SARSA</h3>

<p>오프정책에는 행동정책과 대상정책을 따로 가지고 있다. 행동정책은 다양한 행동을 시도하며 샘플 데이터를 폭넓게 수집하는 데 초점을 맞춘다. 이를 통해 환경에 대한 탐색을 극대화한다. 반면, 대상정책은 탐욕정책을 기반으로 하여 최적의 행동을 선택하고 갱신하는 데 사용된다. 이러한 구조는 행동정책과 대상정책의 역할을 분리하여 학습의 효율성을 높이는 데 기여한다.</p>

<div style="overflow-x: auto;">
  \[
    Q_\pi(S_t, A_t) \gets Q_\pi(S_t, A_t) + \alpha \big(R_{t} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) - Q_\pi(S_t, A_t)\big)
  \]
</div>

<p>강화학습에서 에이전트는 환경과의 상호작용을 통해 최적의 정책을 학습하게 된다. 이때, 에이전트가 따르는 정책과 학습에 사용되는 정책이 같다면 이를 <strong>on-policy</strong>, 다르다면 <strong>off-policy</strong>라고 부른다. Off-policy 학습은 정책 평가와 데이터 수집의 주체를 분리함으로써 더 유연하고 강력한 학습을 가능하게 한다. 대표적인 예로는 Q-learning, Expected SARSA, 그리고 이 글에서 다룰 <strong>Off-policy SARSA</strong>가 있다.</p>

<p>Off-policy SARSA는 기존의 SARSA 방식과 달리, 행동을 생성하는 정책과 학습을 위한 업데이트에 사용되는 정책을 분리한다. <strong>행동정책(behavior policy)</strong>은 데이터를 수집하는 데 사용되며, 보통 ε-greedy와 같이 일정 확률로 무작위 행동을 선택하는 <strong>탐험 중심의 정책</strong>이다. 반면, <strong>대상정책(target policy)</strong>은 실제로 Q 값을 업데이트할 때 기준이 되는 정책이며, 일반적으로 greedy 정책이나 soft policy가 사용된다. 이러한 구조는 다양한 행동을 시도하면서도 최적의 정책을 학습하는 데 유리한 조건을 제공한다.</p>

<p>그러나 행동정책과 대상정책이 다르기 때문에, 수집한 데이터가 학습에 직접적으로 반영되기에는 차이가 존재한다. 이 문제를 해결하기 위해 <strong>중요도 비율(Importance Sampling Ratio)</strong>이 도입된다. 중요도 비율은 <em>“이 행동이 대상정책이었다면 얼마나 일어났을까?”</em>를 확률적으로 보정해주는 계수이며, 수식으로는 다음과 같이 정의된다:</p>

<div style="overflow-x: auto;">
$$
\rho_t = \frac{\pi(A_t \mid S_t)}{\mu(A_t \mid S_t)}
$$
</div>

<p>여기서 \(\pi\)는 대상정책, \(\mu\)는 행동정책을 의미한다. 이 비율을 업데이트 식에 곱해줌으로써, 행동정책으로 수집한 데이터를 대상정책 관점에서 해석할 수 있게 된다.</p>

<p>Off-policy SARSA의 업데이트 식은 다음과 같다:</p>

<div style="overflow-x: auto;">
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \, \rho_t \left( R_t + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right)
$$
</div>

<p>위 식에서 \(\rho_t\)는 앞서 설명한 중요도 비율이며, 이를 통해 대상정책 기준으로 TD 오류 항을 조정하게 된다. 만약 \(\rho_t = 1\)이라면, 이는 on-policy 상황과 동일해진다. 반대로, 행동정책과 대상정책이 다를수록 \(\rho_t\)는 1에서 멀어지며, 보정의 영향력이 커진다.</p>

<p>이러한 방식은 이론적으로 매우 강력하지만, 실용적으로는 한 가지 주의할 점이 있다. \(\rho_t\)가 지나치게 크거나 작아질 경우, 학습 과정에서 <strong>분산이 커지고 불안정해질 수 있다.</strong> 이를 방지하기 위해 <strong>클리핑(clipping)</strong>이나 <strong>평균 정규화(mean normalization)</strong> 등의 기법이 사용되기도 한다. 특히 여러 시점에 걸쳐 중요도 비율을 누적해서 사용하는 경우(예: SARSA(\(\lambda\)))에는 분산 문제가 더욱 심각해지므로 주의가 필요하다.</p>

<p>Off-policy SARSA는 정책의 유연성과 데이터 재사용 가능성을 극대화할 수 있다는 점에서 매우 실용적인 접근법이며, 다양한 실제 환경에서도 효과적으로 활용될 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">common.gridworld</span> <span class="kn">import</span> <span class="n">GridWorld</span>
<span class="kn">from</span> <span class="n">common.utils</span> <span class="kn">import</span> <span class="n">greedy_probs</span>


<span class="k">class</span> <span class="nc">SarsaOffPolicyAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span>

        <span class="n">random_actions</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">action_probs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>  <span class="c1"># 행동 정책에서 가져옴
</span>        <span class="n">actions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">action_probs</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">rho</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">]</span>
            <span class="n">rho</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">next_state</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="p">[</span><span class="n">next_state</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span>  <span class="c1"># 가중치 rho 계산
</span>
        <span class="c1"># rho로 TD 목표 보정
</span>        <span class="n">target</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>

        <span class="c1"># 각각의 정책 개선
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pi</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nf">greedy_probs</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span>


<span class="n">env</span> <span class="o">=</span> <span class="nc">GridWorld</span><span class="p">()</span>
<span class="n">agent</span> <span class="o">=</span> <span class="nc">SarsaOffPolicyAgent</span><span class="p">()</span>

<span class="n">episodes</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">agent</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="p">.</span><span class="nf">render_q</span><span class="p">(</span><span class="n">agent</span><span class="p">.</span><span class="n">Q</span><span class="p">)</span>
</code></pre></div></div>

<p>start Point is (0, 0)
<br />
end Point is (5, 5)</p>

<div align="center">
  <img src="/images/sarsa.png" alt="bandit1" style="width: 90%;" />
</div>

<div align="center">
  <img src="/images/sarsa2.png" alt="bandit2" style="width: 90%;" />
</div>

<h3 id="q-learning">Q-learning</h3>

<p>Q-learning은 강화학습에서 가장 널리 쓰이는 알고리즘 중 하나로, off-policy TD 방법에 속한다. 에이전트는 환경과 상호작용하며 Q 함수를 업데이트하지만, 업데이트 대상은 실제로 선택한 행동이 아닌 미래에 가장 높은 Q 값을 갖는 행동에 기반한다. 이로 인해 Q-learning은 실제 행동과는 무관하게 탐욕적인(target) 정책을 따르는 학습이 가능해진다.</p>

<p>대표적인 특징은 다음과 같다.</p>

<p><code class="language-plaintext highlighter-rouge">TD(Temporal Difference) 학습</code>: 부트스트래핑 방식으로, 미래 상태의 Q 값을 이용해 현재 Q 값을 점진적으로 갱신한다.</p>

<p><code class="language-plaintext highlighter-rouge">Off-policy 학습</code>: 실제 행동은 ε-greedy 정책 등 탐험적인 행동정책을 따르지만, 학습은 항상 greedy한 대상정책 기준으로 이루어진다.</p>

<p><code class="language-plaintext highlighter-rouge">중요도 샘플링 불필요</code>: 대상정책이 항상 greedy하므로, 행동정책과의 차이를 보정할 필요가 없다. 따라서 중요도 비율(importance sampling ratio)을 계산하지 않는다.</p>

<p>Q-learning의 핵심 업데이트 식은 다음과 같다:</p>

<div style="overflow-x: auto;"> \[ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( R_t + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right) \] </div>

<p>이 식에서 핵심은 다음 상태 
\(S_t+1\)
에서 가능한 모든 행동 중 가장 큰 Q 값을 선택하여, 그것을 목표 값(target)으로 사용한다는 점이다. 즉, 실제 수행한 
\(A_t+1\)
와 무관하게, greedy한 가치를 기준으로 현재 Q 값을 업데이트하는 것이다.</p>

<p>이러한 방식은 학습 안정성과 수렴 이론 측면에서 유리하다. 특히, 수렴 조건이 잘 정의되어 있으며, 충분한 탐험과 적절한 학습률 하에서 최적의 Q 함수로 수렴함이 증명되어 있다. 다만, 탐험을 위한 ε-greedy 정책 등 별도의 수집 전략이 필요하며, 초기에 잘못된 Q 값이 고착되는 문제도 있을 수 있다. 이를 보완하기 위해 Double Q-learning, DQN, Prioritized Experience Replay 등의 다양한 확장 기법이 존재한다.</p>

						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
			<div class="section section--grey">
				<div class="container">
					<div class="row">
						<div class="col-md-7">
							<div class="comments-area">
  <h3 class="comments-title">Comments</h3>
  <div id="comments-list" class="comments-list">
    <!-- Comments will be populated here -->
  </div>
  
  <div class="comment-respond">
    <h4 class="comment-reply-title">Leave a Comment</h4>
    <form id="comment-form" class="comment-form">
      <div class="form-group">
        <label for="name">Name</label>
        <input type="text" id="name" name="name" required class="form-control">
      </div>
      <div class="form-group">
        <label for="email">Email</label>
        <input type="email" id="email" name="email" required class="form-control">
      </div>
      <div class="form-group">
        <label for="comment">Comment</label>
        <textarea id="comment" name="comment" required class="form-control" rows="5"></textarea>
      </div>
      <div class="form-group admin-field">
        <label for="password">Admin Password</label>
        <input type="password" id="password" name="password" class="form-control" placeholder="For comment moderation">
      </div>
      <div class="form-submit">
        <button type="submit" class="btn btn--dark btn--rounded">Submit Comment</button>
      </div>
    </form>
  </div>
</div>

<script>
  (function() {
    // Simple storage for comments using localStorage
    const COMMENTS_STORAGE_KEY = 'page_comments_/page/reinforce/';
    
    // Load comments from storage
    function loadComments() {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      const commentsList = document.getElementById('comments-list');
      commentsList.innerHTML = '';
      
      if (comments.length === 0) {
        commentsList.innerHTML = '<div class="no-comments">No comments yet. Be the first to comment!</div>';
        return;
      }
      
      comments.forEach((comment, index) => {
        const commentDiv = document.createElement('div');
        commentDiv.className = 'comment';
        commentDiv.dataset.id = index;
        
        const commentHTML = `
          <div class="comment-meta">
            <div class="comment-author">
              <strong>${comment.name}</strong>
            </div>
            <div class="comment-metadata">
              <span>${new Date(comment.date).toLocaleDateString()} ${new Date(comment.date).toLocaleTimeString()}</span>
            </div>
          </div>
          <div class="comment-content">
            <p>${comment.text}</p>
          </div>
          <div class="comment-actions">
            <button class="btn-link reply-btn" data-id="${index}">
              <i class="icon icon--arrow-right"></i> Reply
            </button>
            <button class="btn-link delete-btn" data-id="${index}">
              <i class="icon icon--cross"></i> Delete
            </button>
          </div>
          <div class="reply-form-wrapper" id="reply-form-${index}" style="display: none;">
            <form class="reply-form" data-parent="${index}">
              <div class="form-group">
                <label for="reply-name-${index}">Name</label>
                <input type="text" id="reply-name-${index}" name="name" required class="form-control">
              </div>
              <div class="form-group">
                <label for="reply-comment-${index}">Reply</label>
                <textarea id="reply-comment-${index}" name="comment" required class="form-control" rows="3"></textarea>
              </div>
              <div class="form-submit">
                <button type="submit" class="btn btn--dark btn--rounded btn--sm">Submit Reply</button>
              </div>
            </form>
          </div>
          <div class="children" id="replies-${index}">
            ${renderReplies(comment.replies || [])}
          </div>
        `;
        
        commentDiv.innerHTML = commentHTML;
        commentsList.appendChild(commentDiv);
      });
      
      // Add event listeners to reply buttons
      document.querySelectorAll('.reply-btn').forEach(button => {
        button.addEventListener('click', function() {
          const commentId = this.dataset.id;
          const replyForm = document.getElementById(`reply-form-${commentId}`);
          replyForm.style.display = replyForm.style.display === 'none' ? 'block' : 'none';
        });
      });
      
      // Add event listeners to delete buttons
      document.querySelectorAll('.delete-btn').forEach(button => {
        button.addEventListener('click', function() {
          const commentId = this.dataset.id;
          const password = prompt('Enter admin password to delete:');
          
          if (password === 'admin123') { // Simple password for demo
            deleteComment(parseInt(commentId));
          } else {
            alert('Incorrect password');
          }
        });
      });
      
      // Add event listeners to reply forms
      document.querySelectorAll('.reply-form').forEach(form => {
        form.addEventListener('submit', function(e) {
          e.preventDefault();
          const parentId = parseInt(this.dataset.parent);
          const replyName = this.querySelector('[name="name"]').value;
          const replyText = this.querySelector('[name="comment"]').value;
          
          addReply(parentId, replyName, replyText);
          this.reset();
          document.getElementById(`reply-form-${parentId}`).style.display = 'none';
        });
      });
    }
    
    // Render replies
    function renderReplies(replies) {
      if (!replies || replies.length === 0) return '';
      
      let html = '';
      replies.forEach((reply, replyIndex) => {
        html += `
          <div class="comment child-comment" data-id="${replyIndex}">
            <div class="comment-meta">
              <div class="comment-author">
                <strong>${reply.name}</strong>
              </div>
              <div class="comment-metadata">
                <span>${new Date(reply.date).toLocaleDateString()} ${new Date(reply.date).toLocaleTimeString()}</span>
              </div>
            </div>
            <div class="comment-content">
              <p>${reply.text}</p>
            </div>
            <div class="comment-actions">
              <button class="btn-link delete-reply-btn" data-parent="${replyIndex}">
                <i class="icon icon--cross"></i> Delete
              </button>
            </div>
          </div>
        `;
      });
      return html;
    }
    
    // Add a new comment
    function addComment(name, email, text) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      comments.push({
        name: name,
        email: email,
        text: text,
        date: new Date().toISOString(),
        replies: []
      });
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Add a reply to a comment
    function addReply(parentId, name, text) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      if (!comments[parentId].replies) {
        comments[parentId].replies = [];
      }
      
      comments[parentId].replies.push({
        name: name,
        text: text,
        date: new Date().toISOString()
      });
      
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Delete a comment
    function deleteComment(commentId) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      comments.splice(commentId, 1);
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Event listener for comment form
    document.getElementById('comment-form').addEventListener('submit', function(e) {
      e.preventDefault();
      
      const name = document.getElementById('name').value;
      const email = document.getElementById('email').value;
      const comment = document.getElementById('comment').value;
      
      addComment(name, email, comment);
      this.reset();
    });
    
    // Initial load
    document.addEventListener('DOMContentLoaded', loadComments);
  })();
</script>

<style>
  /* Comments styling that matches Doks theme */
  .comments-area {
    margin-top: 2.5rem;
    font-family: 'Noto Sans', sans-serif;
  }
  
  .comments-title {
    margin-bottom: 1.5rem;
    font-size: 1.75em;
    font-weight: 600;
    color: #333;
  }
  
  .comment {
    margin-bottom: 1.5rem;
    padding: 1.25rem;
    background-color: #fff;
    border-radius: 4px;
    box-shadow: 0 1px 4px rgba(0,0,0,.04);
    border-left: 4px solid #253951;
    transition: all .2s ease;
  }
  
  .comment:hover {
    box-shadow: 0 1px 10px rgba(0,0,0,.08);
  }
  
  .comment-meta {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 0.75rem;
  }
  
  .comment-author {
    font-weight: 600;
  }
  
  .comment-metadata {
    font-size: 0.75em;
    color: #8a8a8a;
  }
  
  .comment-content {
    line-height: 1.6;
    color: #333;
  }
  
  .comment-content p {
    margin-bottom: 0.5rem;
  }
  
  .comment-actions {
    margin-top: 0.75rem;
    padding-top: 0.5rem;
    border-top: 1px solid #f5f5f5;
  }
  
  .btn-link {
    background: none;
    border: none;
    padding: 0;
    color: #253951;
    cursor: pointer;
    font-size: 0.85em;
    text-decoration: none;
    margin-right: 1rem;
    transition: color .2s ease;
  }
  
  .btn-link:hover {
    color: #0056b3;
    text-decoration: underline;
  }
  
  .reply-form-wrapper {
    margin-top: 1rem;
    margin-bottom: 1rem;
    padding: 1rem;
    background-color: #f9f9f9;
    border-radius: 4px;
  }
  
  .children {
    margin-top: 1rem;
    margin-left: 1.5rem;
    padding-left: 1rem;
    border-left: 1px solid #eaeaea;
  }
  
  .child-comment {
    margin-bottom: 1rem;
    border-left: 3px solid #6c757d;
  }
  
  .no-comments {
    padding: 1rem;
    background-color: #f5f5f5;
    border-radius: 4px;
    font-style: italic;
    color: #666;
  }
  
  /* Form styling */
  .comment-respond {
    margin-top: 2rem;
    padding: 1.5rem;
    background-color: #f9f9f9;
    border-radius: 4px;
  }
  
  .comment-reply-title {
    margin-bottom: 1.25rem;
    font-size: 1.25em;
    font-weight: 600;
    color: #333;
  }
  
  .comment-form .form-group {
    margin-bottom: 1rem;
  }
  
  .comment-form label {
    display: block;
    margin-bottom: 0.5rem;
    font-weight: 600;
    color: #333;
  }
  
  .comment-form .form-control {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid #e0e0e0;
    border-radius: 4px;
    background-color: #fff;
    font-family: inherit;
    font-size: 0.95em;
    transition: border-color .2s ease;
  }
  
  .comment-form .form-control:focus {
    border-color: #253951;
    outline: none;
  }
  
  .admin-field {
    opacity: 0.8;
  }
  
  .form-submit {
    margin-top: 1.5rem;
  }
  
  /* Match the Doks buttons */
  .btn {
    display: inline-block;
    font-weight: 500;
    text-align: center;
    white-space: nowrap;
    vertical-align: middle;
    user-select: none;
    padding: 0.75rem 1.25rem;
    font-size: 1rem;
    line-height: 1.5;
    border-radius: 4px;
    transition: all .2s ease-in-out;
    text-decoration: none;
    cursor: pointer;
  }
  
  .btn--dark {
    background-color: #253951;
    border-color: #253951;
    color: #fff;
  }
  
  .btn--dark:hover {
    background-color: #1a2a3c;
    border-color: #1a2a3c;
  }
  
  .btn--rounded {
    border-radius: 100px;
  }
  
  .btn--sm {
    padding: 0.5rem 1rem;
    font-size: 0.875rem;
  }
</style>

						</div><!-- /.col -->
					</div><!-- /.row -->
				</div><!-- /.container -->
			</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
				<div class="micro-nav">
	<div class="container">
		<div class="row">
			<div class="col-xs-12">
				<a href="/" class="micro-nav__back">
					<i class="icon icon--arrow-left"></i>
					Back to homepage
				</a><!-- /.micro-nav__back -->
			</div><!-- /.col -->
		</div><!-- /.row -->
	</div><!-- /.container -->
</div><!-- /.micro-nav -->

			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
						<a href="/" class="site-footer__logo">InchanBaek Note</a>
					
					
						<hr>
						<p class="site-footer__copyright">Copyright &copy; 2017. - InchanBaek Note <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
					<div class="col-sm-6 align-right">
						<ul class="social-list">
							
								<li>
									<a href="https://www.linkedin.com/in/inchan-baek-728197265/" target="_blank" class="social-list__item social-list__item--linkedin">
										<i class="icon icon--linkedin"></i>
									</a>
								</li>
							
								<li>
									<a href="https://www.instagram.com/in_chanchan/" target="_blank" class="social-list__item social-list__item--instagram">
										<i class="icon icon--instagram"></i>
									</a>
								</li>
							
								<li>
									<a href="https://www.youtube.com/@icb6048" target="_blank" class="social-list__item social-list__item--youtube">
										<i class="icon icon--youtube"></i>
									</a>
								</li>
							
						</ul><!-- /.social-list -->
					</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>





		</div><!-- /.js-footer-area -->
	</body>
</html>