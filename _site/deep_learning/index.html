<!DOCTYPE html>
<html >
	<head>
		<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Deep Learning Basics | InchanBaek Note - AI, Architecture & Technology</title>

	<meta name="description" content="퍼셉트론부터 경사하강법까지, 딥러닝의 핵심 개념을 처음부터 차근차근 배우는 딥러닝 기초 튜토리얼입니다. 인공신경망의 기본 구조와 학습 과정을 이해하기 쉽게 설명합니다.">


	<meta name="keywords" content="딥러닝, 머신러닝, 인공신경망, 퍼셉트론, 시그모이드, ReLU, 경사하강법, 손실함수, 인찬백, 딥러닝 기초">

<meta name="author" content="Inchan Baek">
<!-- Open Graph / Facebook -->
<meta property="og:type" content="website">
<meta property="og:url" content="http://localhost:4000/deep_learning/">
<meta property="og:title" content="Deep Learning Basics | InchanBaek Note - AI, Architecture & Technology">
<meta property="og:description" content="퍼셉트론부터 경사하강법까지, 딥러닝의 핵심 개념을 처음부터 차근차근 배우는 딥러닝 기초 튜토리얼입니다. 인공신경망의 기본 구조와 학습 과정을 이해하기 쉽게 설명합니다.">
<!-- Twitter -->
<meta property="twitter:card" content="summary">
<meta property="twitter:url" content="http://localhost:4000/deep_learning/">
<meta property="twitter:title" content="Deep Learning Basics | InchanBaek Note - AI, Architecture & Technology">
<meta property="twitter:description" content="퍼셉트론부터 경사하강법까지, 딥러닝의 핵심 개념을 처음부터 차근차근 배우는 딥러닝 기초 튜토리얼입니다. 인공신경망의 기본 구조와 학습 과정을 이해하기 쉽게 설명합니다.">
<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="canonical" href="http://localhost:4000/deep_learning/">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep Learning Basics</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Deep Learning Basics" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="퍼셉트론부터 경사하강법까지, 딥러닝의 핵심 개념을 처음부터 차근차근 배우는 딥러닝 기초 튜토리얼입니다. 인공신경망의 기본 구조와 학습 과정을 이해하기 쉽게 설명합니다." />
<meta property="og:description" content="퍼셉트론부터 경사하강법까지, 딥러닝의 핵심 개념을 처음부터 차근차근 배우는 딥러닝 기초 튜토리얼입니다. 인공신경망의 기본 구조와 학습 과정을 이해하기 쉽게 설명합니다." />
<link rel="canonical" href="http://localhost:4000/deep_learning/" />
<meta property="og:url" content="http://localhost:4000/deep_learning/" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Learning Basics" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"퍼셉트론부터 경사하강법까지, 딥러닝의 핵심 개념을 처음부터 차근차근 배우는 딥러닝 기초 튜토리얼입니다. 인공신경망의 기본 구조와 학습 과정을 이해하기 쉽게 설명합니다.","headline":"Deep Learning Basics","url":"http://localhost:4000/deep_learning/"}</script>
<!-- End Jekyll SEO tag -->

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/moonspam/NanumSquare@1.0/nanumsquare.css">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<link rel="stylesheet" href="/doks-theme/assets/css/custom.css">
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      securityLevel: 'loose'
    });
    
    // Find all pre blocks with mermaid class and render them
    document.querySelectorAll('pre code.language-mermaid').forEach(function(element) {
      // Get the mermaid code
      const mermaidCode = element.textContent;
      
      // Create a new div for the mermaid diagram
      const newDiv = document.createElement('div');
      newDiv.className = 'mermaid';
      newDiv.textContent = mermaidCode;
      
      // Replace the pre element with the new div
      const preElement = element.parentElement;
      preElement.parentElement.replaceChild(newDiv, preElement);
    });
    
    // Trigger mermaid render
    mermaid.init();
  });
</script>

	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		


	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">InchanBaek Note</a>
					
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Deep Learning Basics</h1>
								
								
									<p class="hero-subheader__desc">퍼셉트론부터 경사하강법까지, 딥러닝의 핵심 개념을 처음부터 차근차근 배우는 딥러닝 기초 튜토리얼입니다. 인공신경망의 기본 구조와 학습 과정을 이해하기 쉽게 설명합니다.</p>
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="content">
							<h2 id="introduction-to-ai">Introduction to AI</h2>
<p>This document summarizes key concepts from <em>Deep Learning from Scratch (Volume 1).</em></p>
<h2 id="what-is-a-perceptron">What is a Perceptron?</h2>
<p><u>퍼셉트론</u>이란 무엇인가?<br /></p>

<p>퍼셉트론은 인공신경망의 한 종류로, 다수의 입력을 받아 하나의 출력을 내보내는 알고리즘이다. 
퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호를 출력한다. 이때 입력 신호에는 각각 고유한 가중치가 곱해지는데, 이 가중치는 각 신호의 중요도를 조절하는 매개변수이다.
뉴런에서 보내온 신호의 총합이 정해진 한계를 넘어설 때만 1을 출력한다.</p>

<p>1은 신호가 흐른다, 0은 신호가 흐르지 않는다.</p>

<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

\[y =
\begin{cases}
0, &amp; w_1 x_1 + w_2 x_2 \leq \theta \\
1, &amp; w_1 x_1 + w_2 x_2 &gt; \theta
\end{cases}\]

<h2 id="simple-logic-circuits">Simple Logic Circuits</h2>
<p>퍼셉트론을 이용하면 간단한 논리회로를 구현할 수 있다.</p>
<h3 id="and-gate">AND Gate</h3>

<p>입력이 모두 1일 때만 출력이 1이 되는 논리회로이다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">x1</th>
      <th style="text-align: center">x2</th>
      <th style="text-align: center">y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">AND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">*</span> <span class="n">w2</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">tmp</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
</code></pre></div></div>

<h3 id="nand-gate-and-or-gate">NAND Gate and OR Gate</h3>

<p>NAND 게이트는 AND 게이트의 출력을 반전시킨 것이다. 즉, 입력이 모두 1일 때만 출력이 0이 된다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">x1</th>
      <th style="text-align: center">x2</th>
      <th style="text-align: center">y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">NAND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span>  <span class="c1"># 가중치와 임계값 설정
</span>    <span class="n">tmp</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">*</span> <span class="n">w2</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">tmp</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
</code></pre></div></div>

<p>OR 게이트는 입력 신호 중 하나 이상이 1이면 출력이 1이 되는 논리회로이다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">x1</th>
      <th style="text-align: center">x2</th>
      <th style="text-align: center">y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">OR</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">*</span> <span class="n">w2</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">tmp</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
</code></pre></div></div>

<h3 id="introducing-weights-and-bias">Introducing Weights and Bias</h3>

<p>세타를 -b로 치환하면 다음과 같이 표현할 수 있다.<br /></p>

\[y =
\begin{cases}
0, &amp; b + w_1 x_1 + w_2 x_2 \leq 0 \\
1, &amp; b + w_1 x_1 + w_2 x_2 &gt; 0
\end{cases}\]

<p>여기서 b는 편향이라고 하며, w1과 w2는 가중치이다.</p>

<h3 id="implementing-weights-and-bias">Implementing Weights and Bias</h3>

<p>넘파이를 이용해 가중치와 편향을 도입한 AND 게이트를 구현해보자.<br />
그렇다면 왜 넘파이를 사용하는가? 넘파이를 사용하면 배열을 쉽게 다룰 수 있기 때문이다.<br />
또한 넘파이 모듈은 그래픽카드를 이용한 병렬 계산을 지원하기 때문에 빠르게 계산할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">AND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.7</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">NAND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>  <span class="c1"># AND 게이트와 가중치 반대
</span>    <span class="n">b</span> <span class="o">=</span> <span class="mf">0.7</span>  <span class="c1"># 편향 변경
</span>    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">OR</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.2</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
</code></pre></div></div>

<p>가중치는 w1과 w2는 각 입력 신호가 결과에 주는 영향력(중요도)을 조절하는 매개변수<br />
편향은 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수</p>

<p>위의 단층 퍼셉트론을 그래프로 나타내면 다음과 같다.</p>

<p><img src="/images/AND_perceptron.png" width="600" /></p>

<p><img src="/images/OR_perceptron.png" width="600" /></p>

<p><img src="/images/NAND_perceptron.png" width="600" /></p>

<h2 id="limitations-of-perceptrons">Limitations of Perceptrons</h2>

<p>우리는 지금까지 AND, OR, NAND 게이트를 구현했다. 이제 XOR 게이트를 구현해보자.<br />
XOR 게이트는 배타적 논리합이라는 논리회로이다. 즉, x1과 x2 중 한쪽이 1일 때만 출력이 1이 된다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">x1</th>
      <th style="text-align: center">x2</th>
      <th style="text-align: center">y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<p>XOR 게이트는 단층 퍼셉트론으로 구현할 수 없다.<br />
그 이유는 단층 퍼셉트론이 <strong>직선 하나로 나눌 수 있는 영역</strong>만 표현할 수 있기 때문이다.<br />
하지만 XOR 게이트는 <strong>단일 직선으로 구분할 수 없는 패턴</strong>을 가진다.</p>

<p>아래 그림을 보면 이를 직관적으로 이해할 수 있다.</p>

<p><img src="/images/xor_gate_plot.png" alt="XOR" /></p>

<p>퍼셉트론은 직선 하나로 나눈 영역만 표현할 수 있기 때문에 XOR 게이트를 구현할 수 없다.
<br /></p>
<div style="text-align: center;">
<b>이러한 한계를 해결하기 위해 다층 퍼셉트론을 사용한다.</b>
</div>

<h3 id="multi-layer-perceptron">Multi-Layer Perceptron</h3>

<p>일단 XOR 게이트를 구현하려면 AND, NAND, OR 게이트를 조합해야 한다.<br /></p>

<div align="center">
    <img src="/images/basic.png" alt="basic logic gates" width="300" />
</div>

<p><br />
<b>어떻게 조합하면 XOR 게이트를 구현할 수 있을까?</b></p>

<div align="center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/a/a2/254px_3gate_XOR.jpg" alt="XOR 게이트" width="300" />
</div>

<p><br />
위 그림을 보면 XOR 게이트는 AND, NAND, OR 게이트를 조합해 만들 수 있다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">A</th>
      <th style="text-align: center">B</th>
      <th style="text-align: center">NAND</th>
      <th style="text-align: center">OR</th>
      <th style="text-align: center">AND</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">XOR</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">s1</span> <span class="o">=</span> <span class="nc">NAND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">s2</span> <span class="o">=</span> <span class="nc">OR</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nc">AND</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div>

<div align="center">
    <img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*qA_APGgbbh0QfRNsRyMaJg.png" alt="XOR 게이트" width="500" />
</div>

<p>위의 그림과 같이 다층구조의 퍼셉트론을 이용해 XOR 게이트를 구현할 수 있다.</p>

<h2 id="from-perceptron-to-neural-networks">From Perceptron to Neural Networks</h2>

<p>우리는 지금까지 퍼셉트론을 이용해 AND, OR, NAND, XOR 게이트를 구현했다.
신경망은 가중치의 매개변수를 적절한 값으로 자동으로 학습하는 능력이다.</p>

<h3 id="examples-of-neural-networks">Examples of Neural Networks</h3>

<p>신경망은 입력층, 은닉층, 출력층으로 구성되어 있다.</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph LR;
        subgraph Input Layer
            I1["input1"] 
            I2["input2"]
            I3["input3"]
        end

        subgraph Hidden Layer
            H1["Hidden1"]
            H2["Hidden2"]
            H3["Hidden3"]
        end

        subgraph Output Layer
            O1["output1"]
            O2["output2"]
        end

        %% 가중치를 포함한 엣지 연결
        I1 -- w1 --&gt; H1
        I2 -- w2 --&gt; H1
        I3 -- w3 --&gt; H1
        I1 -- w4 --&gt; H2
        I2 -- w5 --&gt; H2
        I3 -- w6 --&gt; H2
        I1 -- w7 --&gt; H3
        I2 -- w8 --&gt; H3
        I3 -- w9 --&gt; H3

        H1 -- w10 --&gt; O1
        H2 -- w11 --&gt; O1
        H3 -- w12 --&gt; O1
        H1 -- w13 --&gt; O2
        H2 -- w14 --&gt; O2
        H3 -- w15 --&gt; O2
    </div>
</div>

<p>신경망은 위에서 볼 수 있듯이 가중치를 곱한 입력 신호의 총합이 활성화 함수를 거쳐 출력값을 내보낸다.
가중치는 사람이 직접 설정하는 것이 아니라, 데이터를 학습하여 자동으로 설정된다. 이것이 신경망의 중요한 특징이다.</p>

<h3 id="emergence-of-activation-functions">Emergence of Activation Functions</h3>

<p>신경망의 활성화 함수는 입력 신호의 총합을 출력 신호로 변환하는 함수이다.
활성화 함수는 h(x)로 표현하며, 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다.</p>

\[y = h(b + w_1 x_1 + w_2 x_2)\]

<div style="text-align: center;">
    <div class="mermaid">
    graph LR;
        subgraph h_x
            style h_x fill:#f9f9f9,stroke:#333,stroke-width:2px,rx:50,ry:50;
            A[h]
            Y[y]
        end

        X1[x₁] -- w₁ --&gt; A[h]
        X2[x₂] -- w₂ --&gt; A[h]
        B[b] -- b --&gt; A[h]
        A -- h(b + w1x1 + w2x2) --&gt; Y[y]
    </div>
</div>

<p>위의 그림은 활성화 함수 처리 과정을 나타낸다.</p>

<h3 id="step-function">step Function</h3>
<p>활성화 함수에 대해 알아보자. 먼저 계단 함수를 소개한다.</p>

\[h(x) =
\begin{cases} 
0, &amp; x \leq 0 \\
1, &amp; x &gt; 0
\end{cases}\]

<p>위의 활성화 함수는 계단 함수라고 한다. 계단 함수는 입력이 0을 넘으면 1을 출력하고, 그 외에는 0을 출력한다.</p>

<div align="center">
    <img src="/images/step_function.png" alt="step function" width="400" />
</div>

<h3 id="sigmoid-function">Sigmoid Function</h3>

<p>두번째로 소개할 활성화 함수는 시그모이드 함수이다. 시그모이드 함수는 입력이 커지면 1에 가까워지고, 작아지면 0에 가까워진다.</p>

<p>신경망에서는 활성화 함수로 시그모이드 함수를 이용해 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달한다.</p>

\[h(x) = \frac{1}{1 + \exp(-x)}\]

<div align="center">
    <img src="/images/sigmoid_function.png" alt="sigmoid function" width="400" />
</div>
<h3 id="comparison-of-sigmoid-and-step-functions">Comparison of Sigmoid and Step Functions</h3>
<p>두개의 활성화 함수를 비교해보자.
시그모이드 함수는 곡선이며, 입력에 따라 출력이 연속적으로 변화한다. 반면 계단 함수는 0을 경계로 출력이 갑자기 변화한다.<br /><br />
두 활성화 함수의 공통점은 입력이 작을 때는 0에 가깝고, 입력이 커지면 1에 가까워진다는 것이다.
또한 두 함수는 모두 비선형 함수이다.(선형함수의 예시 : f(x) = ax + b, 비선형함수의 예시 : f(x) = x^2)</p>

<h3 id="non-linear-functions">Non-Linear Functions</h3>

<p>왜 비선형 함수를 사용해야 하는가? 비선형 함수를 사용하지 않으면 신경망의 층을 깊게 하는 의미가 없어진다.
선형함수의 문제는 다음과 같다. 층을 깊게 쌓아도 은닉층이 없는 네트워크로 표현할 수 있다.<br /> <br />
예를 들어, h(x) = cx라는 선형함수가 있다고 하자. 이 함수를 사용한 신경망은 y(x) = h(h(h(x)))로 표현할 수 있다. 이는 y(x) = c * c * c * x로 표현할 수 있으며, 이는 y(x) = ax로 표현할 수 있다. 따라서 선형함수를 사용하면 층을 깊게 쌓는 것이 의미가 없어진다.(a = c^3 으로 표현할 수 있기 때문)
그래서 층을 쌓기 위해서는 비선형 함수를 사용해야 한다.</p>

<h3 id="relu-function">ReLU Function</h3>

<p>신경망 분야에서는 최근에 ReLU 함수를 주로 사용한다. ReLU 함수는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력한다.</p>

\[h(x) = \max(0, x)\]

<div align="center">
    <img src="/images/relu_function.png" alt="relu function" width="400" />
</div>

<p>왜 시그모이드 함수 대신 ReLU 함수를 사용하는가? ReLU 함수는 시그모이드 함수보다 계산이 간단하다. 또한, 신경망의 학습 속도를 빠르게 하고, 효율적으로 학습할 수 있다.
<br /><br />
어떻게 ReLU 함수가 학습 속도를 빠르게 하는가? 시그모이드 함수는 입력이 작을 때 기울기가 0에 가까워지는 문제가 있다. 이는 역전파에서 기울기가 사라지는 문제를 야기한다. 하지만 ReLU 함수는 입력이 0 이상이면 기울기가 1이므로, 역전파에서 기울기가 사라지는 문제를 해결할 수 있다. 기울기가 사라진다는 의미가 무엇인가? 기울기가 사라지면 가중치가 제대로 갱신되지 않는다는 의미이다. 따라서 ReLU 함수를 사용하면 학습 속도가 빨라진다.</p>

<h2 id="computation-with-multi-dimensional-arrays">Computation with Multi-Dimensional Arrays</h2>

<p>넘파이를 이용하면 다차원 배열을 쉽게 다룰 수 있다. 넘파이를 이용해 행렬의 곱셈을 계산해보자.</p>

<p>-1차원 배열</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="c1"># [1 2 3 4]
</span></code></pre></div></div>

<p>-2차원 배열</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="c1"># [[1 2] [3 4] [5 6]]
</span></code></pre></div></div>

<p>-행렬의 곱셈</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="c1"># [22 28]
</span></code></pre></div></div>
<p>행렬의 곱셈은 다음과 같이 계산된다.
하지만 행렬의 곱셈은 행렬의 대응하는 차원의 원소 수가 일치해야 한다. 즉, 앞의 행렬의 열 수와 뒤의 행렬의 행 수가 일치해야 한다. 또한 행렬의 곱셈은 교환법칙이 성립하지 않는다.</p>

<div align="center">
    <img src="/images/matrix_multiplication.png" alt="matrix multiplication" width="400" />
</div>
<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="c1"># [[19 22] [43 50]]
</span><span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="c1"># [[23 34] [31 46]]
</span></code></pre></div></div>

<h2 id="implementing-a-three-layer-neural-network">Implementing a Three-Layer Neural Network</h2>

<p>이제 3층 신경망을 구현해보자. 이 신경망은 입력층, 은닉층, 출력층으로 구성되어 있다.</p>

<div style="text-align: center;">
    <div class="mermaid">
        graph LR;
    %% 입력층
    X1["x₁"] --&gt; H1
    X1 --&gt; H2
    X1 --&gt; H3
    X2["x₂"] --&gt; H1
    X2 --&gt; H2
    X2 --&gt; H3

    subgraph Input
        style Input fill:#add8e6,stroke:#333,stroke-width:2px,rx:20,ry:20;
        X1["x₁"]
        X2["x₂"]
    end

    %% 첫 번째 은닉층 (1층)
    subgraph Hidden1
        style Hidden1 fill:#f9f9f9,stroke:#333,stroke-width:2px,rx:20,ry:20; 
        H1["●"]
        H2["●"]
        H3["●"]
    end

    %% 첫 번째 은닉층에서 두 번째 은닉층으로 연결
    H1 --&gt; I1
    H1 --&gt; I2
    H2 --&gt; I1
    H2 --&gt; I2
    H3 --&gt; I1
    H3 --&gt; I2

    %% 두 번째 은닉층 (2층)
    subgraph Hidden2
        style Hidden2 fill:#f9f9f9,stroke:#333,stroke-width:2px,rx:20,ry:20;
        I1["●"]
        I2["●"]
    end

    %% 두 번째 은닉층에서 출력층으로 연결
    I1 --&gt; Y1["y₁"]
    I1 --&gt; Y2["y₂"]
    I2 --&gt; Y1
    I2 --&gt; Y2

    %% 출력층
    subgraph Output
        style Output fill:#ffc0cb,stroke:#333,stroke-width:2px,rx:20,ry:20;
        Y1["y₁"]
        Y2["y₂"]
    end

    %% 스타일 적용
    classDef neuron fill:#f9f9f9,stroke:#333,stroke-width:2px,radius:50%;
    class X1,X2,H1,H2,H3,I1,I2,Y1,Y2 neuron;

    </div>
</div>

<h3 id="implementing-signal-transmission-in-each-layer">Implementing Signal Transmission in Each Layer</h3>

<div style="text-align: center;">
    <div class="mermaid">
        graph LR;
    subgraph Input
        style Input fill:#add8e6,stroke:#333,stroke-width:2px,rx:20,ry:20;
        X1["x₁"]
        X2["x₂"]
    end
    %% 입력층
    B["1"] --&gt;|b₁¹| A1
    X1["x₁"] --&gt;|w₁₁¹| A1
    X1 --&gt;|w₁₂¹| A2
    X1 --&gt;|w₁₃¹| A3
    X2["x₂"] --&gt;|w₂₁¹| A1
    X2 --&gt;|w₂₂¹| A2
    X2 --&gt;|w₂₃¹| A3

    %% 첫 번째 은닉층 (1층)
    subgraph Hidden1
        style Hidden1 fill:#f9f9f9,stroke:#333,stroke-width:2px,rx:20,ry:20;
        A1["a₁¹"]
        A2["a₂¹"]
        A3["a₃¹"]
    end

    %% 스타일 적용
    classDef neuron fill:#f9f9f9,stroke:#333,stroke-width:2px,radius:50%;
    class B,X1,X2,A1,A2,A3 neuron;

    %% 강조 표시 (굵은 선)
    linkStyle 0 stroke-width:3px;

    </div>
</div>

<p>1층 뉴런 \(a_1^{(1)}\) 은 가중치를 곱한 신호 두 개와 편향을 합하여 계산한다</p>

\[a_1^{(1)} = w_{11}^{(1)} x_1 + w_{21}^{(1)} x_2 + b_1^{(1)}\]

<p>행렬의 곱을 이용하면 1층의 가중치 부분을 다음과 같이 간소화할 수 있다.</p>

\[A^{(1)} = X W^{(1)} + B^{(1)}\]

<p>이때 행렬 
\(A^{(1)}, X, B^{(1)}, W^{(1)}\)
는 각각 다음과 같다.</p>

\[A^{(1)} =
\begin{bmatrix}
a_1^{(1)} \\
a_2^{(1)} \\
a_3^{(1)}
\end{bmatrix},\]

<p><br /></p>

\[X =
\begin{bmatrix}
x_1 &amp; x_2
\end{bmatrix},\]

<p><br /></p>

\[B^{(1)} =
\begin{bmatrix}
b_1^{(1)} \\
b_2^{(1)} \\
b_3^{(1)}
\end{bmatrix},\]

<p><br /></p>

\[W^{(1)} =
\begin{bmatrix}
w_{11}^{(1)} &amp; w_{12}^{(1)} &amp; w_{13}^{(1)} \\
w_{21}^{(1)} &amp; w_{22}^{(1)} &amp; w_{23}^{(1)}
\end{bmatrix}\]

<p>입력층에서 1층으로의 신호 전달을 행렬의 곱으로 나타낼 수 있다.
그렇게 나온 결과를 활성화 함수에 넣어 출력값을 계산한다.</p>

<div style="text-align: center;">
    <div class="mermaid">
        graph LR;

            B["1"] --&gt;|b₁¹| A1
            X1["x₁"] --&gt;|w₁₁¹| A1
            X1 --&gt;|w₁₂¹| A2
            X1 --&gt;|w₁₃¹| A3
            X2["x₂"] --&gt;|w₂₁¹| A1
            X2 --&gt;|w₂₂¹| A2
            X2 --&gt;|w₂₃¹| A3

            A1["a₁¹"] --&gt;|h| Z1["z₁¹"]
            A2["a₂¹"] --&gt;|h| Z2["z₂¹"]
            A3["a₃¹"] --&gt;|h| Z3["z₃¹"]

        %% 스타일 적용
        classDef neuron fill:#f9f9f9,stroke:#333,stroke-width:2px,radius:50%;
        class B,X1,X2,A1,A2,A3,Z1,Z2,Z3 neuron;
    </div>
</div>

<p>왜 비선형 활성화함수의 출력값을 다음 레이어의 입력값으로 사용해야 하는지 알 수 있다. 만약 활성화 함수가 없다면, 신경망은 선형함수가 되어버린다. 즉, 층을 깊게 쌓는 것이 의미가 없어진다. 따라서 비선형 활성화 함수를 사용해야 한다. 또한 비선형 활성화 함수를 사용하면 신경망이 더 복잡한 문제를 풀 수 있다.</p>

<div style="text-align: center;">
    <div class="mermaid">
     graph LR
    %% 입력층 (Input Layer)
    subgraph Input["Input Layer"]
        style Input fill:#add8e6,stroke:#333,stroke-width:2px,rx:20,ry:20;
        X1["x₁"]
        X2["x₂"]
    end

    %% 편향 뉴런 (Bias)
    B1["1"] 
    B2["1"] 
    B3["1"] 

    %% 1층 (Hidden Layer 1)
    subgraph Hidden1["Hidden Layer 1"]
        style Hidden1 fill:#f9f9f9,stroke:#333,stroke-width:2px,rx:20,ry:20;
        A1["a₁¹"] --&gt;|σ| Z1["z₁¹"]
        A2["a₂¹"] --&gt;|σ| Z2["z₂¹"]
        A3["a₃¹"] --&gt;|σ| Z3["z₃¹"]
    end

    %% 2층 (Hidden Layer 2)
    subgraph Hidden2["Hidden Layer 2"]
        style Hidden2 fill:#f9f9f9,stroke:#333,stroke-width:2px,rx:20,ry:20;
        A4["a₁²"] --&gt;|σ| Z4["z₁²"]
        A5["a₂²"] --&gt;|σ| Z5["z₂²"]
    end

    %% 출력층 (Output Layer)
    subgraph Output["Output Layer"]
        style Output fill:#ffc0cb,stroke:#333,stroke-width:2px,rx:20,ry:20;
        A6["a₁³"] --&gt;|σ| Y1["y₁"]
        A7["a₂³"] --&gt;|σ| Y2["y₂"]
    end

    %% 입력층 → 1층
    X1 --&gt;|w₁₁¹| A1
    X1 --&gt;|w₁₂¹| A2
    X1 --&gt;|w₁₃¹| A3
    X2 --&gt;|w₂₁¹| A1
    X2 --&gt;|w₂₂¹| A2
    X2 --&gt;|w₂₃¹| A3
    B1 --&gt;|b₁¹| A1
    B1 --&gt;|b₂¹| A2
    B1 --&gt;|b₃¹| A3

    %% 1층 → 2층
    Z1 --&gt;|w₁₁²| A4
    Z2 --&gt;|w₂₁²| A4
    Z3 --&gt;|w₃₁²| A4
    Z1 --&gt;|w₁₂²| A5
    Z2 --&gt;|w₂₂²| A5
    Z3 --&gt;|w₃₂²| A5
    B2 --&gt;|b₁²| A4
    B2 --&gt;|b₂²| A5

    %% 2층 → 출력층
    Z4 --&gt;|w₁₁³| A6
    Z5 --&gt;|w₂₁³| A6
    Z4 --&gt;|w₁₂³| A7
    Z5 --&gt;|w₂₂³| A7
    B3 --&gt;|b₁³| A6
    B3 --&gt;|b₂³| A7

    %% 스타일 적용
    classDef neuron fill:#f9f9f9,stroke:#333,stroke-width:2px,radius:50%;
    class B1,B2,B3,X1,X2,A1,A2,A3,A4,A5,A6,A7,Z1,Z2,Z3,Z4,Z5,Y1,Y2 neuron;

    %% 강조 표시 (굵은 선)
    linkStyle 0 stroke-width:3px;
    linkStyle 1 stroke-width:3px;
    linkStyle 2 stroke-width:3px;
   

    </div>
</div>

<p>위의 진행식을 파이썬으로 구현해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">init_network</span><span class="p">():</span>
    <span class="n">network</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]),</span>
        <span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]),</span>
        <span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]),</span>
        <span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]),</span>
        <span class="sh">'</span><span class="s">W3</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]]),</span>
        <span class="sh">'</span><span class="s">b3</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">network</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">identity_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">W3</span> <span class="o">=</span> <span class="n">network</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="sh">'</span><span class="s">W3</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">network</span><span class="p">[</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="sh">'</span><span class="s">b3</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
    
    <span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
    
    <span class="n">a3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="n">W3</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nf">identity_function</span><span class="p">(</span><span class="n">a3</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">y</span>

<span class="n">network</span> <span class="o">=</span> <span class="nf">init_network</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">forward</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># [0.31682708 0.69627909]
</span></code></pre></div></div>

<h2 id="designing-the-output-layer">Designing the Output Layer</h2>

<p>가중치와 편향은 network 딕셔너리에 저장한다. 이 딕셔너리는 init_network 함수로 초기화한다.
순전파는 forward 함수로 구현한다. 이 함수는 입력 신호를 출력으로 변환하는 처리 과정을 모두 구현한다.
순전파 처리는 다음과 같다.</p>
<ol>
  <li>입력 신호를 1층의 가중치와 편향의 곱을 계산한다.</li>
  <li>1층의 출력을 활성화 함수인 시그모이드 함수에 넣어 출력을 계산한다.</li>
  <li>2층의 가중치와 편향의 곱을 계산한다.</li>
  <li>2층의 출력을 활성화 함수인 시그모이드 함수에 넣어 출력을 계산한다.</li>
  <li>3층의 가중치와 편향의 곱을 계산한다.</li>
  <li>출력층의 출력을 활성화 함수인 항등 함수에 넣어 최종 출력을 계산한다.</li>
</ol>

<p>신경망은 분류와 회귀 문제에 모두 사용할 수 있다. 분류는 데이터가 어느 클래스에 속하는지를 구분하는 문제이다. 회귀는 입력 데이터에서 (연속적인) 수치를 예측하는 문제이다.</p>

<p>일반적으로 분류에는 소프트맥스 함수를, 회귀에는 항등 함수를 사용한다.</p>

<h3 id="implementing-identity-and-softmax-functions">Implementing Identity and Softmax Functions</h3>

<p><strong>항등함수</strong>는 입력을 그대로 출력한다. 즉, 입력이 1이면 출력도 1이다. 항등 함수는 회귀 문제에 사용된다. 회귀 문제는 입력 데이터에서 연속적인 수치를 예측하는 문제이다. 예를 들어, 입력 데이터에서 주택 가격을 예측하는 문제가 있다. 이때 출력층의 활성화 함수로 항등 함수를 사용한다.</p>

<div style="text-align: center;">
    <div class="mermaid">
        graph LR;
            A6["a₁³"] --&gt;|σ| Y1["y₁"]
            A7["a₂³"] --&gt;|σ| Y2["y₂"]
    </div>
</div>
<p><br />
<br />
<strong>소프트맥스 함수</strong>는 다음과 같이 정의된다.</p>

\[y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">exp_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">sum_exp_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_a</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">exp_a</span> <span class="o">/</span> <span class="n">sum_exp_a</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div>

<p>n은 출력층의 뉴런 수이다. 소프트맥스 함수는 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받는다. 이때 소프트맥스 함수의 출력은 0에서 1.0 사이의 실수이며, 출력의 총합은 1이다. 이는 확률로 해석할 수 있다. 즉, 소프트맥스 함수를 이용해 문제를 확률적(통계적)으로 대응할 수 있다.</p>

<div style="text-align: center;">
    <div class="mermaid">
graph LR;
    A1["a₁"] --&gt;|σ| Y1["y₁"]
    A1 --&gt; Y2
    A1 --&gt; Y3
    A2["a₂"] --&gt;|σ| Y2["y₂"]
    A2 --&gt; Y1
    A2 --&gt; Y3
    A3["a₃"] --&gt;|σ| Y3["y₃"]
    A3 --&gt; Y1
    A3 --&gt; Y2
            
    </div>
</div>

<h3 id="considerations-when-implementing-the-softmax-function">Considerations When Implementing the Softmax Function</h3>

<p>exp(x)에 만약 큰 값이 들어가면 오버플로 문제가 발생할 수 있다. 이를 해결하기 위해 소프트맥스 함수를 다음과 같이 수정할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1010</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">990</span><span class="p">])</span>
</code></pre></div></div>

<p>이라면, exp(1010)은 너무 큰 값이므로 오버플로 문제가 발생한다. 이를 해결하기 위해 C를 빼주면 다음과 같다.
<br /></p>

<p>지수함수의 성질을 이용하여 오버플로 문제를 해결할 수 있다. 지수함수는 단조증가 함수이므로, 지수함수에 어떤 값을 더하거나 빼도 함수의 형태는 변하지 않는다. 따라서 입력 신호 중 최댓값을 빼주어 오버플로 문제를 해결할 수 있다.
<br /></p>

\[y_k = \frac{\exp(a_k - C)}{\sum_{i=1}^{n} \exp(a_i - C)}\]

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">exp_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span>
<span class="n">exp_a</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_a</span><span class="p">)</span>
<span class="c1"># [9.99954600e-01 4.53978686e-05 2.06106005e-09]
</span></code></pre></div></div>
<p>C는 입력 신호 중 최댓값이다. 이를 이용해 소프트맥스 함수를 구현해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">exp_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span>
    <span class="n">sum_exp_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_a</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">exp_a</span> <span class="o">/</span> <span class="n">sum_exp_a</span>
    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div>

<h3 id="characteristics-of-the-softmax-function">Characteristics of the Softmax Function</h3>

<p>소프트맥스 함수의 특징은 모든 출력이 0에서 1.0 사이의 실수이며, 출력의 총합은 1이라는 것이다. 이는 소프트맥스 함수의 출력을 ‘확률’로 해석할 수 있다. 즉, 소프트맥스 함수를 이용해 문제를 확률적(통계적)으로 대응할 수 있다.
<br />
<br />
하지만 소프트맥스 함수를 사용하지 않고 출력층의 각 뉴런의 출력값 중 가장 큰 값을 선택해도 결과는 같다. 이는 신경망의 출력이 가장 큰 뉴런에 해당하는 클래스로만 인식한다는 것이다. 이를 ‘단일 뉴런의 출력’이라고 한다.현업에서도 지수함수 계산에 드는 자원 낭비를 줄이기 위해 출력층의 소프트맥스 함수는 생략하는 경우가 많다. 이때는 출력층의 뉴런 중에서 가장 큰 값을 선택하면 된다.</p>

<h3 id="determining-the-number-of-neurons-in-the-output-layer">Determining the Number of Neurons in the Output Layer</h3>

<p>출력층의 뉴런 수는 문제에 따라 다른다. 예를 들어, 손글씨 숫자 인식에서는 10개의 숫자(0에서 9)를 구분해야 하므로 출력층의 뉴런 수는 10개이다. 이때 소프트맥스 함수를 이용해 출력층의 출력을 계산한다. 소프트맥스 함수의 출력은 각 클래스에 대응하는 확률로 해석할 수 있다. 즉, 소프트맥스 함수를 이용해 문제를 확률적(통계적)으로 대응할 수 있다.</p>

<h2 id="learning-from-data">Learning from Data</h2>

<p>딥러닝을 종단간 기계학습(end-to-end machine learning)이라고 한다. 이는 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다는 뜻이다. 즉, 데이터로부터 학습한다는 것이다.</p>

<h3 id="data-learning-approaches">Data Learning Approaches</h3>

<ul>
  <li><strong>Data-Driven Learning</strong>
<br />
기계학습에는 두가지 접근법이 있다. 하나는 사람이 규칙을 만드는 방법이고, 다른 하나는 데이터로부터 규칙을 찾아내는 방법이다. 전자는 전문가 시스템이라고 하며, 후자는 데이터 기반 기계학습이라고 한다. 딥러닝은 데이터 기반 기계학습에 속한다.</li>
  <li><strong>Training Data and Test Data</strong>
<br />
기계학습 문제는 데이터를 훈련 데이터와 시험 데이터로 나눠 학습과 실험을 수행한다. 훈련 데이터로 학습한 모델을 시험 데이터로 평가한다. 이를 통해 모델이 범용적으로 동작하는지 확인할 수 있다.
<br />
<br />
여기서 범용능력(generalization ability)이란, 아직 보지 못한 데이터(훈련 데이터에 포함되지 않은 데이터)로도 문제를 올바르게 풀어내는 능력을 말한다. 범용능력을 획득하는 것이 기계학습의 최종 목표이다.
<br />
<br />
또한 한 데이터셋에만 지나치게 최적화된 상태를 <strong>과적합(overfitting)</strong>이라고 한다. 이는 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하는 상태를 말한다. 과적합을 방지하는 것이 범용능력을 획득하는 핵심이다.</li>
</ul>

<h2 id="loss-function">Loss Function</h2>

<p>신경망의 학습에서는 현재의 상태를 하나의 지표로 표현한다. 이 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것이 학습의 목표이다. 이 지표를 <strong>손실 함수(loss function)</strong>이라고 한다. 손실 함수는 신경망 성능의 ‘나쁨’을 나타내는 지표이다. 즉, 손실 함수의 결과값이 작을수록 좋은 것이다.</p>

<h3 id="sum-of-squared-errors">Sum of Squared Errors</h3>
<p>가장 많이 쓰이는 손실 함수는 <strong>평균 제곱 오차(mean squared error, MSE)</strong>이다. 이는 신경망의 출력과 정답 레이블의 차이를 제곱한 후, 그 총합을 구한다. 평균 제곱 오차는 다음과 같이 정의된다.</p>

\[E = \frac{1}{2} \sum_{k} (y_k - t_k)^2\]

<p>여기서 \(y_k\)는 신경망의 출력, \(t_k\)는 정답 레이블, k는 데이터의 차원 수를 나타낸다.</p>

<p>예를 들어, 
신경망의 출력이 <strong>y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]</strong>이고, 
<br />
정답 레이블이 <strong>t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</strong>이라면 평균 제곱 오차는 다음과 같다.</p>

<p><strong>y는 소프트 맥스 함수의 출력이므로 0과 1 사이의 값이다.</strong><br />
<strong>정답 레이블은 원-핫 인코딩이므로 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다.</strong></p>

<p>따라서 정답 레이블과 소프트맥스 함수의 출력의 차이가 작을수록 평균 제곱 오차는 작아진다.
(원핫 인코딩이란, 정답의 인덱스만 1이고 나머지는 0인 인코딩 방식이다.)</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="c1"># 0.09750000000000003
</span></code></pre></div></div>

<p>즉, y는 소프트 맥스의 결과로 2일 확률이 가장 높다고 판단할 수 있고, t는 정답 2를 말한다. 
원소의 출력의 추정값과 정답의 차이가 작을수록 평균 제곱 오차는 작아진다.</p>

<p>만약에 y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]이라면 평균 제곱 오차는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="c1"># 0.5975
</span></code></pre></div></div>

<h3 id="cross-entropy-loss">Cross-Entropy Loss</h3>

<p>평균 제곱 오차는 신경망의 출력과 정답 레이블의 차이를 줄이면서 학습하는 것을 목표로 한다. 하지만 신경망의 출력이 확률로 해석될 때는 <strong>교차 엔트로피 오차(cross-entropy error)</strong>를 사용하는 것이 바람직하다. 교차 엔트로피 오차는 다음과 같이 정의된다.</p>

\[E = - \sum_{k} t_k \log y_k\]

<p>여기서 \(y_k\)는 신경망의 출력, \(t_k\)는 정답 레이블, k는 데이터의 차원 수를 나타낸다.
교차 엔트로피의 성질은 정답일 때의 출력이 전체 값을 정하게 된다.</p>

\[y = log(x)\]

<div align="center">
    <img src="/images/log_function.png" alt="log" width="400" />
</div>

<p>의 그래프를 보면 x가 1일때, y는 0이 되고, x가 0에 가까워질수록 y는음의 무한대로 커진다. 따라서 정답일 때의 출력이 전체 값을 정하게 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-7</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span>
</code></pre></div></div>

<p>delta의 값을 더하는 이유는 np.log() 함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없기 때문이다. 따라서 아주 작은 값을 더해준다.</p>

<p>예를 들어,</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="c1"># 0.510825457099338
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="c1"># 2.302584092994546
</span></code></pre></div></div>

<p>첫번째는 정답일 때의 출력이 0.6이고, 두번째는 0.1이다. 따라서 첫번째의 오차가 더 작다.
즉, 정답을 2라고 추정할 수 있다.</p>

<h3 id="mini-batch-learning">Mini-Batch Learning</h3>

<p>훈련데이터에 대한 손실함수의 값을 구하고, 이 값을 최대한 줄여주는 가중치 매개변수를 찾는 것이다. 이때 손실함수의 값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 목표이다.</p>

\[E = -\frac{1}{N} \sum_{n} \sum_{k} t_{nk} \log y_{nk}\]

<p>여기서 N은 데이터의 개수이다. 이때 손실함수의 값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 목표이다.
N으로 나누어 정규화한다. N으로 나눔으로써 “평균 손실함수”를 구할 수 있다. 이는 데이터 개수와 관계없이 통일된 지표를 얻을 수 있다.</p>

<p>모든 데이터를 대상으로 손실함수의 합을 구하면 시간이 오래 걸린다. 따라서 데이터 일부를 추려 전체의 근사치로 이용할 수 있다. 이를 <strong>미니배치 학습</strong>이라고 한다.</p>

<p>먼저 케라스에 있는 데이터 셋을 가지고와서 데이터를 가공해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>

<span class="c1"># MNIST 데이터 불러오기
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">훈련 데이터 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 28, 28)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">훈련 라벨 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">t_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000,)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">테스트 데이터 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (10000, 28, 28)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">테스트 라벨 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">t_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (10000,)
</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>  <span class="c1"># (60000, 784)
</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>    <span class="c1"># (10000, 784)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Flatten 후 훈련 데이터 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 784)
</span>
<span class="n">t_train</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">t_train</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">t_test</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">t_test</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">One-hot 변환 후 레이블 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">t_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 10)
</span></code></pre></div></div>

<p>x_train에서 10개의 데이터를 무작위로 추출해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_size</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
<span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
</code></pre></div></div>

<p>np.random.choice(a, b)는 0 이상 a 미만의 수 중에서 무작위로 b개를 골라낸다. 이를 이용해 미니배치를 뽑아낼 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="c1"># array([8013, 1232, 8549, 12334, 9815, 123,  456,  789,  1234, 5678])
</span></code></pre></div></div>

<p>원-핫 인코딩으로 변환된 정답 레이블을 사용할 경우</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
        
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
</code></pre></div></div>

<p>2, 7등의 레이블로 주어진 경우는 아래의 코드를 사용한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
        
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 예측 확률 분포 (3개의 샘플, 4개의 클래스)
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>  <span class="c1"># 첫 번째 샘플의 확률 분포
</span>    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>  <span class="c1"># 두 번째 샘플의 확률 분포
</span>    <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>  <span class="c1"># 세 번째 샘플의 확률 분포
</span><span class="p">])</span>

<span class="c1"># 정답 레이블 (정수형, 각 샘플의 정답 클래스 인덱스)
</span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># 첫 번째 샘플은 클래스 2, 두 번째 샘플은 클래스 0, 세 번째 샘플은 클래스 3
</span>
<span class="c1"># np.arange를 사용하여 정답 클래스의 확률값을 가져오기
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">selected_probs</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">t</span><span class="p">]</span>  <span class="c1"># 정답 클래스에 해당하는 확률만 선택
</span><span class="nf">print</span><span class="p">(</span><span class="n">selected_probs</span><span class="p">)</span>  <span class="c1"># [0.4 0.3 0.25]
</span>
<span class="c1"># y[np.arange(batch_size), t]의 작동방식
</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.4</span>   <span class="c1"># 첫 번째 샘플의 정답 클래스(2)의 확률값
</span><span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.3</span>   <span class="c1"># 두 번째 샘플의 정답 클래스(0)의 확률값
</span><span class="n">y</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.25</span>  <span class="c1"># 세 번째 샘플의 정답 클래스(3)의 확률값
</span></code></pre></div></div>

<h3 id="why-define-a-loss-function">Why Define a Loss Function?</h3>

<p>왜 손실함수를 설정하는가? 신경망 학습에서 미분의 역할에 주목하면 해결된다. 신경망의 목적은 손실함의 값을 가능한 한 작게 하는 매개변수를 찾는 것이다. 이때 매개변수의 미분(정확히는 기울기)을 계산하고, 그 미분값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다. 이때 손실함수의 미분값이 중요하다.</p>

<p>신경망을 학습할 때 정확도를 지표로 삼아서는 안된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다. 즉, 매개변수의 값을 조금 바꾼다고 해도 정확도는 거의 개선되지 않는다. 이는 계단 함수를 활성화 함수로 사용하지 않는 이유와도 일맥상통한다. 계단 함수는 미분값이 대부분 0이기 때문에 신경망 학습에 사용할 수 없다.</p>

<p>시그모이드 함수의 미분은 어느장소라도 0이 되지 않는다. 따라서 신경망 학습에 사용할 수 있다.</p>

<div align="center">
    <img src="/images/step_and_sigmoid_derivative.png" alt="sigmoid_derivative" width="800" />
</div>

<h3 id="gradient-descent-method">Gradient Descent Method</h3>

<p>경사하강법이란 함수의 기울기를 구해 기울기가 낮은 쪽으로 이동시키는 방법이다. 이때 기울기를 구할 때 사용하는 것이 바로 미분이다. 미분은 한순간의 변화량을 나타낸다. 이를 이용해 손실함수의 기울기를 구하고, 그 기울기의 반대 방향으로 매개변수를 갱신한다.</p>

\[\frac{\partial f(x)}{\partial x} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}\]

<p>이때 h는 0에 가까운 아주 작은 값이다. 이를 이용해 수치 미분을 구할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="nf">return </span><span class="p">(</span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
</code></pre></div></div>

<p>모든 변수의 편미분을 벡터로 정리한 것을 <strong>기울기(gradient)</strong>라고 한다. 기울기는 다음과 같이 구할 수 있다.</p>

\[(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial x_1})\]

<p>이때 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">tmp_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        
        <span class="c1"># f(x+h) 계산
</span>        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">+</span> <span class="n">h</span>
        <span class="n">fxh1</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># f(x-h) 계산
</span>        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">-</span> <span class="n">h</span>
        <span class="n">fxh2</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh1</span> <span class="o">-</span> <span class="n">fxh2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span>
        
    <span class="k">return</span> <span class="n">grad</span>
</code></pre></div></div>

<p>이를 이용해 \(f(x0, x1) = x0^2 + x1^2\)의 기울기를 구해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">function_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>

<span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]))</span>
<span class="c1"># array([6., 8.])
</span></code></pre></div></div>

<div align="center">
    <img src="/images/function_and_gradient.png" alt="gradient" width="900" />
</div>

<p>하지만 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지는 보장할 수 없다. 이는 기울기가 가리키는 방향이 꼭 최솟값이 아닐 수도 있기 때문이다. 이를 해결하기 위해 경사하강법을 사용한다.</p>

<p>경사법을 수식으로 나타내면 다음과 같다.</p>

\[x_0 = x_0 - \eta \frac{\partial f}{\partial x_0}\]

\[x_1 = x_1 - \eta \frac{\partial f}{\partial x_1}\]

<p>이때 \(\eta\)는 학습률을 의미한다. 이는 매개변수 값을 갱신하는 양을 나타낸다. 즉, 학습률은 매개변수 값을 얼마나 갱신하느냐를 정하는 하이퍼파라미터이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">init_x</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">step_num</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="nf">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
        
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>경사법으로 \(f(x0, x1) = x0^2 + x1^2\)의 최솟값을 구해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">function_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    

<span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="nf">gradient_descent</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">init_x</span><span class="o">=</span><span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="c1"># array([-6.11110793e-10,  8.14814391e-10])
</span></code></pre></div></div>

<p>이를 이용해 경사하강법을 구현할 수 있다. 이때 lr은 학습률을 의미한다. 학습률은 매개변수 값을 갱신할 때 얼마나 갱신할지를 정하는 하이퍼파라미터이다. 이 값이 너무 크거나 작으면 좋은 장소를 찾아갈 수 없다.</p>

<div align="center">
    <img src="/images/gradient_descent_convergence.png" alt="learning_rate" width="600" />
</div>

<h3 id="gradients-in-neural-networks">Gradients in Neural Networks</h3>

<p>신경망 학습에서도 기울기를 구해야 한다. 여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다. 이 기울기는 가중치 매개변수의 값을 갱신하기 위해 사용한다. 이때 가중치 매개변수의 기울기를 구해야 한다. 이를 구현해보자.</p>

<p>가중치가 \(W\), 손실함수가 \(L\)인 경우, 가중치 매개변수에 대한 기울기는 다음과 같이 구할 수 있다.
각 원소에 대한 편미분을 계산한다.</p>

\[\frac{\partial L}{\partial W}\]

<p>형상이 2x3인 가중치 \(W\), 손실함수 \(L\)인 경우의 기울기를 구해보자.</p>

\[W = \begin{pmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23} \end{pmatrix}\]

\[\frac{\partial L}{\partial W} = \begin{pmatrix} \frac{\partial L}{\partial w_{11}} &amp; \frac{\partial L}{\partial w_{12}} &amp; \frac{\partial L}{\partial w_{13}} \\ \frac{\partial L}{\partial w_{21}} &amp; \frac{\partial L}{\partial w_{22}} &amp; \frac{\partial L}{\partial w_{23}} \end{pmatrix}\]

<p>\(\frac{\partial L}{\partial W}\) 라는 의미는 손실 함수 \(L\)을 가중치 행렬 \(𝑊\)에 대해 편미분한 것으로,\(𝐿\)의 각 원소에 대한 편미분을 정리한 행렬이다.</p>

<p>예를 들어, 1행 1번째 원소인 \(\frac{\partial L}{\partial w_{11}}\). 이는 \(w_{11}\)을 조금 변경했을 때 손실함수 \(L\)이 얼마나 변화하느냐를 나타낸다.</p>

<p>이를 구현해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>  <span class="c1"># Missing import for np
</span>
<span class="c1"># Define softmax function which is used but not defined
</span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Define cross_entropy_error function which is used but not defined
</span><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
    
    <span class="c1"># If t is one-hot encoded
</span>    <span class="k">if</span> <span class="n">t</span><span class="p">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># If t is label encoded
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">simpleNet</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 정규분포로 초기화
</span>        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="nf">simpleNet</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
<span class="c1"># [[ 0.47355232 -1.6420551  -0.4380743 ]
#  [-1.1186056  -0.51709446 -0.99752602]]
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="c1"># [-1.06852808 -1.57996397 -1.19312484] 
</span>
<span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># 0
</span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># 1.413822588029725
</span></code></pre></div></div>

<p>이제 손실함수를 구하는 함수를 구현했으니, 이를 이용해 기울기를 구해보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">tmp_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        
        <span class="c1"># f(x+h) 계산
</span>        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">+</span> <span class="n">h</span>
        <span class="n">fxh1</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># f(x-h) 계산
</span>        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">-</span> <span class="n">h</span>
        <span class="n">fxh2</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh1</span> <span class="o">-</span> <span class="n">fxh2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span>
        
    <span class="k">return</span> <span class="n">grad</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">net</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="n">dW</span> <span class="o">=</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">net</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dW</span><span class="p">)</span>
<span class="c1"># [[ 0.21603469  0.14352979 -0.35956448]
#  [ 0.32405204  0.21529468 -0.53934672]]
</span></code></pre></div></div>

<p>신경망의 기울기를 구한 다음 경사하강법을 이용해 가중치 매개변수를 갱신한다.</p>

<h3 id="implementing-learning-algorithms">Implementing Learning Algorithms</h3>

<p><strong>전제</strong></p>
<ul>
  <li>신경망은 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 ‘학습’이라 한다.</li>
</ul>

<p><strong>1단계 - 미니배치</strong></p>
<ul>
  <li>훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.</li>
</ul>

<p><strong>2단계 - 기울기 산출</strong></p>
<ul>
  <li>미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.</li>
</ul>

<p><strong>3단계 - 매개변수 갱신</strong></p>
<ul>
  <li>가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.</li>
</ul>

<p><strong>4단계 - 반복</strong></p>
<ul>
  <li>1~3단계를 반복한다.</li>
</ul>

<p>이것이 신경망 학습이 이뤄지는 순서이다. 이때 데이터를 미니배치로 무작위로 선정하기 때문에 이를 <strong>확률적 경사 하강법(Stochastic Gradient Descent, SGD)</strong>라고 한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Prevent overflow
</span>    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">weight_init_std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="c1"># Initialize weights and biases
</span>        <span class="n">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">]</span>
        
        <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
        <span class="n">y</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">_cross_entropy_error</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">y</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
        
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Add small value to prevent log(0)
</span>        <span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-7</span>
        
        <span class="c1"># Handle both one-hot and label encoded targets
</span>        <span class="k">if</span> <span class="n">t</span><span class="p">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">:</span>  <span class="c1"># one-hot
</span>            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># label encoded
</span>            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_numerical_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>  <span class="c1"># Small value for numerical differentiation
</span>        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Vectorized operations where possible
</span>        <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">multi_index</span><span class="sh">'</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[[</span><span class="sh">'</span><span class="s">readwrite</span><span class="sh">'</span><span class="p">]])</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="p">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">it</span><span class="p">.</span><span class="n">multi_index</span>
            <span class="n">orig_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            
            <span class="c1"># Calculate f(x+h)
</span>            <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">orig_val</span> <span class="o">+</span> <span class="n">h</span>
            <span class="n">fxh1</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="c1"># Calculate f(x-h)
</span>            <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">orig_val</span> <span class="o">-</span> <span class="n">h</span>
            <span class="n">fxh2</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="c1"># Gradient at this point
</span>            <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh1</span> <span class="o">-</span> <span class="n">fxh2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
            
            <span class="c1"># Restore original value
</span>            <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">orig_val</span>
            <span class="n">it</span><span class="p">.</span><span class="nf">iternext</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">grad</span>

    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">t</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">t</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="nf">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">loss_W</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">grads</span>
</code></pre></div></div>

<p>지금까지 배운 내용을 이용해 신경망을 학습시켜보자.</p>

<hr />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>  <span class="c1"># tqdm 임포트
</span>
<span class="c1"># MNIST 데이터 불러오기
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">훈련 데이터 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 28, 28)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">훈련 라벨 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">t_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000,)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">테스트 데이터 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (10000, 28, 28)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">테스트 라벨 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">t_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (10000,)
</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>  <span class="c1"># (60000, 784)
</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>    <span class="c1"># (10000, 784)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Flatten 후 훈련 데이터 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 784)
</span>
<span class="n">t_train</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">t_train</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">t_test</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">t_test</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">One-hot 변환 후 레이블 크기:</span><span class="sh">"</span><span class="p">,</span> <span class="n">t_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 10)
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">iters_num</span> <span class="o">=</span> <span class="mf">1e4</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">network</span> <span class="o">=</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">train_loss_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_acc_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">iter_per_epoch</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">train_size</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">iters_num</span><span class="p">)),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">mininterval</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>  <span class="c1"># tqdm 설정
</span>    <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    <span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    
    <span class="n">grad</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">network</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
    
    <span class="n">loss</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    <span class="n">train_loss_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">iter_per_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">train_acc</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>
        <span class="n">train_acc_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
        <span class="n">test_acc_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">train acc, test acc | </span><span class="si">{</span><span class="n">train_acc</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">test_acc</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<hr />

<p>위와 같이 클래스로 구현하여 학습을 시키면 학습시간이 대략 9시간 정도 걸린다. 케라스 모델을 사용하면 더 빠르게 학습시킬 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">wandb</span>
<span class="kn">from</span> <span class="n">wandb.integration.keras</span> <span class="kn">import</span> <span class="n">WandbMetricsLogger</span><span class="p">,</span> <span class="n">WandbModelCheckpoint</span>
<span class="kn">import</span> <span class="n">keras</span>

<span class="c1"># Initialize Wandb
</span><span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">workshop_1</span><span class="sh">'</span><span class="p">,</span> <span class="n">project</span><span class="o">=</span><span class="sh">"</span><span class="s">mnist_project-sgd</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 데이터 생성
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># 모델 생성
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">softmax</span><span class="sh">"</span><span class="p">),</span>
<span class="p">])</span>

<span class="c1"># 컴파일 및 학습
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">"</span><span class="s">sgd</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">sparse_categorical_crossentropy</span><span class="sh">"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># Define the checkpoint callback
</span><span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="nc">WandbModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="sh">"</span><span class="s">models/mnins_sgd.keras</span><span class="sh">"</span><span class="p">,</span> <span class="n">save_freq</span><span class="o">=</span><span class="sh">'</span><span class="s">epoch</span><span class="sh">'</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
        <span class="nc">WandbMetricsLogger</span><span class="p">(),</span>
        <span class="n">checkpoint_callback</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>
</code></pre></div></div>

<p>위와 같이 케라스 모델을 사용하면 더 빠르게 학습시킬 수 있다.
손실함수 그래프와 정확도 그래프를 확인해보자. 128번의 에폭을 돌린 결과이다.</p>

<div align="center">
    <img src="/images/mnist_sgd (2).png" alt="loss_accuracy" width="600" />
    <img src="/images/mnist_sgd (3).png" alt="loss_accuracy" width="600" />
</div>
<p><br />
에폭이 증가할수록 손실함수는 감소하고 정확도는 증가하는 것을 확인할 수 있다.</p>

<hr />

<h3 id="summary">Summary</h3>

<ul>
  <li>손실함수는 신경망 학습에서 사용하는 지표이다. 이 손실함수를 최소화하는 것이 신경망 학습의 목표이다.</li>
  <li>평균 제곱 오차와 교차 엔트로피 오차를 사용한다.</li>
  <li>미니배치 학습을 사용하면 훈련 데이터의 일부를 사용해 학습을 수행할 수 있다.</li>
  <li>수치 미분을 사용해 가중치 매개변수의 기울기를 구할 수 있다.</li>
  <li>경사하강법을 사용해 가중치 매개변수를 갱신할 수 있다.</li>
  <li>신경망 학습은 미니배치로 데이터를 무작위로 선정하고, 기울기를 구해 가중치 매개변수를 갱신하는 과정을 반복한다.</li>
  <li>이를 확률적 경사 하강법(SGD)이라고 한다.</li>
</ul>

<hr />

<h1 id="backpropagation">Backpropagation</h1>

<h2 id="computational-graph">Computational Graph</h2>

<ul>
  <li>계산 그래프는 계산 과정을 그래프로 나타낸 것이다.</li>
  <li>계산 그래프의 노드는 연산을, 에지는 데이터를 나타낸다.</li>
  <li>계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.</li>
</ul>

<h3 id="solving-with-a-computational-graph">Solving with a Computational Graph</h3>

<p>간단한 문제를 계산 그래프로 풀어보자.</p>

<p>현빈군은 슈퍼에서 사과를 2개 샀습니다. 사과 한 개는 100원이고, 소비세가 10% 부과됩니다. 이때 현빈군이 지불하는 금액을 구해보자.</p>

<div style="text-align: center;">
    <div class="mermaid">
        graph LR
        A[apple] --&gt; |100| B[x2]
        B --&gt;|200| C[x1.1]
        C --&gt;|220| D[mission complete]
    </div>
</div>

<p>원안에 있는 노드 <strong>x</strong> 만을 <strong>multiply</strong> 연산으로 생각할 수 있다.</p>

<div style="text-align: center;">
    <div class="mermaid">
        graph LR
        A[apple] --&gt; |100| B[x]
        B --&gt;|200| C[x]
        F[apple count] --&gt; |2| B
        E[tax] --&gt; |1.1| C
        C --&gt;|220| D[mission complete]
    </div>
</div>

<h3 id="local-computation">Local Computation</h3>

<p>계산그래프의 특징은 <strong>국소적 계산</strong>을 전파함으로써 최종 결과를 얻는다는 것이다. 이를 <strong>순전파</strong>라고 한다.</p>

<h3 id="why-use-a-computational-graph">Why Use a Computational Graph?</h3>

<p>그렇다면, 계산그래프의 이점은 무엇일까? 계산그래프의 이점은 <strong>국소적 계산</strong>을 통해 최종 결과를 얻을 수 있다는 것이다. 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화할 수 있다. 또한 역전파를 통해 각 노드의 미분을 효율적으로 구할 수 있다.</p>

<p>역전파는 순전파와는 반대로 노드의 미분을 효율적으로 구하는 방법이다. 이를 통해 각 노드의 미분을 효율적으로 구할 수 있다.</p>

<div style="text-align: center;">
    <div class="mermaid">
    graph LR
        A[apple] --&gt;|100| B[x]
        B --&gt;|200| C[x]
        C --&gt;|220| D[mission complete]
        D --&gt;|1| C
        C --&gt;|1.1| B
        B --&gt;|2.2| A
    </div>
</div>

<p>사과 1원이 오른다면 최종금액은 2.2원이 오른다는 것을 알 수 있다.</p>

<h2 id="chain-rule">Chain Rule</h2>

<p>계산 그래프의 순전파의 방향은 왼쪽에서 오른쪽으로, 역전파의 방향은 오른쪽에서 왼쪽으로 진행된다. 이 ‘국소적미분’을 전달하는 원리는 연쇄법칙에 따른다.</p>

\[y = f(x)\]

<div style="text-align: center;">
    <div class="mermaid">
    graph LR
        A[.] --&gt;|x| B[f]
        B --&gt;|y| C[.]
        C --&gt;|E| B
        B --&gt;|E ∂y/∂x| A
        
        style A opacity:0
        style C opacity:0
    </div>
</div>

<p>위의 그림과 같이 <strong>역전파</strong>는 <strong>국소적 미분</strong>을 곱하여 전달한다. 신호 (E)에 노드의 국소적 미분 (E ∂y/∂x)을 곱한 후 다음 노드로 전달한다.</p>

\[y = f(x) = x^2\]

<p>이라면 미분은 다음과 같다.</p>

\[\frac{\partial y}{\partial x} = 2x\]

<p>상류에서 계산된 값에 국소적 미분을 곱하여 하류로 전달한다.</p>

<h3 id="what-is-the-chain-rule">What is the Chain Rule?</h3>

<p>연쇄법칙을 설명하기 전에 합성함수에 대해 알아보자. 합성함수란 여러함수로 구성된 함수이다.</p>

\[z = t^2\]

\[t = x + y\]

<p><strong>합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</strong></p>

<p>x에 대한 z의 미분</p>

\[\frac{\partial z}{\partial x} = 2t\]

<p>t에 대한 z의 미분</p>

\[\frac{\partial z}{\partial t} = 1\]

<p>x에 대한 t의 미분</p>

\[\frac{\partial t}{\partial x} = 1\]

<p>따라서 x에 대한 z의 미분은 다음과 같다.</p>

<div align="center">
    <img src="/images/propagate2.png" alt="chain_rule" width="400" />
</div>

\[\frac{\partial z}{\partial x} = \frac{\partial z}{\partial t} \frac{\partial t}{\partial x}\]

<div align="center">
    <img src="/images/propagate1.png" alt="chain_rule_example" width="400" />
</div>

\[\frac{\partial z}{\partial x} = \frac{\partial z}{\partial t} \frac{\partial t}{\partial x} = 2t * 1 = 2t = 2(x + y)\]

<h2 id="backpropagation-1">Backpropagation</h2>
<hr />
<h3 id="backpropagation-of-addition-nodes">Backpropagation of Addition Nodes</h3>

<p>덧셈노드의 역전파는 1을 곱하기만 할 뿐이다.
즉 덧셈노드의 역전파는 입력된 값을 그대로 다음 노드로 전달한다.</p>

<h3 id="backpropagation-of-multiplication-nodes">Backpropagation of Multiplication Nodes</h3>
<p>곱셈노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 ‘서로 바꾼 값’을 곱해서 하류로 전달한다.</p>

\[z = xy\]

<p>x에 대한 z의 미분</p>

\[\frac{\partial z}{\partial x} = y\]

<p>y에 대한 z의 미분</p>

\[\frac{\partial z}{\partial y} = x\]

<div align="center">
    <img src="/images/propagate3.png" alt="multiply_node" width="400" />
</div>

<h2 id="implementing-simple-layers">Implementing Simple Layers</h2>
<h3 id="multiplication-layer">Multiplication Layer</h3>
<h3 id="addition-layer">Addition Layer</h3>

<h2 id="implementing-activation-function-layers">Implementing Activation Function Layers</h2>
<h3 id="relu-layer">ReLU Layer</h3>
<h3 id="sigmoid-layer">Sigmoid Layer</h3>

<h2 id="implementing-affinesoftmax-layers">Implementing Affine/Softmax Layers</h2>
<h3 id="affine-layer">Affine Layer</h3>
<h3 id="batch-affine-layer">Batch Affine Layer</h3>
<h3 id="softmax-with-loss-layer">Softmax-with-Loss Layer</h3>

<h2 id="implementing-backpropagation">Implementing Backpropagation</h2>
<h3 id="overview-of-neural-network-learning">Overview of Neural Network Learning</h3>
<h3 id="implementing-a-neural-network-with-backpropagation">Implementing a Neural Network with Backpropagation</h3>
<h3 id="verifying-gradients-with-backpropagation">Verifying Gradients with Backpropagation</h3>
<h3 id="implementing-training-with-backpropagation">Implementing Training with Backpropagation</h3>

<h2 id="summary-1">Summary</h2>


						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
			<div class="section section--grey">
				<div class="container">
					<div class="row">
						<div class="col-md-7">
							<div class="comments-area">
  <h3 class="comments-title">Comments</h3>
  <div id="comments-list" class="comments-list">
    <!-- Comments will be populated here -->
  </div>
  
  <div class="comment-respond">
    <h4 class="comment-reply-title">Leave a Comment</h4>
    <form id="comment-form" class="comment-form">
      <div class="form-group">
        <label for="name">Name</label>
        <input type="text" id="name" name="name" required class="form-control">
      </div>
      <div class="form-group">
        <label for="email">Email</label>
        <input type="email" id="email" name="email" required class="form-control">
      </div>
      <div class="form-group">
        <label for="comment">Comment</label>
        <textarea id="comment" name="comment" required class="form-control" rows="5"></textarea>
      </div>
      <div class="form-group admin-field">
        <label for="password">Admin Password</label>
        <input type="password" id="password" name="password" class="form-control" placeholder="For comment moderation">
      </div>
      <div class="form-submit">
        <button type="submit" class="btn btn--dark btn--rounded">Submit Comment</button>
      </div>
    </form>
  </div>
</div>

<script>
  (function() {
    // Simple storage for comments using localStorage
    const COMMENTS_STORAGE_KEY = 'page_comments_/deep_learning/';
    
    // Load comments from storage
    function loadComments() {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      const commentsList = document.getElementById('comments-list');
      commentsList.innerHTML = '';
      
      if (comments.length === 0) {
        commentsList.innerHTML = '<div class="no-comments">No comments yet. Be the first to comment!</div>';
        return;
      }
      
      comments.forEach((comment, index) => {
        const commentDiv = document.createElement('div');
        commentDiv.className = 'comment';
        commentDiv.dataset.id = index;
        
        const commentHTML = `
          <div class="comment-meta">
            <div class="comment-author">
              <strong>${comment.name}</strong>
            </div>
            <div class="comment-metadata">
              <span>${new Date(comment.date).toLocaleDateString()} ${new Date(comment.date).toLocaleTimeString()}</span>
            </div>
          </div>
          <div class="comment-content">
            <p>${comment.text}</p>
          </div>
          <div class="comment-actions">
            <button class="btn-link reply-btn" data-id="${index}">
              <i class="icon icon--arrow-right"></i> Reply
            </button>
            <button class="btn-link delete-btn" data-id="${index}">
              <i class="icon icon--cross"></i> Delete
            </button>
          </div>
          <div class="reply-form-wrapper" id="reply-form-${index}" style="display: none;">
            <form class="reply-form" data-parent="${index}">
              <div class="form-group">
                <label for="reply-name-${index}">Name</label>
                <input type="text" id="reply-name-${index}" name="name" required class="form-control">
              </div>
              <div class="form-group">
                <label for="reply-comment-${index}">Reply</label>
                <textarea id="reply-comment-${index}" name="comment" required class="form-control" rows="3"></textarea>
              </div>
              <div class="form-submit">
                <button type="submit" class="btn btn--dark btn--rounded btn--sm">Submit Reply</button>
              </div>
            </form>
          </div>
          <div class="children" id="replies-${index}">
            ${renderReplies(comment.replies || [])}
          </div>
        `;
        
        commentDiv.innerHTML = commentHTML;
        commentsList.appendChild(commentDiv);
      });
      
      // Add event listeners to reply buttons
      document.querySelectorAll('.reply-btn').forEach(button => {
        button.addEventListener('click', function() {
          const commentId = this.dataset.id;
          const replyForm = document.getElementById(`reply-form-${commentId}`);
          replyForm.style.display = replyForm.style.display === 'none' ? 'block' : 'none';
        });
      });
      
      // Add event listeners to delete buttons
      document.querySelectorAll('.delete-btn').forEach(button => {
        button.addEventListener('click', function() {
          const commentId = this.dataset.id;
          const password = prompt('Enter admin password to delete:');
          
          if (password === 'admin123') { // Simple password for demo
            deleteComment(parseInt(commentId));
          } else {
            alert('Incorrect password');
          }
        });
      });
      
      // Add event listeners to reply forms
      document.querySelectorAll('.reply-form').forEach(form => {
        form.addEventListener('submit', function(e) {
          e.preventDefault();
          const parentId = parseInt(this.dataset.parent);
          const replyName = this.querySelector('[name="name"]').value;
          const replyText = this.querySelector('[name="comment"]').value;
          
          addReply(parentId, replyName, replyText);
          this.reset();
          document.getElementById(`reply-form-${parentId}`).style.display = 'none';
        });
      });
    }
    
    // Render replies
    function renderReplies(replies) {
      if (!replies || replies.length === 0) return '';
      
      let html = '';
      replies.forEach((reply, replyIndex) => {
        html += `
          <div class="comment child-comment" data-id="${replyIndex}">
            <div class="comment-meta">
              <div class="comment-author">
                <strong>${reply.name}</strong>
              </div>
              <div class="comment-metadata">
                <span>${new Date(reply.date).toLocaleDateString()} ${new Date(reply.date).toLocaleTimeString()}</span>
              </div>
            </div>
            <div class="comment-content">
              <p>${reply.text}</p>
            </div>
            <div class="comment-actions">
              <button class="btn-link delete-reply-btn" data-parent="${replyIndex}">
                <i class="icon icon--cross"></i> Delete
              </button>
            </div>
          </div>
        `;
      });
      return html;
    }
    
    // Add a new comment
    function addComment(name, email, text) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      comments.push({
        name: name,
        email: email,
        text: text,
        date: new Date().toISOString(),
        replies: []
      });
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Add a reply to a comment
    function addReply(parentId, name, text) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      if (!comments[parentId].replies) {
        comments[parentId].replies = [];
      }
      
      comments[parentId].replies.push({
        name: name,
        text: text,
        date: new Date().toISOString()
      });
      
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Delete a comment
    function deleteComment(commentId) {
      const comments = JSON.parse(localStorage.getItem(COMMENTS_STORAGE_KEY) || '[]');
      comments.splice(commentId, 1);
      localStorage.setItem(COMMENTS_STORAGE_KEY, JSON.stringify(comments));
      loadComments();
    }
    
    // Event listener for comment form
    document.getElementById('comment-form').addEventListener('submit', function(e) {
      e.preventDefault();
      
      const name = document.getElementById('name').value;
      const email = document.getElementById('email').value;
      const comment = document.getElementById('comment').value;
      
      addComment(name, email, comment);
      this.reset();
    });
    
    // Initial load
    document.addEventListener('DOMContentLoaded', loadComments);
  })();
</script>

<style>
  /* Comments styling that matches Doks theme */
  .comments-area {
    margin-top: 2.5rem;
    font-family: 'Noto Sans', sans-serif;
  }
  
  .comments-title {
    margin-bottom: 1.5rem;
    font-size: 1.75em;
    font-weight: 600;
    color: #333;
  }
  
  .comment {
    margin-bottom: 1.5rem;
    padding: 1.25rem;
    background-color: #fff;
    border-radius: 4px;
    box-shadow: 0 1px 4px rgba(0,0,0,.04);
    border-left: 4px solid #253951;
    transition: all .2s ease;
  }
  
  .comment:hover {
    box-shadow: 0 1px 10px rgba(0,0,0,.08);
  }
  
  .comment-meta {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 0.75rem;
  }
  
  .comment-author {
    font-weight: 600;
  }
  
  .comment-metadata {
    font-size: 0.75em;
    color: #8a8a8a;
  }
  
  .comment-content {
    line-height: 1.6;
    color: #333;
  }
  
  .comment-content p {
    margin-bottom: 0.5rem;
  }
  
  .comment-actions {
    margin-top: 0.75rem;
    padding-top: 0.5rem;
    border-top: 1px solid #f5f5f5;
  }
  
  .btn-link {
    background: none;
    border: none;
    padding: 0;
    color: #253951;
    cursor: pointer;
    font-size: 0.85em;
    text-decoration: none;
    margin-right: 1rem;
    transition: color .2s ease;
  }
  
  .btn-link:hover {
    color: #0056b3;
    text-decoration: underline;
  }
  
  .reply-form-wrapper {
    margin-top: 1rem;
    margin-bottom: 1rem;
    padding: 1rem;
    background-color: #f9f9f9;
    border-radius: 4px;
  }
  
  .children {
    margin-top: 1rem;
    margin-left: 1.5rem;
    padding-left: 1rem;
    border-left: 1px solid #eaeaea;
  }
  
  .child-comment {
    margin-bottom: 1rem;
    border-left: 3px solid #6c757d;
  }
  
  .no-comments {
    padding: 1rem;
    background-color: #f5f5f5;
    border-radius: 4px;
    font-style: italic;
    color: #666;
  }
  
  /* Form styling */
  .comment-respond {
    margin-top: 2rem;
    padding: 1.5rem;
    background-color: #f9f9f9;
    border-radius: 4px;
  }
  
  .comment-reply-title {
    margin-bottom: 1.25rem;
    font-size: 1.25em;
    font-weight: 600;
    color: #333;
  }
  
  .comment-form .form-group {
    margin-bottom: 1rem;
  }
  
  .comment-form label {
    display: block;
    margin-bottom: 0.5rem;
    font-weight: 600;
    color: #333;
  }
  
  .comment-form .form-control {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid #e0e0e0;
    border-radius: 4px;
    background-color: #fff;
    font-family: inherit;
    font-size: 0.95em;
    transition: border-color .2s ease;
  }
  
  .comment-form .form-control:focus {
    border-color: #253951;
    outline: none;
  }
  
  .admin-field {
    opacity: 0.8;
  }
  
  .form-submit {
    margin-top: 1.5rem;
  }
  
  /* Match the Doks buttons */
  .btn {
    display: inline-block;
    font-weight: 500;
    text-align: center;
    white-space: nowrap;
    vertical-align: middle;
    user-select: none;
    padding: 0.75rem 1.25rem;
    font-size: 1rem;
    line-height: 1.5;
    border-radius: 4px;
    transition: all .2s ease-in-out;
    text-decoration: none;
    cursor: pointer;
  }
  
  .btn--dark {
    background-color: #253951;
    border-color: #253951;
    color: #fff;
  }
  
  .btn--dark:hover {
    background-color: #1a2a3c;
    border-color: #1a2a3c;
  }
  
  .btn--rounded {
    border-radius: 100px;
  }
  
  .btn--sm {
    padding: 0.5rem 1rem;
    font-size: 0.875rem;
  }
</style>

						</div><!-- /.col -->
					</div><!-- /.row -->
				</div><!-- /.container -->
			</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
				<div class="micro-nav">
	<div class="container">
		<div class="row">
			<div class="col-xs-12">
				<a href="/" class="micro-nav__back">
					<i class="icon icon--arrow-left"></i>
					Back to homepage
				</a><!-- /.micro-nav__back -->
			</div><!-- /.col -->
		</div><!-- /.row -->
	</div><!-- /.container -->
</div><!-- /.micro-nav -->

			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
						<a href="/" class="site-footer__logo">InchanBaek Note</a>
					
					
						<hr>
						<p class="site-footer__copyright">Copyright &copy; 2017. - InchanBaek Note <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
					<div class="col-sm-6 align-right">
						<ul class="social-list">
							
								<li>
									<a href="https://www.linkedin.com/in/inchan-baek-728197265/" target="_blank" class="social-list__item social-list__item--linkedin">
										<i class="icon icon--linkedin"></i>
									</a>
								</li>
							
								<li>
									<a href="https://www.instagram.com/in_chanchan/" target="_blank" class="social-list__item social-list__item--instagram">
										<i class="icon icon--instagram"></i>
									</a>
								</li>
							
								<li>
									<a href="https://www.youtube.com/@icb6048" target="_blank" class="social-list__item social-list__item--youtube">
										<i class="icon icon--youtube"></i>
									</a>
								</li>
							
						</ul><!-- /.social-list -->
					</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>





		</div><!-- /.js-footer-area -->
	</body>
</html>