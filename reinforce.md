---
# Page settings
layout: default
keywords: ê°•í™”í•™ìŠµ, ë¨¸ì‹ ëŸ¬ë‹, ì¸ê³µì§€ëŠ¥, ì¸ì°¬ë°±, InchanBaek, ë¦¬ì›Œë“œ, ì—ì´ì „íŠ¸, ì•¡ì…˜, MDP, ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •, Q-ëŸ¬ë‹, reinforcement learning, machine learning, AI, reward, agent, action, Markov decision process, Q-learning, deep reinforcement learning
comments: true
seo:
  title: Reinforcement Learning from Scratch - Complete Guide | InchanBaek Note
  description: ê°•í™”í•™ìŠµì˜ ê¸°ì´ˆë¶€í„° ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ê¹Œì§€ ë°°ìš°ëŠ” ì™„ë²½ ê°€ì´ë“œ. ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •, Q-ëŸ¬ë‹, ì •ì±… ê²½ì‚¬ë²• ë“± í•µì‹¬ ê°œë…ê³¼ ì‹¤ì œ êµ¬í˜„ ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
  canonical: https://bic98.github.io/reinforce/
  image: https://bic98.github.io/images/layout/logo.png

# Hero section
title: Reinforcement Learning from Scratch
description: ê°•í™”í•™ìŠµì˜ ê¸°ë³¸ ê°œë…ë¶€í„° ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ê¹Œì§€, ì´ë¡ ê³¼ ì‹¤ìŠµì„ í†µí•´ ì²˜ìŒë¶€í„° ì°¨ê·¼ì°¨ê·¼ ë°°ìš°ëŠ” ê°•í™”í•™ìŠµ ê¸°ì´ˆ ê°€ì´ë“œì…ë‹ˆë‹¤.

# # Author box
# author:
#     title: About Author
#     title_url: '#'
#     external_url: true
#     description: Author description

# Micro navigation
micro_nav: true

# Page navigation
# page_nav:
#     prev:
#         content: Previous page
#         url: '/deep_learning/'
#     next:
#         content: Next page
#         url: '/qgis/'


# Language setting
---


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## ë°´ë””íŠ¸ ë¬¸ì œ (Bandit Problem)

### ê°•í™”í•™ìŠµ

- ì§€ë„í•™ìŠµ (Supervised Learning) : ì…ë ¥ê³¼ ì¶œë ¥ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ ë°ì´í„° ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê¸°ê³„í•™ìŠµì˜ í•œ ë°©ë²•

- ë¹„ì§€ë„í•™ìŠµ (Unsupervised Learning) : ì…ë ¥ ë°ì´í„°ë§Œ ì£¼ì–´ì¡Œì„ ë•Œ, ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ì°¾ì•„ë‚´ëŠ” ê¸°ê³„í•™ìŠµì˜ í•œ ë°©ë²•

- ê°•í™”í•™ìŠµ (Reinforcement Learning) : ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©°, í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°›ì•„ ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” ë°©ë²•

### ë°´ë””íŠ¸ ë¬¸ì œë€?

ë°´ë””íŠ¸ == ìŠ¬ë¡¯ë¨¸ì‹ 

ìŠ¬ë¡¯ë¨¸ì‹ ì€ ê°ê°ì˜ í™•ë¥ ì´ ë‹¤ë¦„. 

ì²˜ìŒì—ëŠ” ì–´ë–¤ ìŠ¬ë¡¯ë¨¸ì‹ ì´ ê°€ì¥ ì¢‹ì€ì§€ ëª¨ë¦„. 

ì‹¤ì œë¡œ í”Œë ˆì´ë¥¼ í•´ë³´ë©´ì„œ ì¢‹ì€ ë¨¸ì‹ ì„ ì°¾ì•„ì•¼ í•¨. 

ì •í•´ì§„ íšŸìˆ˜ ì•ˆì— ìµœëŒ€í•œ ë§ì´ ë³´ìƒì„ ì–»ëŠ” ê²ƒì´ ëª©í‘œ.

<div align="center">
  <div class = 'mermaid'>
    graph LR
    A[Agent] -->|í–‰ë™| B[Environment]
    B -->|ë³´ìƒ| A
  </div>
</div>


**í”Œë ˆì´ì–´ì¸ ì—ì´ì „íŠ¸ëŠ” ì£¼ì–´ì§„ í™˜ê²½ì—ì„œ í–‰ë™ì„ ì„ íƒí•˜ê³ , í™˜ê²½ì€ ì—ì´ì „íŠ¸ì—ê²Œ ë³´ìƒì„ ì œê³µí•œë‹¤.**

ëª©í‘œ : **ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê²ƒ** -> **ì½”ì¸ì„ ìµœëŒ€í•œ ë§ì´ ì–»ëŠ” ê²ƒ** -> **ì¢‹ì€ ìŠ¬ë¡¯ë¨¸ì‹ ì„ ì°¾ëŠ” ê²ƒ**

### ê°€ì¹˜ì™€ í–‰ë™ê°€ì¹˜

- ê°€ì¹˜ (Value) : íŠ¹ì • ìƒíƒœì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ë³´ìƒì˜ ê¸°ëŒ€ê°’

$$
E[R_t] 
$$

- í–‰ë™ê°€ì¹˜ (Action Value) :í–‰ë™ì˜ ê²°ê³¼ë¡œ ì–»ì€ ë³´ìƒì˜ ê¸°ëŒ€ê°’

$$
Q(A) = E[R_t | A] 
$$

(E = Expectation, Q = Quality, A = Action, R = Reward)

ìŠ¬ë¡¯ ë¨¸ì‹  aì™€ bì˜ ë³´ìƒì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•´ë³´ì. 

ë°‘ì—ëŠ” ìŠ¬ë¡¯ë¨¸ì‹  aì— ëŒ€í•œ í‘œì´ë‹¤. 

| ìŠ¬ë¡¯ë¨¸ì‹  a | 
|:---:|:---:|:---:|:---:|:---:|:---:|
| ì–»ì„ ìˆ˜ ìˆëŠ” ì½”ì¸| 0 | 1 | 5 | 10 |
| ë³´ìƒ | 0.70 | 0.15 | 0.12 | 0.03 |


ìŠ¬ë¡¯ ë¨¸ì‹  bì— ëŒ€í•œ í‘œì´ë‹¤.

| ìŠ¬ë¡¯ë¨¸ì‹  b | 
|:---:|:---:|:---:|:---:|:---:|:---:|
| ì–»ì„ ìˆ˜ ìˆëŠ” ì½”ì¸| 0 | 1 | 5 | 10 |
| ë³´ìƒ | 0.50 | 0.40 | 0.09 | 0.01 |

ë‘ ë¨¸ì‹ ì— ëŒ€í•œ ê¸°ëŒ“ê°’ì€

- ìŠ¬ë¡¯ë¨¸ì‹  a : (0.7 * 0 + 0.15 * 1 + 0.12 * 5 + 0.03 * 10) = 1.05
- ìŠ¬ë¡¯ë¨¸ì‹  b : (0.5 * 0 + 0.4 * 1 + 0.09 * 5 + 0.01 * 10) = 0.95

ìŠ¬ë¡¯ë¨¸ì‹  aê°€ ìŠ¬ë¡¯ë¨¸ì‹  bë³´ë‹¤ ì¢‹ë‹¤.

### ê°€ì¹˜ ì¶”ì •

në²ˆì˜ í”Œë ˆì´ë¥¼ í•˜ë©´ì„œ ì–»ì€ ë³´ìƒì„ R1, R2, ..., Rnì´ë¼ê³  í•˜ì.
ê·¸ë•Œ í–‰ë™ ê°€ì¹˜ì˜ ì¶”ì •ì¹˜ Qnì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$
Q_n = \frac{R_1 + R_2 + ... + R_n}{n}
$$

í•˜ì§€ë§Œ ë§Œì•½ ì´ë ‡ê²Œ në²ˆ í”Œë ˆì´ í•˜ë©´ì„œ ê°€ì¹˜ ì¶”ì •ì„ í•œë‹¤ë©´ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ë¶€í•˜ê°€ ì»¤ì§„ë‹¤.
n - 1ë²ˆì§¸ì˜ ê°€ì¹˜ ì¶”ì •ì¹˜ë¥¼ ì´ìš©í•´ì„œ në²ˆì§¸ ê°€ì¹˜ ì¶”ì •ì¹˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$
Q_{n-1} = \frac{R_1 + R_2 + ... + R_{n-1}}{n-1}
$$

ì´ì‹ì˜ ì–‘ë³€ì— (n - 1)ì„ ê³±í•˜ë©´

$$
(n - 1)Q_{n-1} = R_1 + R_2 + ... + R_{n-1}
$$

ì´ì œ në²ˆì§¸ ê°€ì¹˜ ì¶”ì •ì¹˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$
Q_n = \frac{1}{n} (R_1 + R_2 + ... + R_{n-1} + R_n) 
$$

$$
=\frac{1}{n} (n - 1)Q_{n-1} + \frac{1}{n} R_n
$$


$$
= Q_{n - 1} + \frac{1}{n} (R_n - Q_{n - 1})
$$

### í”Œë ˆì´ì–´ì˜ ì •ì±…

í™•ì‹¤í•˜ì§€ ì•Šì€ ì¶”ì •ì¹˜ë¥¼ ì „ì ìœ¼ë¡œ ì‹ ë¢°í•˜ë©´ ìµœì„ ì˜ í–‰ë™ì„ ë†“ì¹  ìˆ˜ ìˆìŒ. ë”°ë¼ì„œ ì—ì´ì „íŠ¸ëŠ” ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì—¬ ì¶”ì •ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì—¬ì•¼ í•œë‹¤. 

- ì •ì±… (Policy) : ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ, ì—ì´ì „íŠ¸ê°€ ì„ íƒí•˜ëŠ” í–‰ë™ì„ ê²°ì •í•˜ëŠ” ì „ëµ

ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ì„œëŠ” ë‘ê°€ì§€ ì •ì±…ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

1. **íƒí—˜ (Exploration)** : ë¶ˆí™•ì‹¤í•œ í–‰ë™ì„ ì„ íƒí•˜ì—¬ í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ëŠ” ê²ƒ
2. **í™œìš© (Exploitation)** : í˜„ì¬ê¹Œì§€ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ìµœì„ ì˜ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê²ƒ

ê²°êµ­ ê°•í™”í•™ìŠµì•Œê³ ë¦¬ì¦˜ì€ 'í™œìš©ê³¼ íƒí—˜ì˜ ê· í˜•'ì„ ë§ì¶”ëŠ” ê²ƒ!!!!

### ì—¡ì‹¤ë¡ -ê·¸ë¦¬ë”” ì •ì±…
íƒí—˜ê³¼ í™œìš©ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•œ ë°©ë²•ì˜ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì´ë‹¤. 
ì˜ˆë¥¼ ë“¤ì–´, $$\epsilon$$ = 0.1ë¡œ ì„¤ì •í•˜ë©´ 10%ì˜ í™•ë¥ ë¡œ ë¬´ì‘ìœ„ í–‰ë™ì„ ì„ íƒí•˜ê³ , 90%ì˜ í™•ë¥ ë¡œ ê°€ì¥ ì¢‹ì€ í–‰ë™ì„ ì„ íƒí•œë‹¤.

### ë°´ë””íŠ¸ ë¬¸ì œì˜ í•´ê²°

- **í–‰ë™ê°€ì¹˜ ì¶”ì •** : í–‰ë™ê°€ì¹˜ë¥¼ ì¶”ì •í•˜ê³ , ê°€ì¥ ì¢‹ì€ í–‰ë™ì„ ì„ íƒí•œë‹¤.
- **ì •ì±…** : ì—¡ì‹¤ë¡ -ê·¸ë¦¬ë”” ì •ì±…ì„ ì‚¬ìš©í•˜ì—¬ íƒí—˜ê³¼ í™œìš©ì˜ ê· í˜•ì„ ë§ì¶˜ë‹¤.

ê·¸ëŸ¼ ìœ„ì˜ ë‚´ìš©ì„ ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì.

```python

import numpy as np

class Bandit:
    def __init__(self, arms = 10):
        self.rates = [0.38991635, 0.5937864,  0.55356798, 0.46228943, 0.48251845, 0.47595196, 0.53560295, 0.43374032, 0.55913105, 0.57484477]

    def play(self, arm):
        rate = self.rates[arm]
        if rate > np.random.rand():
            return 1
        else : 
            return 0


class Agent:
    def __init__(self, epslion, action_size = 10):
        self.epslion = epslion
        self.Qs = np.zeros(action_size)
        self.Ns = np.zeros(action_size)

    def update(self, action, reward):
        self.Ns[action] += 1
        self.Qs[action] += (reward - self.Qs[action]) / self.Ns[action]

    def get_action(self):
        if np.random.rand() < self.epslion:
            return (np.random.randint(len(self.Qs)), 0)
        return (np.argmax(self.Qs), 1)

steps = 10000
agent = Agent(0.1)
bandit = Bandit()

total_reward = 0
total_rewards = []
rates = []
actions = []

for i in range(steps):
    act = agent.get_action()
    action = act[0]
    reward = bandit.play(action)
    agent.update(action, reward)
    total_reward += reward
    total_rewards.append(total_reward)
    rates.append(total_reward / (i + 1))
    if act[1] == 1:
        actions.append(action)

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))

plt.subplot(3, 1, 1)
plt.plot(total_rewards, label='Total Reward')
plt.xlabel('Steps')
plt.ylabel('Total Reward')
plt.legend()

plt.subplot(3, 1, 2)
plt.plot(actions, label='Actions')
plt.xlabel('Steps')
plt.ylabel('Action')
plt.legend()

plt.subplot(3, 1, 3)
plt.plot(rates, label='Average Reward')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.legend()

plt.tight_layout()
plt.show()

```

<div align="center">
  <img src="/images/bandit.png" alt="bandit" width="100%">
</div>

ì•½ 10000ë²ˆì˜ í”Œë ˆì´ë¥¼ í•˜ë©´ ì¸ë±ìŠ¤ 1ë²ˆì˜ ìŠ¬ë¡¯ë¨¸ì‹ ì„ ì•¡ì…˜ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì´ ìµœì„ ì„ì„ ì•„ì§ì„ ì•Œì§€ ëª»í•œë‹¤. 
ë” ë§ì€ ìŠ¤í…ì„ ì£¼ì–´ë³´ì. 

<div align="center">
  <img src="/images/bandit2.png" alt="bandit" width="100%">
</div>

ì•½ 30000ë²ˆì˜ í”Œë ˆì´ë¥¼ í•˜ë©´ ì¸ë±ìŠ¤ 1ë²ˆì˜ ìŠ¬ë¡¯ë¨¸ì‹ ì„ ì•¡ì…˜ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì´ ìµœì„ ì„ì„ ì•Œê²Œ ëœë‹¤.
ì•½ 2%ì˜ í™•ë¥  ì°¨ì´ë¥¼ ì¸ì§€í•˜ê¸° ìœ„í•´ 20000ë²ˆì˜ í”Œë ˆì´ê°€ ë” í•„ìš”í–ˆë‹¤. 


### ë¹„ì •ìƒ ë¬¸ì œ (Non-stationary Problem)

ì§€ê¸ˆê¹Œì§€ ë‹¤ë£¬ ë°´ë””íŠ¸ ë¬¸ì œë¥¼ ì •ìƒë¬¸ì œì— ì†í•œë‹¤. ì •ìƒë¬¸ì œë€ ë³´ìƒì˜ í™•ë¥  ë¶„í¬ê°€ ë³€í•˜ì§€ ì•ŠëŠ” ë¬¸ì œì´ë‹¤. ìœ„ì˜ ì½”ë“œë¥¼ ë³´ë©´ ratesë¼ëŠ” ë³€ìˆ˜ì— í™•ë¥ ì´ ê³ ì •ë˜ì–´ ìˆë‹¤.

í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë³´ìƒì˜ í™•ë¥  ë¶„í¬ê°€ ë³€í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì´ëŸ° ê²½ìš°ë¥¼ ë¹„ì •ìƒ ë¬¸ì œë¼ê³  í•œë‹¤. ì´ëŸ° ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?



ë¨¼ì € ì •ìƒ ë¬¸ì œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ í–‰ë™ê°€ì¹˜ ì¶”ì •ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í–ˆë‹¤.

$$
Q_n = Q_{n - 1} + \frac{1}{n} (R_n - Q_{n - 1})
$$

í•˜ì§€ë§Œ ë¹„ì •ìƒ ë¬¸ì œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ í–‰ë™ê°€ì¹˜ ì¶”ì •ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.

$$
Q_n = Q_{n - 1} + \alpha (R_n - Q_{n - 1})
$$

ì˜¤ë˜ì „ì— ì–»ì€ ë³´ìƒì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì´ê³ , ìµœê·¼ì— ì–»ì€ ë³´ìƒì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì´ëŠ” ë°©ë²•ì´ë‹¤. ì´ë•Œ $$\alpha$$ëŠ” í•™ìŠµë¥ ì´ë¼ê³  í•œë‹¤.


<div style="overflow-x: auto;">

$$
= Q_{n - 1} + \alpha (R_n - Q_{n - 1})
$$
$$
= (1 - \alpha) Q_{n - 1} + \alpha R_n
$$
$$
= \alpha R_n + (1 - \alpha) {(\alpha R_{n - 1} + (1 - \alpha) Q_{n - 2})}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 Q_{n - 2}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} + (1 - \alpha)^3 Q_{n - 3}
$$
$$
= \alpha R_n + (1 - \alpha) \alpha R_{n - 1} + (1 - \alpha)^2 \alpha R_{n - 2} +
$$
$$
(1 - \alpha)^3 \alpha R_{n - 3} + ... + (1 - \alpha)^{n - 1} \alpha R_1 + (1 - \alpha)^n Q_0
$$

</div>

$$Q_0$$ëŠ” ì´ˆê¸°ê°’ì´ë‹¤. ìš°ë¦¬ê°€ ì„¤ì •í•œ ê°’ì— ë”°ë¼ì„œ  í•™ìŠµê²°ê³¼ì— í¸í–¥ì´ ìƒê¸´ë‹¤. í•˜ì§€ë§Œ í‘œë³¸ í‰ê· ì„ ì‚¬ìš©í•˜ë©´ í¸í–¥ì´ ì‚¬ë¼ì§„ë‹¤.


ìœ„ì˜ ë°©ì‹ì„ ì§€ìˆ˜ì´ë™í‰ê· , ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· ì´ë¼ê³  í•œë‹¤.


- **ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê·  (Exponential Weighted Moving Average)** : ìµœê·¼ì— ì–»ì€ ë³´ìƒì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê³ , ì˜¤ë˜ì „ì— ì–»ì€ ë³´ìƒì—ëŠ” ì ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ë°©ë²•


python ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì.

```python
import numpy as np

class Bandit:
    def __init__(self, arms = 10):
        self.rates = [0.38991635, 0.7837864,  0.55356798, 0.46228943, 0.48251845, 0.47595196, 0.53560295, 0.43374032, 0.55913105, 0.57484477]

    def play(self, arm):
        rate = self.rates[arm]
        self.rates += 0.1 * np.random.randn(len(self.rates))
        if rate > np.random.rand():
            return 1
        else : 
            return 0

class Agent:
    def __init__(self, epslion, action_size = 10):
        self.epslion = epslion
        self.Qs = np.zeros(action_size)

    def update(self, action, reward, alpha = 0.8):
        self.Qs[action] += alpha * (reward - self.Qs[action])

    def get_action(self):
        if np.random.rand() < self.epslion:
            return (np.random.randint(len(self.Qs)), 0)
        return (np.argmax(self.Qs), 1) 

steps = 50000
agent = Agent(0.1)
bandit = Bandit()

total_reward = 0
total_rewards = []
rates = []
actions = []

for i in range(steps):
    act = agent.get_action()
    action = act[0]
    reward = bandit.play(action)
    agent.update(action, reward)
    total_reward += reward
    total_rewards.append(total_reward)
    rates.append(total_reward / (i + 1))
    if act[1] == 1:
        actions.append(action)

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))

plt.subplot(3, 1, 1)
plt.plot(total_rewards, label='Total Reward')
plt.xlabel('Steps')
plt.ylabel('Total Reward')
plt.legend()

plt.subplot(3, 1, 2)
plt.plot(actions, label='Actions')
plt.xlabel('Steps')
plt.ylabel('Action')
plt.ylim(0, 9)
plt.legend()

plt.subplot(3, 1, 3)
plt.plot(rates, label='Average Reward')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.ylim(0, 1)
plt.legend()

plt.tight_layout()
plt.show()
```

<div align="center">
  <img src="/images/bandit3.png" alt="bandit" width="100%">
</div>

ê³ ì •ê°’ $$\alpha$$ = 0.8ë¡œ ì„¤ì •í•˜ë©´ í‘œë³¸ í‰ê· ì„ ì‚¬ìš©í•œ ê²½ìš°ë³´ë‹¤ ë” ê²°ê³¼ê°€ ë¹¨ë¦¬ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

### ì •ë¦¬

- **ë°´ë””íŠ¸ ë¬¸ì œ** : ê°•í™”í•™ìŠµì˜ ê¸°ì´ˆ ë¬¸ì œë¡œ, ì—¬ëŸ¬ ê°œì˜ ìŠ¬ë¡¯ë¨¸ì‹  ì¤‘ì—ì„œ ìµœëŒ€ ë³´ìƒì„ ì–»ëŠ” ë°©ë²•ì„ ì°¾ëŠ” ë¬¸ì œ
- **í–‰ë™ê°€ì¹˜** : í–‰ë™ì˜ ê²°ê³¼ë¡œ ì–»ì€ ë³´ìƒì˜ ê¸°ëŒ€ê°’
- **ì •ì±…** : ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ, ì—ì´ì „íŠ¸ê°€ ì„ íƒí•˜ëŠ” í–‰ë™ì„ ê²°ì •í•˜ëŠ” ì „ëµ
- **ì—¡ì‹¤ë¡ -ê·¸ë¦¬ë”” ì •ì±…** : íƒí—˜ê³¼ í™œìš©ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•œ ë°©ë²•ì˜ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜
- **ë¹„ì •ìƒ ë¬¸ì œ** : ë³´ìƒì˜ í™•ë¥  ë¶„í¬ê°€ ë³€í•˜ëŠ” ë¬¸ì œ
- **ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· ** : ìµœê·¼ì— ì–»ì€ ë³´ìƒì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê³ , ì˜¤ë˜ì „ì— ì–»ì€ ë³´ìƒì—ëŠ” ì ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ë°©ë²•

## ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì • (Markov Decision Process)

ì—ë¦¬ì „íŠ¸ì˜ í–‰ë™ì— ë”°ë¼ í™˜ê²½ì˜ ìƒíƒœê°€ ë³€í•˜ëŠ” ë¬¸ì œë¥¼ ë‹¤ë£¨ì–´ë³´ì. 

### ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •ì´ë€?

- **ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì • (Markov Decision Process, MDP)** : ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©°, í™˜ê²½ì˜ ìƒíƒœê°€ ë§ˆë¥´ì½”í”„ ì„±ì§ˆì„ ë§Œì¡±í•˜ëŠ” í™˜ê²½ì„ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•

- **ë§ˆë¥´ì½”í”„ ì„±ì§ˆ** : ë¯¸ë˜ì˜ ìƒíƒœê°€ í˜„ì¬ì˜ ìƒíƒœì—ë§Œ ì˜ì¡´í•˜ëŠ” ì„±ì§ˆ

MDPì—ëŠ” ì‹œê°„ ê°œë…ì´ í•„ìš”í•˜ë‹¤. íŠ¹ì • ì‹œê°„ì— ì—ì´ì „íŠ¸ê°€ í–‰ë™ì„ ì·¨í•˜ê³ , ê·¸ ê²°ê³¼ ìƒˆë¡œìš° ìƒíƒœë¡œ ì „ì´í•œë‹¤. ì´ë•Œì˜ ì‹œê°„ë‹¨ìœ„ë¥¼ time stepì´ë¼ê³  í•œë‹¤.

<div align="center">
  <div class = 'mermaid'>
    graph LR
    A[Agent] -->|í–‰ë™| B[Environment]
    B -->|ë³´ìƒ, ìƒíƒœ| A
  </div>
</div>

- **ìƒíƒœì „ì´** : ìƒíƒœëŠ” ì–´ë–»ê²Œ ì „ì´ë˜ëŠ”ê°€?
- **ë³´ìƒ** : ë³´ìƒì€ ì–´ë–»ê²Œ ì£¼ì–´ì§€ëŠ”ê°€?
- **ì •ì±…** : ì—ì´ì „íŠ¸ëŠ” í–‰ë™ì„ ì–´ë–»ê²Œ ê²°ì •í•˜ëŠ”ê°€?

ìœ„ì˜ ì„¸ê°€ì§€ ìš”ì†Œë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•´ì•¼í•œë‹¤. 

ìƒíƒœì „ì´ê°€ ê²°ì •ì ì¼ ê²½ìš° ë‹¤ìŒ ìƒíƒœ s'ëŠ” í˜„ì¬ ìƒíƒœ sì™€ í–‰ë™ aì—ë§Œ ì˜ì¡´í•œë‹¤.

ìƒíƒœì „ì´í•¨ìˆ˜ => 
$$
s' = f(s, a)
$$

ìƒíƒœì „ì´ê°€ í™•ë¥ ì ì¼ ê²½ìš° ë‹¤ìŒ ìƒíƒœ s'ëŠ” í˜„ì¬ ìƒíƒœ sì™€ í–‰ë™ aì—ë§Œ ì˜ì¡´í•œë‹¤.

ìƒíƒœì „ì´í™•ë¥  =>
$$
P(s' | s, a)
$$

### ë³´ìƒí•¨ìˆ˜

ë³´ìƒí•¨ìˆ˜ëŠ” ìƒíƒœ sì™€ í–‰ë™ aì— ëŒ€í•œ ë³´ìƒì„ ë°˜í™˜í•œë‹¤.ì—ì´ì „íŠ¸ê°€ ìƒíƒœ sì—ì„œ í–‰ë™ aë¥¼ ì·¨í•˜ì—¬ ë‹¤ìŒ ìƒíƒœ s'ë¡œ ì´ë™í–ˆì„ ë•Œ ë°›ëŠ” ë³´ìƒì„ ë°˜í™˜í•œë‹¤.

ë³´ìƒí•¨ìˆ˜ =>
$$
r(s, a, s')
$$


### ì—ì´ì „íŠ¸ì˜ ì •ì±…

ì—ì´ì „íŠ¸ì˜ ì •ì±…ì€ ì—ì´ì „íŠ¸ê°€ í–‰ë™ì„ ê²°ì •í•˜ëŠ” ë°©ì‹ì„ ë§í•œë‹¤. ì—ì´ì „íŠ¸ëŠ” 'í˜„ì¬ ìƒíƒœ' ë§Œë“œì˜¤ í–‰ë™ì„ ê²°ì •í•œë‹¤.
ì™œëƒí•˜ë©´ 'í™˜ê²½ì— ëŒ€í•´ í•„ìš”í•œ ì •ë³´ëŠ” ëª¨ë‘ í˜„ì¬ ìƒíƒœì— ë‹´ê²¨' ìˆê¸° ë•Œë¬¸ì´ë‹¤.

ì—ì´ì „íŠ¸ê°€ í™•ë¥ ì ìœ¼ë¡œ ê²°ì •ë˜ëŠ” ì •ì±…ì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

ì •ì±… =>
$$
\pi(a | s) = P(a | s)
$$
    
### MDPì˜ ëª©í‘œ

MDPì˜ ëª©í‘œëŠ” ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ì •ì±…ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ì—ì´ì „ëŠ” ì •ì±… 
$$ 
\pi(a | s) 
$$
ì— ë”°ë¼ í–‰ë™í•œë‹¤. ê·¸ í–‰ë™ê³¼ ìƒíƒœ ì „ì´ í™•ë¥  $$ P(s' | s, a) $$ ì— ë”°ë¼ ë‹¤ìŒ ìƒíƒœê°€ ê²°ì •ëœë‹¤. ê·¸ë¦¬ê³  ë³´ìƒí•¨ìˆ˜ $$ r(s, a, s') $$ ì— ë”°ë¼ ë³´ìƒì„ ë°›ëŠ”ë‹¤.

### return : ìˆ˜ìµ

ì‹œê°„ tì—ì„œì˜ ìƒíƒœë¥¼ $$ S_t $$, ì •ì±… $$ \pi $$ ì— ë”°ë¼ í–‰ë™ì„ $$ A_t $$, ë³´ìƒì„ $$ R_t $$ë¥¼ ì–»ê³ , ìƒˆë¡œìš´ ìƒíƒœ $$ S_{t+1} $$ì „ì´ë˜ëŠ” íë¦„ìœ¼ë¡œ ì´ì–´ì§„ë‹¤. ì´ë•Œì˜ ìˆ˜ìµì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$
G_t = R_t + rR_{t+1} + r^2R_{t+2} + ... = \sum_{k=0}^{\infty} r^k R_{t+k}
$$

ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ë³´ìƒì„ $$ \gamma $$ ì— ì˜í•´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê°ì†Œí•œë‹¤. 

### ìƒíƒœê°€ì¹˜í•¨ìˆ˜

ìˆ˜ìµì„ ê·¹ëŒ€í™” í™”ëŠ” ê²ƒì´ ì—ì´ì „íŠ¸ì˜ ëª©í‘œì„. ê°™ì€ ìƒíƒœì—ì„œ ì—ì´ì „íŠ¸ê°€ ì‹œì‘í•˜ë”ë¼ë„ ìˆ˜ìµì´ ì—í”¼ì†Œë“œë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ í™•ë¥ ì  ë™ì‘ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´, ê¸°ëŒ“ê°’, ì¦‰, ìˆ˜ìµì˜ ê¸°ëŒ“ê°’ì„ ì§€í‘œë¡œ ì‚¼ëŠ”ë‹¤.

ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ëŠ” ê°•í™” í•™ìŠµì—ì„œ íŠ¹ì • ìƒíƒœì—ì„œ ì‹œì‘í•˜ì—¬ ë¯¸ë˜ì— ë°›ì„ ìˆ˜ ìˆëŠ” ë³´ìƒì˜ ê¸°ëŒ€ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ $$V(s)$$ë¡œ í‘œí˜„ë˜ë©°, ì—¬ê¸°ì„œ $$s$$ëŠ” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ëŠ” ì •ì±… $$\pi$$ì— ë”°ë¼ ê³„ì‚°ë˜ë©°, ìˆ˜ì‹ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:

<div style="overflow-x: auto;">
$$
V_{\pi}(s) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s, \pi \right]
$$
</div>

$$
= \mathbb{E}_\pi \left[G_t \mid S_t = s \right]
$$

ì—¬ê¸°ì„œ:
- $$\mathbb{E}_\pi$$: ì •ì±… $$\pi$$ì— ë”°ë¥¸ ê¸°ëŒ€ê°’
- $$\gamma$$: í• ì¸ìœ¨ (0 â‰¤ $$\gamma$$ < 1)
- $$R_{t+1}$$: ì‹œê°„ $$t+1$$ì—ì„œì˜ ë³´ìƒ
- $$S_0 = s$$: ì´ˆê¸° ìƒíƒœ

ì¦‰, ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì •ì±…ì„ ë”°ë¥¼ ë•Œ íŠ¹ì • ìƒíƒœì—ì„œ ì‹œì‘í•˜ì—¬ ì¥ê¸°ì ìœ¼ë¡œ ë°›ì„ ë³´ìƒì˜ ì´í•©ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ì´ëŠ” ì •ì±…ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê±°ë‚˜ ìµœì  ì •ì±…ì„ ì°¾ëŠ”ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.

### ìµœì  ì •ì±…ê³¼ ìµœì  ê°€ì¹˜ í•¨ìˆ˜


ê°•í™” í•™ìŠµì—ì„œ ìµœì  ì •ì±…(optimal policy) $$\pi^*$$ëŠ” ëª¨ë“  ìƒíƒœì—ì„œ ê¸°ëŒ€ ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ì •ì±…ì´ë‹¤. ìµœì  ì •ì±…ì„ ë”°ë¥¸ë‹¤ë©´ ì—ì´ì „íŠ¸ëŠ” ê°€ëŠ¥í•œ ìµœëŒ€ ë³´ìƒì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

ìµœì  ê°€ì¹˜ í•¨ìˆ˜(optimal value function) $$V^*(s)$$ëŠ” ìµœì  ì •ì±…ì„ ë”°ë¥¼ ë•Œ ìƒíƒœ $$s$$ì—ì„œ ì‹œì‘í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆëŠ” ê¸°ëŒ€ ë³´ìƒì˜ í•©ì´ë‹¤:

<div style="overflow-x: auto;">
$$
V^*(s) = \max_{\pi} V^{\pi}(s) = \max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s \right]
$$
</div>

ë§ˆì°¬ê°€ì§€ë¡œ, ìµœì  í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜(optimal action-value function) $$Q^*(s,a)$$ëŠ” ìƒíƒœ $$s$$ì—ì„œ í–‰ë™ $$a$$ë¥¼ ì·¨í•˜ê³  ê·¸ ì´í›„ ìµœì  ì •ì±…ì„ ë”°ë¥¼ ë•Œ ì–»ì„ ìˆ˜ ìˆëŠ” ê¸°ëŒ€ ë³´ìƒì˜ í•©ì´ë‹¤:

<div style="overflow-x: auto;">
$$
Q^*(s,a) = \max_{\pi} Q^{\pi}(s,a) = \max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right]
$$
</div>

ìµœì  ì •ì±…ê³¼ ìµœì  ê°€ì¹˜ í•¨ìˆ˜ëŠ” ë²¨ë§Œ ìµœì  ë°©ì •ì‹(Bellman Optimality Equation)ì„ í†µí•´ ì •ì˜ë  ìˆ˜ ìˆë‹¤:


<div style="overflow-x: auto;">
$$
V^*(s) = \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]
$$
</div>

<div style="overflow-x: auto;">
$$
Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')
$$
</div>

ê°•í™” í•™ìŠµì˜ ëª©í‘œëŠ” ì´ëŸ¬í•œ ìµœì  ì •ì±… ë˜ëŠ” ìµœì  ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.

## Bellman equation

First, Summary of the Above. 

- **â“ What is an MDP?**

An MDP is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under the control of an agent. 

It consisis of : 

- A set of states (S)
- A set of actions (A)
- A transition Probablity function (P)
- A reward function (R)
- A discount factor (Î³)

So, MDP is the foundation of reinforcement learning, where an agent learns to choose actions that maximize cumulative reward over time. 


- **â“Why is important Bellman equation in MDP?**

The Bellman equation is important in Markov Decision Processes (MDPs) because it provides a recursive decomposition of the value function, which represents the expected return starting from a given state. It serves as the foundation for many reinforcement learning algorithms, enabling efficient computation of optimal policies by breaking down complex problems into smaller subproblems.

ğŸ”‘ Bellman Equation â€“ Easy Explanation (with Keywords)
- **The Bellman equation expresses**
"What kind of future reward can I expect if I act well in this state?"

- **It uses recursion to break down a complex problem into smaller subproblems.**

- **This allows us to efficiently and systematically optimize the overall policy.**

- **Many reinforcement learning algorithms like Q-learning and Value Iteration**
are based on the Bellman equation.

### Derivation of Bellman Equation. 

First, let's define 'Return at time t' as the sum of rewards from time 't'

<div style="overflow-x: auto;">
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$
</div>

Second, What is 'Return at time t + 1'?
<div style="overflow-x: auto;">
$$
G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
</div>

So, we can rearrange the equation as above two equatios. 

<div style="overflow-x: auto;">
$$
G_t = R_t + \gamma G_{t+1}
$$
</div>

We know the relation between $$G_t$$ and $$G_{t+1}$$.  

Based on the state-vlaue function $$V_\pi(s)$$ we obtained earlier, we can derived the following conclusion. 

<div style="overflow-x: auto;">
$$
V_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
    = \mathbb{E}_\pi[R_t + \gamma G_{t+1} | S_t = s]
    = \mathbb{E}_\pi[R_t | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]
$$
</div>

(Since Linearity of Expectation ğŸ‘‰ $$\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]$$)
